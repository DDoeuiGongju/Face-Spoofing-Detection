{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 230, 230, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1/conv (Conv2D)             (None, 112, 112, 64) 9408        zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1/bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1/conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1/relu (Activation)         (None, 112, 112, 64) 0           conv1/bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 114, 114, 64) 0           conv1/relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 56, 56, 64)   0           zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 56, 56, 64)   256         pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_relu (Activation (None, 56, 56, 64)   0           conv2_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 128)  8192        conv2_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 128)  0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_concat (Concatenat (None, 56, 56, 96)   0           pool1[0][0]                      \n",
      "                                                                 conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_bn (BatchNormali (None, 56, 56, 96)   384         conv2_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_relu (Activation (None, 56, 56, 96)   0           conv2_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 128)  12288       conv2_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 128)  0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_concat (Concatenat (None, 56, 56, 128)  0           conv2_block1_concat[0][0]        \n",
      "                                                                 conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_0_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_0_relu (Activation (None, 56, 56, 128)  0           conv2_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 128)  16384       conv2_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 128)  0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_concat (Concatenat (None, 56, 56, 160)  0           conv2_block2_concat[0][0]        \n",
      "                                                                 conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_0_bn (BatchNormali (None, 56, 56, 160)  640         conv2_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_0_relu (Activation (None, 56, 56, 160)  0           conv2_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_conv (Conv2D)    (None, 56, 56, 128)  20480       conv2_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_relu (Activation (None, 56, 56, 128)  0           conv2_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_concat (Concatenat (None, 56, 56, 192)  0           conv2_block3_concat[0][0]        \n",
      "                                                                 conv2_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_0_bn (BatchNormali (None, 56, 56, 192)  768         conv2_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_0_relu (Activation (None, 56, 56, 192)  0           conv2_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_conv (Conv2D)    (None, 56, 56, 128)  24576       conv2_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_relu (Activation (None, 56, 56, 128)  0           conv2_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_concat (Concatenat (None, 56, 56, 224)  0           conv2_block4_concat[0][0]        \n",
      "                                                                 conv2_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_0_bn (BatchNormali (None, 56, 56, 224)  896         conv2_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_0_relu (Activation (None, 56, 56, 224)  0           conv2_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_conv (Conv2D)    (None, 56, 56, 128)  28672       conv2_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_relu (Activation (None, 56, 56, 128)  0           conv2_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_concat (Concatenat (None, 56, 56, 256)  0           conv2_block5_concat[0][0]        \n",
      "                                                                 conv2_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool2_bn (BatchNormalization)   (None, 56, 56, 256)  1024        conv2_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool2_relu (Activation)         (None, 56, 56, 256)  0           pool2_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool2_conv (Conv2D)             (None, 56, 56, 128)  32768       pool2_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool2_pool (AveragePooling2D)   (None, 28, 28, 128)  0           pool2_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 28, 28, 128)  512         pool2_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_relu (Activation (None, 28, 28, 128)  0           conv3_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  16384       conv3_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_concat (Concatenat (None, 28, 28, 160)  0           pool2_pool[0][0]                 \n",
      "                                                                 conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_bn (BatchNormali (None, 28, 28, 160)  640         conv3_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_relu (Activation (None, 28, 28, 160)  0           conv3_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  20480       conv3_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_concat (Concatenat (None, 28, 28, 192)  0           conv3_block1_concat[0][0]        \n",
      "                                                                 conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_0_bn (BatchNormali (None, 28, 28, 192)  768         conv3_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_0_relu (Activation (None, 28, 28, 192)  0           conv3_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  24576       conv3_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_concat (Concatenat (None, 28, 28, 224)  0           conv3_block2_concat[0][0]        \n",
      "                                                                 conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_0_bn (BatchNormali (None, 28, 28, 224)  896         conv3_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_0_relu (Activation (None, 28, 28, 224)  0           conv3_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  28672       conv3_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_concat (Concatenat (None, 28, 28, 256)  0           conv3_block3_concat[0][0]        \n",
      "                                                                 conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_0_bn (BatchNormali (None, 28, 28, 256)  1024        conv3_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_0_relu (Activation (None, 28, 28, 256)  0           conv3_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_conv (Conv2D)    (None, 28, 28, 128)  32768       conv3_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_relu (Activation (None, 28, 28, 128)  0           conv3_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_concat (Concatenat (None, 28, 28, 288)  0           conv3_block4_concat[0][0]        \n",
      "                                                                 conv3_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_0_bn (BatchNormali (None, 28, 28, 288)  1152        conv3_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_0_relu (Activation (None, 28, 28, 288)  0           conv3_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_conv (Conv2D)    (None, 28, 28, 128)  36864       conv3_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_relu (Activation (None, 28, 28, 128)  0           conv3_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_concat (Concatenat (None, 28, 28, 320)  0           conv3_block5_concat[0][0]        \n",
      "                                                                 conv3_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_0_bn (BatchNormali (None, 28, 28, 320)  1280        conv3_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_0_relu (Activation (None, 28, 28, 320)  0           conv3_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_conv (Conv2D)    (None, 28, 28, 128)  40960       conv3_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_relu (Activation (None, 28, 28, 128)  0           conv3_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_concat (Concatenat (None, 28, 28, 352)  0           conv3_block6_concat[0][0]        \n",
      "                                                                 conv3_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_0_bn (BatchNormali (None, 28, 28, 352)  1408        conv3_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_0_relu (Activation (None, 28, 28, 352)  0           conv3_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_conv (Conv2D)    (None, 28, 28, 128)  45056       conv3_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_relu (Activation (None, 28, 28, 128)  0           conv3_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_concat (Concatenat (None, 28, 28, 384)  0           conv3_block7_concat[0][0]        \n",
      "                                                                 conv3_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_0_bn (BatchNormali (None, 28, 28, 384)  1536        conv3_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_0_relu (Activation (None, 28, 28, 384)  0           conv3_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_conv (Conv2D)    (None, 28, 28, 128)  49152       conv3_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_relu (Activation (None, 28, 28, 128)  0           conv3_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_concat (Concatenat (None, 28, 28, 416)  0           conv3_block8_concat[0][0]        \n",
      "                                                                 conv3_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_0_bn (BatchNormal (None, 28, 28, 416)  1664        conv3_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_0_relu (Activatio (None, 28, 28, 416)  0           conv3_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_conv (Conv2D)   (None, 28, 28, 128)  53248       conv3_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_concat (Concatena (None, 28, 28, 448)  0           conv3_block9_concat[0][0]        \n",
      "                                                                 conv3_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_0_bn (BatchNormal (None, 28, 28, 448)  1792        conv3_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_0_relu (Activatio (None, 28, 28, 448)  0           conv3_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_conv (Conv2D)   (None, 28, 28, 128)  57344       conv3_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_concat (Concatena (None, 28, 28, 480)  0           conv3_block10_concat[0][0]       \n",
      "                                                                 conv3_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_0_bn (BatchNormal (None, 28, 28, 480)  1920        conv3_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_0_relu (Activatio (None, 28, 28, 480)  0           conv3_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_conv (Conv2D)   (None, 28, 28, 128)  61440       conv3_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_concat (Concatena (None, 28, 28, 512)  0           conv3_block11_concat[0][0]       \n",
      "                                                                 conv3_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3_bn (BatchNormalization)   (None, 28, 28, 512)  2048        conv3_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3_relu (Activation)         (None, 28, 28, 512)  0           pool3_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool3_conv (Conv2D)             (None, 28, 28, 256)  131072      pool3_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool3_pool (AveragePooling2D)   (None, 14, 14, 256)  0           pool3_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 256)  1024        pool3_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_relu (Activation (None, 14, 14, 256)  0           conv4_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 128)  32768       conv4_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 128)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_concat (Concatenat (None, 14, 14, 288)  0           pool3_pool[0][0]                 \n",
      "                                                                 conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_bn (BatchNormali (None, 14, 14, 288)  1152        conv4_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_relu (Activation (None, 14, 14, 288)  0           conv4_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 128)  36864       conv4_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 128)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_concat (Concatenat (None, 14, 14, 320)  0           conv4_block1_concat[0][0]        \n",
      "                                                                 conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_0_bn (BatchNormali (None, 14, 14, 320)  1280        conv4_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_0_relu (Activation (None, 14, 14, 320)  0           conv4_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 128)  40960       conv4_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 128)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_concat (Concatenat (None, 14, 14, 352)  0           conv4_block2_concat[0][0]        \n",
      "                                                                 conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_0_bn (BatchNormali (None, 14, 14, 352)  1408        conv4_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_0_relu (Activation (None, 14, 14, 352)  0           conv4_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 128)  45056       conv4_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 128)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_concat (Concatenat (None, 14, 14, 384)  0           conv4_block3_concat[0][0]        \n",
      "                                                                 conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_0_bn (BatchNormali (None, 14, 14, 384)  1536        conv4_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_0_relu (Activation (None, 14, 14, 384)  0           conv4_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 128)  49152       conv4_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 128)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_concat (Concatenat (None, 14, 14, 416)  0           conv4_block4_concat[0][0]        \n",
      "                                                                 conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_0_bn (BatchNormali (None, 14, 14, 416)  1664        conv4_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_0_relu (Activation (None, 14, 14, 416)  0           conv4_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 128)  53248       conv4_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 128)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_concat (Concatenat (None, 14, 14, 448)  0           conv4_block5_concat[0][0]        \n",
      "                                                                 conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_0_bn (BatchNormali (None, 14, 14, 448)  1792        conv4_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_0_relu (Activation (None, 14, 14, 448)  0           conv4_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_conv (Conv2D)    (None, 14, 14, 128)  57344       conv4_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_relu (Activation (None, 14, 14, 128)  0           conv4_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_concat (Concatenat (None, 14, 14, 480)  0           conv4_block6_concat[0][0]        \n",
      "                                                                 conv4_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_0_bn (BatchNormali (None, 14, 14, 480)  1920        conv4_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_0_relu (Activation (None, 14, 14, 480)  0           conv4_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_conv (Conv2D)    (None, 14, 14, 128)  61440       conv4_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_relu (Activation (None, 14, 14, 128)  0           conv4_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_concat (Concatenat (None, 14, 14, 512)  0           conv4_block7_concat[0][0]        \n",
      "                                                                 conv4_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_0_bn (BatchNormali (None, 14, 14, 512)  2048        conv4_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_0_relu (Activation (None, 14, 14, 512)  0           conv4_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_conv (Conv2D)    (None, 14, 14, 128)  65536       conv4_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_relu (Activation (None, 14, 14, 128)  0           conv4_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_concat (Concatenat (None, 14, 14, 544)  0           conv4_block8_concat[0][0]        \n",
      "                                                                 conv4_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_0_bn (BatchNormal (None, 14, 14, 544)  2176        conv4_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_0_relu (Activatio (None, 14, 14, 544)  0           conv4_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_conv (Conv2D)   (None, 14, 14, 128)  69632       conv4_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_concat (Concatena (None, 14, 14, 576)  0           conv4_block9_concat[0][0]        \n",
      "                                                                 conv4_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_0_bn (BatchNormal (None, 14, 14, 576)  2304        conv4_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_0_relu (Activatio (None, 14, 14, 576)  0           conv4_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_conv (Conv2D)   (None, 14, 14, 128)  73728       conv4_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_concat (Concatena (None, 14, 14, 608)  0           conv4_block10_concat[0][0]       \n",
      "                                                                 conv4_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_0_bn (BatchNormal (None, 14, 14, 608)  2432        conv4_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_0_relu (Activatio (None, 14, 14, 608)  0           conv4_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_conv (Conv2D)   (None, 14, 14, 128)  77824       conv4_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_concat (Concatena (None, 14, 14, 640)  0           conv4_block11_concat[0][0]       \n",
      "                                                                 conv4_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_0_bn (BatchNormal (None, 14, 14, 640)  2560        conv4_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_0_relu (Activatio (None, 14, 14, 640)  0           conv4_block13_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_conv (Conv2D)   (None, 14, 14, 128)  81920       conv4_block13_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_concat (Concatena (None, 14, 14, 672)  0           conv4_block12_concat[0][0]       \n",
      "                                                                 conv4_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_0_bn (BatchNormal (None, 14, 14, 672)  2688        conv4_block13_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_0_relu (Activatio (None, 14, 14, 672)  0           conv4_block14_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_conv (Conv2D)   (None, 14, 14, 128)  86016       conv4_block14_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_concat (Concatena (None, 14, 14, 704)  0           conv4_block13_concat[0][0]       \n",
      "                                                                 conv4_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_0_bn (BatchNormal (None, 14, 14, 704)  2816        conv4_block14_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_0_relu (Activatio (None, 14, 14, 704)  0           conv4_block15_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_conv (Conv2D)   (None, 14, 14, 128)  90112       conv4_block15_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_concat (Concatena (None, 14, 14, 736)  0           conv4_block14_concat[0][0]       \n",
      "                                                                 conv4_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_0_bn (BatchNormal (None, 14, 14, 736)  2944        conv4_block15_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_0_relu (Activatio (None, 14, 14, 736)  0           conv4_block16_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_conv (Conv2D)   (None, 14, 14, 128)  94208       conv4_block16_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_concat (Concatena (None, 14, 14, 768)  0           conv4_block15_concat[0][0]       \n",
      "                                                                 conv4_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_0_bn (BatchNormal (None, 14, 14, 768)  3072        conv4_block16_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_0_relu (Activatio (None, 14, 14, 768)  0           conv4_block17_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_conv (Conv2D)   (None, 14, 14, 128)  98304       conv4_block17_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block17_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block17_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block17_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_concat (Concatena (None, 14, 14, 800)  0           conv4_block16_concat[0][0]       \n",
      "                                                                 conv4_block17_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_0_bn (BatchNormal (None, 14, 14, 800)  3200        conv4_block17_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_0_relu (Activatio (None, 14, 14, 800)  0           conv4_block18_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_conv (Conv2D)   (None, 14, 14, 128)  102400      conv4_block18_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block18_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block18_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block18_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_concat (Concatena (None, 14, 14, 832)  0           conv4_block17_concat[0][0]       \n",
      "                                                                 conv4_block18_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_0_bn (BatchNormal (None, 14, 14, 832)  3328        conv4_block18_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_0_relu (Activatio (None, 14, 14, 832)  0           conv4_block19_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_conv (Conv2D)   (None, 14, 14, 128)  106496      conv4_block19_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block19_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block19_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block19_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_concat (Concatena (None, 14, 14, 864)  0           conv4_block18_concat[0][0]       \n",
      "                                                                 conv4_block19_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_0_bn (BatchNormal (None, 14, 14, 864)  3456        conv4_block19_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_0_relu (Activatio (None, 14, 14, 864)  0           conv4_block20_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_conv (Conv2D)   (None, 14, 14, 128)  110592      conv4_block20_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block20_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block20_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block20_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_concat (Concatena (None, 14, 14, 896)  0           conv4_block19_concat[0][0]       \n",
      "                                                                 conv4_block20_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_0_bn (BatchNormal (None, 14, 14, 896)  3584        conv4_block20_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_0_relu (Activatio (None, 14, 14, 896)  0           conv4_block21_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_conv (Conv2D)   (None, 14, 14, 128)  114688      conv4_block21_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block21_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block21_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block21_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_concat (Concatena (None, 14, 14, 928)  0           conv4_block20_concat[0][0]       \n",
      "                                                                 conv4_block21_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_0_bn (BatchNormal (None, 14, 14, 928)  3712        conv4_block21_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_0_relu (Activatio (None, 14, 14, 928)  0           conv4_block22_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_conv (Conv2D)   (None, 14, 14, 128)  118784      conv4_block22_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block22_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block22_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block22_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_concat (Concatena (None, 14, 14, 960)  0           conv4_block21_concat[0][0]       \n",
      "                                                                 conv4_block22_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_0_bn (BatchNormal (None, 14, 14, 960)  3840        conv4_block22_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_0_relu (Activatio (None, 14, 14, 960)  0           conv4_block23_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_conv (Conv2D)   (None, 14, 14, 128)  122880      conv4_block23_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block23_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block23_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block23_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_concat (Concatena (None, 14, 14, 992)  0           conv4_block22_concat[0][0]       \n",
      "                                                                 conv4_block23_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_0_bn (BatchNormal (None, 14, 14, 992)  3968        conv4_block23_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_0_relu (Activatio (None, 14, 14, 992)  0           conv4_block24_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_conv (Conv2D)   (None, 14, 14, 128)  126976      conv4_block24_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block24_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block24_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block24_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_concat (Concatena (None, 14, 14, 1024) 0           conv4_block23_concat[0][0]       \n",
      "                                                                 conv4_block24_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool4_bn (BatchNormalization)   (None, 14, 14, 1024) 4096        conv4_block24_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool4_relu (Activation)         (None, 14, 14, 1024) 0           pool4_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool4_conv (Conv2D)             (None, 14, 14, 512)  524288      pool4_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool4_pool (AveragePooling2D)   (None, 7, 7, 512)    0           pool4_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 512)    2048        pool4_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_relu (Activation (None, 7, 7, 512)    0           conv5_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 128)    65536       conv5_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 128)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_concat (Concatenat (None, 7, 7, 544)    0           pool4_pool[0][0]                 \n",
      "                                                                 conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_0_bn (BatchNormali (None, 7, 7, 544)    2176        conv5_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_0_relu (Activation (None, 7, 7, 544)    0           conv5_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 128)    69632       conv5_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 128)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_concat (Concatenat (None, 7, 7, 576)    0           conv5_block1_concat[0][0]        \n",
      "                                                                 conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_0_bn (BatchNormali (None, 7, 7, 576)    2304        conv5_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_0_relu (Activation (None, 7, 7, 576)    0           conv5_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 128)    73728       conv5_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 128)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_concat (Concatenat (None, 7, 7, 608)    0           conv5_block2_concat[0][0]        \n",
      "                                                                 conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_0_bn (BatchNormali (None, 7, 7, 608)    2432        conv5_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_0_relu (Activation (None, 7, 7, 608)    0           conv5_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_conv (Conv2D)    (None, 7, 7, 128)    77824       conv5_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_relu (Activation (None, 7, 7, 128)    0           conv5_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_concat (Concatenat (None, 7, 7, 640)    0           conv5_block3_concat[0][0]        \n",
      "                                                                 conv5_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_0_bn (BatchNormali (None, 7, 7, 640)    2560        conv5_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_0_relu (Activation (None, 7, 7, 640)    0           conv5_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_conv (Conv2D)    (None, 7, 7, 128)    81920       conv5_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_relu (Activation (None, 7, 7, 128)    0           conv5_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_concat (Concatenat (None, 7, 7, 672)    0           conv5_block4_concat[0][0]        \n",
      "                                                                 conv5_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_0_bn (BatchNormali (None, 7, 7, 672)    2688        conv5_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_0_relu (Activation (None, 7, 7, 672)    0           conv5_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_conv (Conv2D)    (None, 7, 7, 128)    86016       conv5_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_relu (Activation (None, 7, 7, 128)    0           conv5_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_concat (Concatenat (None, 7, 7, 704)    0           conv5_block5_concat[0][0]        \n",
      "                                                                 conv5_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_0_bn (BatchNormali (None, 7, 7, 704)    2816        conv5_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_0_relu (Activation (None, 7, 7, 704)    0           conv5_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_conv (Conv2D)    (None, 7, 7, 128)    90112       conv5_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_relu (Activation (None, 7, 7, 128)    0           conv5_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_concat (Concatenat (None, 7, 7, 736)    0           conv5_block6_concat[0][0]        \n",
      "                                                                 conv5_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_0_bn (BatchNormali (None, 7, 7, 736)    2944        conv5_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_0_relu (Activation (None, 7, 7, 736)    0           conv5_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_conv (Conv2D)    (None, 7, 7, 128)    94208       conv5_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_relu (Activation (None, 7, 7, 128)    0           conv5_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_concat (Concatenat (None, 7, 7, 768)    0           conv5_block7_concat[0][0]        \n",
      "                                                                 conv5_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_0_bn (BatchNormali (None, 7, 7, 768)    3072        conv5_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_0_relu (Activation (None, 7, 7, 768)    0           conv5_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_conv (Conv2D)    (None, 7, 7, 128)    98304       conv5_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_relu (Activation (None, 7, 7, 128)    0           conv5_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_concat (Concatenat (None, 7, 7, 800)    0           conv5_block8_concat[0][0]        \n",
      "                                                                 conv5_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_0_bn (BatchNormal (None, 7, 7, 800)    3200        conv5_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_0_relu (Activatio (None, 7, 7, 800)    0           conv5_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_conv (Conv2D)   (None, 7, 7, 128)    102400      conv5_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_concat (Concatena (None, 7, 7, 832)    0           conv5_block9_concat[0][0]        \n",
      "                                                                 conv5_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_0_bn (BatchNormal (None, 7, 7, 832)    3328        conv5_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_0_relu (Activatio (None, 7, 7, 832)    0           conv5_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_conv (Conv2D)   (None, 7, 7, 128)    106496      conv5_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_concat (Concatena (None, 7, 7, 864)    0           conv5_block10_concat[0][0]       \n",
      "                                                                 conv5_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_0_bn (BatchNormal (None, 7, 7, 864)    3456        conv5_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_0_relu (Activatio (None, 7, 7, 864)    0           conv5_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_conv (Conv2D)   (None, 7, 7, 128)    110592      conv5_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_concat (Concatena (None, 7, 7, 896)    0           conv5_block11_concat[0][0]       \n",
      "                                                                 conv5_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_0_bn (BatchNormal (None, 7, 7, 896)    3584        conv5_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_0_relu (Activatio (None, 7, 7, 896)    0           conv5_block13_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_conv (Conv2D)   (None, 7, 7, 128)    114688      conv5_block13_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_concat (Concatena (None, 7, 7, 928)    0           conv5_block12_concat[0][0]       \n",
      "                                                                 conv5_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_0_bn (BatchNormal (None, 7, 7, 928)    3712        conv5_block13_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_0_relu (Activatio (None, 7, 7, 928)    0           conv5_block14_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_conv (Conv2D)   (None, 7, 7, 128)    118784      conv5_block14_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_concat (Concatena (None, 7, 7, 960)    0           conv5_block13_concat[0][0]       \n",
      "                                                                 conv5_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_0_bn (BatchNormal (None, 7, 7, 960)    3840        conv5_block14_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_0_relu (Activatio (None, 7, 7, 960)    0           conv5_block15_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_conv (Conv2D)   (None, 7, 7, 128)    122880      conv5_block15_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_concat (Concatena (None, 7, 7, 992)    0           conv5_block14_concat[0][0]       \n",
      "                                                                 conv5_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_0_bn (BatchNormal (None, 7, 7, 992)    3968        conv5_block15_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_0_relu (Activatio (None, 7, 7, 992)    0           conv5_block16_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_conv (Conv2D)   (None, 7, 7, 128)    126976      conv5_block16_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_concat (Concatena (None, 7, 7, 1024)   0           conv5_block15_concat[0][0]       \n",
      "                                                                 conv5_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bn (BatchNormalization)         (None, 7, 7, 1024)   4096        conv5_block16_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "relu (Activation)               (None, 7, 7, 1024)   0           bn[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 1024)         0           relu[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            1025        global_average_pooling2d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 7,038,529\n",
      "Trainable params: 6,954,881\n",
      "Non-trainable params: 83,648\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "#keras.applications.densenet.DenseNet121(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n",
    "from tensorflow.keras.applications.densenet import DenseNet121\n",
    "#from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.densenet import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Activation\n",
    "\n",
    "def Densenet121(show_layers,weights,input_shape):\n",
    "        base_model = DenseNet121(include_top=False, weights=weights, input_shape=input_shape)\n",
    "        x = GlobalAveragePooling2D()(base_model.output)\n",
    "        pred = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "        model = Model(inputs=base_model.input, outputs=pred)\n",
    "        model.compile(optimizer=SGD(lr=1e-2, decay=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "        if show_layers:\n",
    "            for i, layer in enumerate(model.layers):\n",
    "                print(i, layer.name, layer.trainable)\n",
    "        return model\n",
    "    \n",
    "model = Densenet121(False,weights=None,input_shape=(224, 224, 3))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protocol4 densenet학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRLAB DB\n",
    "\n",
    "## 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 230, 230, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1/conv (Conv2D)             (None, 112, 112, 64) 9408        zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1/bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1/conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1/relu (Activation)         (None, 112, 112, 64) 0           conv1/bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 114, 114, 64) 0           conv1/relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 56, 56, 64)   0           zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 56, 56, 64)   256         pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_relu (Activation (None, 56, 56, 64)   0           conv2_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 128)  8192        conv2_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 128)  0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_concat (Concatenat (None, 56, 56, 96)   0           pool1[0][0]                      \n",
      "                                                                 conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_bn (BatchNormali (None, 56, 56, 96)   384         conv2_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_relu (Activation (None, 56, 56, 96)   0           conv2_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 128)  12288       conv2_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 128)  0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_concat (Concatenat (None, 56, 56, 128)  0           conv2_block1_concat[0][0]        \n",
      "                                                                 conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_0_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_0_relu (Activation (None, 56, 56, 128)  0           conv2_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 128)  16384       conv2_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 128)  0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_concat (Concatenat (None, 56, 56, 160)  0           conv2_block2_concat[0][0]        \n",
      "                                                                 conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_0_bn (BatchNormali (None, 56, 56, 160)  640         conv2_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_0_relu (Activation (None, 56, 56, 160)  0           conv2_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_conv (Conv2D)    (None, 56, 56, 128)  20480       conv2_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_relu (Activation (None, 56, 56, 128)  0           conv2_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_concat (Concatenat (None, 56, 56, 192)  0           conv2_block3_concat[0][0]        \n",
      "                                                                 conv2_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_0_bn (BatchNormali (None, 56, 56, 192)  768         conv2_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_0_relu (Activation (None, 56, 56, 192)  0           conv2_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_conv (Conv2D)    (None, 56, 56, 128)  24576       conv2_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_relu (Activation (None, 56, 56, 128)  0           conv2_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_concat (Concatenat (None, 56, 56, 224)  0           conv2_block4_concat[0][0]        \n",
      "                                                                 conv2_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_0_bn (BatchNormali (None, 56, 56, 224)  896         conv2_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_0_relu (Activation (None, 56, 56, 224)  0           conv2_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_conv (Conv2D)    (None, 56, 56, 128)  28672       conv2_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_relu (Activation (None, 56, 56, 128)  0           conv2_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_concat (Concatenat (None, 56, 56, 256)  0           conv2_block5_concat[0][0]        \n",
      "                                                                 conv2_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool2_bn (BatchNormalization)   (None, 56, 56, 256)  1024        conv2_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool2_relu (Activation)         (None, 56, 56, 256)  0           pool2_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool2_conv (Conv2D)             (None, 56, 56, 128)  32768       pool2_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool2_pool (AveragePooling2D)   (None, 28, 28, 128)  0           pool2_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 28, 28, 128)  512         pool2_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_relu (Activation (None, 28, 28, 128)  0           conv3_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  16384       conv3_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_concat (Concatenat (None, 28, 28, 160)  0           pool2_pool[0][0]                 \n",
      "                                                                 conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_bn (BatchNormali (None, 28, 28, 160)  640         conv3_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_relu (Activation (None, 28, 28, 160)  0           conv3_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  20480       conv3_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_concat (Concatenat (None, 28, 28, 192)  0           conv3_block1_concat[0][0]        \n",
      "                                                                 conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_0_bn (BatchNormali (None, 28, 28, 192)  768         conv3_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_0_relu (Activation (None, 28, 28, 192)  0           conv3_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  24576       conv3_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_concat (Concatenat (None, 28, 28, 224)  0           conv3_block2_concat[0][0]        \n",
      "                                                                 conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_0_bn (BatchNormali (None, 28, 28, 224)  896         conv3_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_0_relu (Activation (None, 28, 28, 224)  0           conv3_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  28672       conv3_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_concat (Concatenat (None, 28, 28, 256)  0           conv3_block3_concat[0][0]        \n",
      "                                                                 conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_0_bn (BatchNormali (None, 28, 28, 256)  1024        conv3_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_0_relu (Activation (None, 28, 28, 256)  0           conv3_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_conv (Conv2D)    (None, 28, 28, 128)  32768       conv3_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_relu (Activation (None, 28, 28, 128)  0           conv3_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_concat (Concatenat (None, 28, 28, 288)  0           conv3_block4_concat[0][0]        \n",
      "                                                                 conv3_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_0_bn (BatchNormali (None, 28, 28, 288)  1152        conv3_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_0_relu (Activation (None, 28, 28, 288)  0           conv3_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_conv (Conv2D)    (None, 28, 28, 128)  36864       conv3_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_relu (Activation (None, 28, 28, 128)  0           conv3_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_concat (Concatenat (None, 28, 28, 320)  0           conv3_block5_concat[0][0]        \n",
      "                                                                 conv3_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_0_bn (BatchNormali (None, 28, 28, 320)  1280        conv3_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_0_relu (Activation (None, 28, 28, 320)  0           conv3_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_conv (Conv2D)    (None, 28, 28, 128)  40960       conv3_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_relu (Activation (None, 28, 28, 128)  0           conv3_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_concat (Concatenat (None, 28, 28, 352)  0           conv3_block6_concat[0][0]        \n",
      "                                                                 conv3_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_0_bn (BatchNormali (None, 28, 28, 352)  1408        conv3_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_0_relu (Activation (None, 28, 28, 352)  0           conv3_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_conv (Conv2D)    (None, 28, 28, 128)  45056       conv3_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_relu (Activation (None, 28, 28, 128)  0           conv3_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_concat (Concatenat (None, 28, 28, 384)  0           conv3_block7_concat[0][0]        \n",
      "                                                                 conv3_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_0_bn (BatchNormali (None, 28, 28, 384)  1536        conv3_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_0_relu (Activation (None, 28, 28, 384)  0           conv3_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_conv (Conv2D)    (None, 28, 28, 128)  49152       conv3_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_relu (Activation (None, 28, 28, 128)  0           conv3_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_concat (Concatenat (None, 28, 28, 416)  0           conv3_block8_concat[0][0]        \n",
      "                                                                 conv3_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_0_bn (BatchNormal (None, 28, 28, 416)  1664        conv3_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_0_relu (Activatio (None, 28, 28, 416)  0           conv3_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_conv (Conv2D)   (None, 28, 28, 128)  53248       conv3_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_concat (Concatena (None, 28, 28, 448)  0           conv3_block9_concat[0][0]        \n",
      "                                                                 conv3_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_0_bn (BatchNormal (None, 28, 28, 448)  1792        conv3_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_0_relu (Activatio (None, 28, 28, 448)  0           conv3_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_conv (Conv2D)   (None, 28, 28, 128)  57344       conv3_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_concat (Concatena (None, 28, 28, 480)  0           conv3_block10_concat[0][0]       \n",
      "                                                                 conv3_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_0_bn (BatchNormal (None, 28, 28, 480)  1920        conv3_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_0_relu (Activatio (None, 28, 28, 480)  0           conv3_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_conv (Conv2D)   (None, 28, 28, 128)  61440       conv3_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_concat (Concatena (None, 28, 28, 512)  0           conv3_block11_concat[0][0]       \n",
      "                                                                 conv3_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3_bn (BatchNormalization)   (None, 28, 28, 512)  2048        conv3_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3_relu (Activation)         (None, 28, 28, 512)  0           pool3_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool3_conv (Conv2D)             (None, 28, 28, 256)  131072      pool3_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool3_pool (AveragePooling2D)   (None, 14, 14, 256)  0           pool3_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 256)  1024        pool3_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_relu (Activation (None, 14, 14, 256)  0           conv4_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 128)  32768       conv4_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 128)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_concat (Concatenat (None, 14, 14, 288)  0           pool3_pool[0][0]                 \n",
      "                                                                 conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_bn (BatchNormali (None, 14, 14, 288)  1152        conv4_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_relu (Activation (None, 14, 14, 288)  0           conv4_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 128)  36864       conv4_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 128)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_concat (Concatenat (None, 14, 14, 320)  0           conv4_block1_concat[0][0]        \n",
      "                                                                 conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_0_bn (BatchNormali (None, 14, 14, 320)  1280        conv4_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_0_relu (Activation (None, 14, 14, 320)  0           conv4_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 128)  40960       conv4_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 128)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_concat (Concatenat (None, 14, 14, 352)  0           conv4_block2_concat[0][0]        \n",
      "                                                                 conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_0_bn (BatchNormali (None, 14, 14, 352)  1408        conv4_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_0_relu (Activation (None, 14, 14, 352)  0           conv4_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 128)  45056       conv4_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 128)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_concat (Concatenat (None, 14, 14, 384)  0           conv4_block3_concat[0][0]        \n",
      "                                                                 conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_0_bn (BatchNormali (None, 14, 14, 384)  1536        conv4_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_0_relu (Activation (None, 14, 14, 384)  0           conv4_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 128)  49152       conv4_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 128)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_concat (Concatenat (None, 14, 14, 416)  0           conv4_block4_concat[0][0]        \n",
      "                                                                 conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_0_bn (BatchNormali (None, 14, 14, 416)  1664        conv4_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_0_relu (Activation (None, 14, 14, 416)  0           conv4_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 128)  53248       conv4_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 128)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_concat (Concatenat (None, 14, 14, 448)  0           conv4_block5_concat[0][0]        \n",
      "                                                                 conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_0_bn (BatchNormali (None, 14, 14, 448)  1792        conv4_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_0_relu (Activation (None, 14, 14, 448)  0           conv4_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_conv (Conv2D)    (None, 14, 14, 128)  57344       conv4_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_relu (Activation (None, 14, 14, 128)  0           conv4_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_concat (Concatenat (None, 14, 14, 480)  0           conv4_block6_concat[0][0]        \n",
      "                                                                 conv4_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_0_bn (BatchNormali (None, 14, 14, 480)  1920        conv4_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_0_relu (Activation (None, 14, 14, 480)  0           conv4_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_conv (Conv2D)    (None, 14, 14, 128)  61440       conv4_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_relu (Activation (None, 14, 14, 128)  0           conv4_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_concat (Concatenat (None, 14, 14, 512)  0           conv4_block7_concat[0][0]        \n",
      "                                                                 conv4_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_0_bn (BatchNormali (None, 14, 14, 512)  2048        conv4_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_0_relu (Activation (None, 14, 14, 512)  0           conv4_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_conv (Conv2D)    (None, 14, 14, 128)  65536       conv4_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_relu (Activation (None, 14, 14, 128)  0           conv4_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_concat (Concatenat (None, 14, 14, 544)  0           conv4_block8_concat[0][0]        \n",
      "                                                                 conv4_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_0_bn (BatchNormal (None, 14, 14, 544)  2176        conv4_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_0_relu (Activatio (None, 14, 14, 544)  0           conv4_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_conv (Conv2D)   (None, 14, 14, 128)  69632       conv4_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_concat (Concatena (None, 14, 14, 576)  0           conv4_block9_concat[0][0]        \n",
      "                                                                 conv4_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_0_bn (BatchNormal (None, 14, 14, 576)  2304        conv4_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_0_relu (Activatio (None, 14, 14, 576)  0           conv4_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_conv (Conv2D)   (None, 14, 14, 128)  73728       conv4_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_concat (Concatena (None, 14, 14, 608)  0           conv4_block10_concat[0][0]       \n",
      "                                                                 conv4_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_0_bn (BatchNormal (None, 14, 14, 608)  2432        conv4_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_0_relu (Activatio (None, 14, 14, 608)  0           conv4_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_conv (Conv2D)   (None, 14, 14, 128)  77824       conv4_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_concat (Concatena (None, 14, 14, 640)  0           conv4_block11_concat[0][0]       \n",
      "                                                                 conv4_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_0_bn (BatchNormal (None, 14, 14, 640)  2560        conv4_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_0_relu (Activatio (None, 14, 14, 640)  0           conv4_block13_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_conv (Conv2D)   (None, 14, 14, 128)  81920       conv4_block13_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_concat (Concatena (None, 14, 14, 672)  0           conv4_block12_concat[0][0]       \n",
      "                                                                 conv4_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_0_bn (BatchNormal (None, 14, 14, 672)  2688        conv4_block13_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_0_relu (Activatio (None, 14, 14, 672)  0           conv4_block14_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_conv (Conv2D)   (None, 14, 14, 128)  86016       conv4_block14_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_concat (Concatena (None, 14, 14, 704)  0           conv4_block13_concat[0][0]       \n",
      "                                                                 conv4_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_0_bn (BatchNormal (None, 14, 14, 704)  2816        conv4_block14_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_0_relu (Activatio (None, 14, 14, 704)  0           conv4_block15_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_conv (Conv2D)   (None, 14, 14, 128)  90112       conv4_block15_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_concat (Concatena (None, 14, 14, 736)  0           conv4_block14_concat[0][0]       \n",
      "                                                                 conv4_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_0_bn (BatchNormal (None, 14, 14, 736)  2944        conv4_block15_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_0_relu (Activatio (None, 14, 14, 736)  0           conv4_block16_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_conv (Conv2D)   (None, 14, 14, 128)  94208       conv4_block16_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_concat (Concatena (None, 14, 14, 768)  0           conv4_block15_concat[0][0]       \n",
      "                                                                 conv4_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_0_bn (BatchNormal (None, 14, 14, 768)  3072        conv4_block16_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_0_relu (Activatio (None, 14, 14, 768)  0           conv4_block17_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_conv (Conv2D)   (None, 14, 14, 128)  98304       conv4_block17_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block17_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block17_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block17_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_concat (Concatena (None, 14, 14, 800)  0           conv4_block16_concat[0][0]       \n",
      "                                                                 conv4_block17_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_0_bn (BatchNormal (None, 14, 14, 800)  3200        conv4_block17_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_0_relu (Activatio (None, 14, 14, 800)  0           conv4_block18_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_conv (Conv2D)   (None, 14, 14, 128)  102400      conv4_block18_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block18_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block18_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block18_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_concat (Concatena (None, 14, 14, 832)  0           conv4_block17_concat[0][0]       \n",
      "                                                                 conv4_block18_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_0_bn (BatchNormal (None, 14, 14, 832)  3328        conv4_block18_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_0_relu (Activatio (None, 14, 14, 832)  0           conv4_block19_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_conv (Conv2D)   (None, 14, 14, 128)  106496      conv4_block19_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block19_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block19_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block19_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_concat (Concatena (None, 14, 14, 864)  0           conv4_block18_concat[0][0]       \n",
      "                                                                 conv4_block19_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_0_bn (BatchNormal (None, 14, 14, 864)  3456        conv4_block19_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_0_relu (Activatio (None, 14, 14, 864)  0           conv4_block20_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_conv (Conv2D)   (None, 14, 14, 128)  110592      conv4_block20_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block20_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block20_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block20_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_concat (Concatena (None, 14, 14, 896)  0           conv4_block19_concat[0][0]       \n",
      "                                                                 conv4_block20_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_0_bn (BatchNormal (None, 14, 14, 896)  3584        conv4_block20_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_0_relu (Activatio (None, 14, 14, 896)  0           conv4_block21_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_conv (Conv2D)   (None, 14, 14, 128)  114688      conv4_block21_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block21_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block21_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block21_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_concat (Concatena (None, 14, 14, 928)  0           conv4_block20_concat[0][0]       \n",
      "                                                                 conv4_block21_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_0_bn (BatchNormal (None, 14, 14, 928)  3712        conv4_block21_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_0_relu (Activatio (None, 14, 14, 928)  0           conv4_block22_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_conv (Conv2D)   (None, 14, 14, 128)  118784      conv4_block22_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block22_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block22_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block22_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_concat (Concatena (None, 14, 14, 960)  0           conv4_block21_concat[0][0]       \n",
      "                                                                 conv4_block22_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_0_bn (BatchNormal (None, 14, 14, 960)  3840        conv4_block22_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_0_relu (Activatio (None, 14, 14, 960)  0           conv4_block23_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_conv (Conv2D)   (None, 14, 14, 128)  122880      conv4_block23_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block23_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block23_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block23_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_concat (Concatena (None, 14, 14, 992)  0           conv4_block22_concat[0][0]       \n",
      "                                                                 conv4_block23_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_0_bn (BatchNormal (None, 14, 14, 992)  3968        conv4_block23_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_0_relu (Activatio (None, 14, 14, 992)  0           conv4_block24_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_conv (Conv2D)   (None, 14, 14, 128)  126976      conv4_block24_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block24_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block24_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block24_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_concat (Concatena (None, 14, 14, 1024) 0           conv4_block23_concat[0][0]       \n",
      "                                                                 conv4_block24_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool4_bn (BatchNormalization)   (None, 14, 14, 1024) 4096        conv4_block24_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool4_relu (Activation)         (None, 14, 14, 1024) 0           pool4_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool4_conv (Conv2D)             (None, 14, 14, 512)  524288      pool4_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool4_pool (AveragePooling2D)   (None, 7, 7, 512)    0           pool4_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 512)    2048        pool4_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_relu (Activation (None, 7, 7, 512)    0           conv5_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 128)    65536       conv5_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 128)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_concat (Concatenat (None, 7, 7, 544)    0           pool4_pool[0][0]                 \n",
      "                                                                 conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_0_bn (BatchNormali (None, 7, 7, 544)    2176        conv5_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_0_relu (Activation (None, 7, 7, 544)    0           conv5_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 128)    69632       conv5_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 128)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_concat (Concatenat (None, 7, 7, 576)    0           conv5_block1_concat[0][0]        \n",
      "                                                                 conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_0_bn (BatchNormali (None, 7, 7, 576)    2304        conv5_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_0_relu (Activation (None, 7, 7, 576)    0           conv5_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 128)    73728       conv5_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 128)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_concat (Concatenat (None, 7, 7, 608)    0           conv5_block2_concat[0][0]        \n",
      "                                                                 conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_0_bn (BatchNormali (None, 7, 7, 608)    2432        conv5_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_0_relu (Activation (None, 7, 7, 608)    0           conv5_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_conv (Conv2D)    (None, 7, 7, 128)    77824       conv5_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_relu (Activation (None, 7, 7, 128)    0           conv5_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_concat (Concatenat (None, 7, 7, 640)    0           conv5_block3_concat[0][0]        \n",
      "                                                                 conv5_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_0_bn (BatchNormali (None, 7, 7, 640)    2560        conv5_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_0_relu (Activation (None, 7, 7, 640)    0           conv5_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_conv (Conv2D)    (None, 7, 7, 128)    81920       conv5_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_relu (Activation (None, 7, 7, 128)    0           conv5_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_concat (Concatenat (None, 7, 7, 672)    0           conv5_block4_concat[0][0]        \n",
      "                                                                 conv5_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_0_bn (BatchNormali (None, 7, 7, 672)    2688        conv5_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_0_relu (Activation (None, 7, 7, 672)    0           conv5_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_conv (Conv2D)    (None, 7, 7, 128)    86016       conv5_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_relu (Activation (None, 7, 7, 128)    0           conv5_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_concat (Concatenat (None, 7, 7, 704)    0           conv5_block5_concat[0][0]        \n",
      "                                                                 conv5_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_0_bn (BatchNormali (None, 7, 7, 704)    2816        conv5_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_0_relu (Activation (None, 7, 7, 704)    0           conv5_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_conv (Conv2D)    (None, 7, 7, 128)    90112       conv5_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_relu (Activation (None, 7, 7, 128)    0           conv5_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_concat (Concatenat (None, 7, 7, 736)    0           conv5_block6_concat[0][0]        \n",
      "                                                                 conv5_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_0_bn (BatchNormali (None, 7, 7, 736)    2944        conv5_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_0_relu (Activation (None, 7, 7, 736)    0           conv5_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_conv (Conv2D)    (None, 7, 7, 128)    94208       conv5_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_relu (Activation (None, 7, 7, 128)    0           conv5_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_concat (Concatenat (None, 7, 7, 768)    0           conv5_block7_concat[0][0]        \n",
      "                                                                 conv5_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_0_bn (BatchNormali (None, 7, 7, 768)    3072        conv5_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_0_relu (Activation (None, 7, 7, 768)    0           conv5_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_conv (Conv2D)    (None, 7, 7, 128)    98304       conv5_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_relu (Activation (None, 7, 7, 128)    0           conv5_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_concat (Concatenat (None, 7, 7, 800)    0           conv5_block8_concat[0][0]        \n",
      "                                                                 conv5_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_0_bn (BatchNormal (None, 7, 7, 800)    3200        conv5_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_0_relu (Activatio (None, 7, 7, 800)    0           conv5_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_conv (Conv2D)   (None, 7, 7, 128)    102400      conv5_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_concat (Concatena (None, 7, 7, 832)    0           conv5_block9_concat[0][0]        \n",
      "                                                                 conv5_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_0_bn (BatchNormal (None, 7, 7, 832)    3328        conv5_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_0_relu (Activatio (None, 7, 7, 832)    0           conv5_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_conv (Conv2D)   (None, 7, 7, 128)    106496      conv5_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_concat (Concatena (None, 7, 7, 864)    0           conv5_block10_concat[0][0]       \n",
      "                                                                 conv5_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_0_bn (BatchNormal (None, 7, 7, 864)    3456        conv5_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_0_relu (Activatio (None, 7, 7, 864)    0           conv5_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_conv (Conv2D)   (None, 7, 7, 128)    110592      conv5_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_concat (Concatena (None, 7, 7, 896)    0           conv5_block11_concat[0][0]       \n",
      "                                                                 conv5_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_0_bn (BatchNormal (None, 7, 7, 896)    3584        conv5_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_0_relu (Activatio (None, 7, 7, 896)    0           conv5_block13_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_conv (Conv2D)   (None, 7, 7, 128)    114688      conv5_block13_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_concat (Concatena (None, 7, 7, 928)    0           conv5_block12_concat[0][0]       \n",
      "                                                                 conv5_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_0_bn (BatchNormal (None, 7, 7, 928)    3712        conv5_block13_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_0_relu (Activatio (None, 7, 7, 928)    0           conv5_block14_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_conv (Conv2D)   (None, 7, 7, 128)    118784      conv5_block14_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_concat (Concatena (None, 7, 7, 960)    0           conv5_block13_concat[0][0]       \n",
      "                                                                 conv5_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_0_bn (BatchNormal (None, 7, 7, 960)    3840        conv5_block14_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_0_relu (Activatio (None, 7, 7, 960)    0           conv5_block15_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_conv (Conv2D)   (None, 7, 7, 128)    122880      conv5_block15_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_concat (Concatena (None, 7, 7, 992)    0           conv5_block14_concat[0][0]       \n",
      "                                                                 conv5_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_0_bn (BatchNormal (None, 7, 7, 992)    3968        conv5_block15_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_0_relu (Activatio (None, 7, 7, 992)    0           conv5_block16_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_conv (Conv2D)   (None, 7, 7, 128)    126976      conv5_block16_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_concat (Concatena (None, 7, 7, 1024)   0           conv5_block15_concat[0][0]       \n",
      "                                                                 conv5_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bn (BatchNormalization)         (None, 7, 7, 1024)   4096        conv5_block16_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "relu (Activation)               (None, 7, 7, 1024)   0           bn[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 1024)         0           relu[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            1025        global_average_pooling2d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 7,038,529\n",
      "Trainable params: 6,954,881\n",
      "Non-trainable params: 83,648\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "#keras.applications.densenet.DenseNet121(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n",
    "from tensorflow.keras.applications.densenet import DenseNet121\n",
    "#from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.densenet import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Activation\n",
    "\n",
    "def Densenet121(show_layers,weights,input_shape):\n",
    "        base_model = DenseNet121(include_top=False, weights=weights, input_shape=input_shape)\n",
    "        x = GlobalAveragePooling2D()(base_model.output)\n",
    "        pred = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "        model = Model(inputs=base_model.input, outputs=pred)\n",
    "        model.compile(optimizer=SGD(lr=1e-2, decay=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "        if show_layers:\n",
    "            for i, layer in enumerate(model.layers):\n",
    "                print(i, layer.name, layer.trainable)\n",
    "        return model\n",
    "    \n",
    "model = Densenet121(False,weights=None,input_shape=(224, 224, 3))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>  protocol_4\n",
      "============================================Densenet121=============================================\n",
      "Found 41040 images belonging to 2 classes.\n",
      "Found 30240 images belonging to 2 classes.\n",
      "train shape : (41040,)\n",
      "Epoch 1/100\n",
      " 478/5130 [=>............................] - ETA: 44:46 - loss: 0.4314 - accuracy: 0.8036"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 4.59 MiB for an array with shape (8, 224, 224, 3) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-76a2b0808bba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-76a2b0808bba>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     79\u001b[0m                                \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                                \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                                callbacks=[early_stop, cb_checkpoint])\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1297\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m   1298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtarget_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m       \u001b[0mbatch_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_next_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36m_get_next_batch\u001b[1;34m(generator)\u001b[0m\n\u001b[0;32m    361\u001b[0m   \u001b[1;34m\"\"\"Retrieves the next batch of input data.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m     \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 785\u001b[1;33m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    694\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    697\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    777\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    642\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mworker\u001b[1;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0mjob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\site-packages\\tensorflow_core\\python\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36mget_index\u001b[1;34m(uid, i)\u001b[0m\n\u001b[0;32m    569\u001b[0m       \u001b[0mThe\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mat\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mi\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m   \"\"\"\n\u001b[1;32m--> 571\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0m_SHARED_SEQUENCES\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    572\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     63\u001b[0m         index_array = self.index_array[self.batch_size * idx:\n\u001b[0;32m     64\u001b[0m                                        self.batch_size * (idx + 1)]\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\site-packages\\keras_preprocessing\\image\\iterator.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[1;34m(self, index_array)\u001b[0m\n\u001b[0;32m    220\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtransformed\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         \u001b[0mbatch_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m         \u001b[1;31m# build batch of image data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[1;31m# self.filepaths is dynamic, is better to call it once outside the loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 4.59 MiB for an array with shape (8, 224, 224, 3) and data type float32"
     ]
    }
   ],
   "source": [
    "#### import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "%matplotlib inline\n",
    "SEED = 777\n",
    "\n",
    "def main():\n",
    "    inputSize = 224\n",
    "    batchSize = 8\n",
    "    nEpoch = 100\n",
    "    \n",
    "    # 디렉토리 정보 설정\n",
    "    # dataDir = 'D:\\\\Face_Database\\\\B-Database'\n",
    "    dataDir = 'E:\\\\Face_Database\\\\B-Database'   \n",
    "    # 학습 및 테스트 할 데이터베이스 디렉토리명\n",
    "    trainDB = 'protocol_4'  # D:\\Face_Database\\B-Database\\protocol_4\\train\n",
    "    validDB = 'protocol_4'\n",
    "    \n",
    "    saveDir =  '.\\\\result_cuda'\n",
    "    if not os.path.exists(saveDir):    \n",
    "        os.makedirs(saveDir)\n",
    "    model_path = os.path.join(saveDir, trainDB + '-{epoch:02d}-{val_loss:.4f}.hdf5')\n",
    "    \n",
    "    # 데이터베이스별 학습 수행\n",
    "    print('>> ', trainDB)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    print('Densenet121'.center(100, '='))\n",
    "   \n",
    "    '''\n",
    "    training\n",
    "    '''\n",
    "    K.clear_session()\n",
    "\n",
    "    # 네트워크 정의\n",
    "    model = Densenet121(False,weights=None,input_shape=(224, 224, 3))\n",
    "\n",
    "    # 데이터 generator 생성\n",
    "    train_datagen = ImageDataGenerator(rescale=1.0/255.0,\n",
    "                                       horizontal_flip=True,)#이미지augmentation > 이미지를 회전,확대,축소 등 여러변형해보는것.\n",
    "    train_generator = train_datagen.flow_from_directory(os.path.join(*[dataDir,trainDB,'train']),\n",
    "                                                        target_size=(inputSize, inputSize),\n",
    "                                                        batch_size=batchSize,\n",
    "                                                        class_mode='binary',\n",
    "                                                        interpolation='bilinear',\n",
    "                                                        shuffle=True,)\n",
    "\n",
    "    valid_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    valid_generator = valid_datagen.flow_from_directory(os.path.join(*[dataDir, validDB,'val']),\n",
    "                                                        target_size=(inputSize, inputSize),\n",
    "                                                        batch_size=batchSize,\n",
    "                                                        class_mode='binary',\n",
    "                                                        interpolation='bilinear',\n",
    "                                                        shuffle=False,)\n",
    "\n",
    "    print('train shape :',train_generator.classes.shape)\n",
    "    # unbalanced class를 해결하기 위한 class_weight 설정 #np.unique는 중복원소제거하는거\n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.unique(train_generator.classes), train_generator.classes) \n",
    "\n",
    "    # callback 함수 정의 dropout함수\n",
    "    early_stop = EarlyStopping(patience=10, monitor='val_loss')\n",
    "    cb_checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "    # 모델 학습\n",
    "    hist = model.fit_generator(train_generator, \n",
    "                               epochs=nEpoch,\n",
    "                               steps_per_epoch=len(train_generator),\n",
    "                               class_weight=class_weights,\n",
    "                               validation_data=valid_generator,\n",
    "                               validation_steps=len(valid_generator),\n",
    "                               verbose=1,\n",
    "                               callbacks=[early_stop, cb_checkpoint])\n",
    "\n",
    "    \n",
    "    # 학습 결과 그래프 그리기\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, loss_ax = plt.subplots()\n",
    "    acc_ax = loss_ax.twinx()\n",
    "\n",
    "    loss_ax.plot(hist.history['loss'], 'b', label='train loss')\n",
    "    loss_ax.plot(hist.history['val_loss'], '--r', label='val loss')\n",
    "    loss_ax.set_xlabel('epoch')\n",
    "    loss_ax.set_ylabel('loss')\n",
    "    loss_ax.legend(loc='upper left')\n",
    "\n",
    "    plt.show()\n",
    "    print('acc: {:.4f}\\tloss: {:.4f}'.format(hist.history['val_accuracy'][-11], hist.history['val_loss'][-11]))\n",
    "\n",
    "    print(('{} finished'.format(trainDB)).center(100,'='))\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> model loaded: protocol_4-16-0.1216_0.01.hdf5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2382\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_buffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2383\u001b[1;33m         \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_OperationGetAttrValueProto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2384\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Operation 'conv5_block8_0_bn/cond' has no attr named '_XlaCompile'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\site-packages\\tensorflow_core\\python\\ops\\gradients_util.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[1;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m       \u001b[0mxla_compile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_XlaCompile\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m       xla_separate_compiled_gradients = op.get_attr(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2386\u001b[0m       \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2387\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2388\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Operation 'conv5_block8_0_bn/cond' has no attr named '_XlaCompile'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-82c3ebe1ae7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-82c3ebe1ae7d>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'>> model loaded: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\">>>> evaluating on '{}'\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestDB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    144\u001b[0m   if (h5py is not None and (\n\u001b[0;32m    145\u001b[0m       isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[1;32m--> 146\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mhdf5_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model_from_hdf5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[1;31m# weights.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m           \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m           \u001b[0moptimizer_weight_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_optimizer_weights_from_hdf5_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m           \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2114\u001b[0m           \u001b[1;31m# Training updates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2115\u001b[0m           updates = self.optimizer.get_updates(\n\u001b[1;32m-> 2116\u001b[1;33m               params=self._collected_trainable_weights, loss=self.total_loss)\n\u001b[0m\u001b[0;32m   2117\u001b[0m           \u001b[1;31m# Unconditional updates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2118\u001b[0m           \u001b[0mupdates\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_updates_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mget_updates\u001b[1;34m(self, loss, params)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_updates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 500\u001b[1;33m     \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    501\u001b[0m     \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m     self._assert_valid_dtypes([\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mget_gradients\u001b[1;34m(self, loss, params)\u001b[0m\n\u001b[0;32m    389\u001b[0m     with backend.get_graph().as_default(), backend.name_scope(self._name +\n\u001b[0;32m    390\u001b[0m                                                               \"/gradients\"):\n\u001b[1;32m--> 391\u001b[1;33m       \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\site-packages\\tensorflow_core\\python\\ops\\gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[1;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[0mys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_ys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[0mgate_gradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maggregation_method\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m         unconnected_gradients)\n\u001b[0m\u001b[0;32m    159\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\site-packages\\tensorflow_core\\python\\ops\\gradients_util.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[1;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[0;32m    677\u001b[0m                 \u001b[1;31m# functions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[1;32m--> 679\u001b[1;33m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[0;32m    680\u001b[0m               \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m                 \u001b[1;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\site-packages\\tensorflow_core\\python\\ops\\gradients_util.py\u001b[0m in \u001b[0;36m_MaybeCompile\u001b[1;34m(scope, op, func, grad_fn)\u001b[0m\n\u001b[0;32m    348\u001b[0m       \u001b[0mxla_scope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_XlaScope\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Exit early\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mxla_compile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\site-packages\\tensorflow_core\\python\\ops\\gradients_util.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    677\u001b[0m                 \u001b[1;31m# functions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n\u001b[1;32m--> 679\u001b[1;33m                                          lambda: grad_fn(op, *out_grads))\n\u001b[0m\u001b[0;32m    680\u001b[0m               \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m                 \u001b[1;31m# For function call ops, we add a 'SymbolicGradient'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\site-packages\\tensorflow_core\\python\\ops\\cond_v2.py\u001b[0m in \u001b[0;36m_IfGrad\u001b[1;34m(op, *grads)\u001b[0m\n\u001b[0;32m    108\u001b[0m   \u001b[1;31m# Get the if operator (this logic handles the case where op is a MockOp)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m   \u001b[0mif_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m   \u001b[0mtrue_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfalse_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_func_graphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mif_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m   \u001b[1;31m# Note: op.graph != ops.get_default_graph() when we are computing the gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m   \u001b[1;31m# of a nested cond.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\site-packages\\tensorflow_core\\python\\ops\\cond_v2.py\u001b[0m in \u001b[0;36mget_func_graphs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"If\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"StatelessIf\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m     return (_get_func_graph_for_branch(op.get_attr(\"then_branch\")),\n\u001b[0m\u001b[0;32m    332\u001b[0m             _get_func_graph_for_branch(op.get_attr(\"else_branch\")))\n\u001b[0;32m    333\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Case\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\site-packages\\tensorflow_core\\python\\ops\\cond_v2.py\u001b[0m in \u001b[0;36m_get_func_graph_for_branch\u001b[1;34m(name_attr_list)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m       func_graph = function_def_to_graph.function_def_to_graph(\n\u001b[1;32m--> 322\u001b[1;33m           fdef, input_shapes)\n\u001b[0m\u001b[0;32m    323\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mexternal_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minternal_t\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m       \u001b[0mcustom_gradient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_handle_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexternal_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minternal_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\site-packages\\tensorflow_core\\python\\framework\\function_def_to_graph.py\u001b[0m in \u001b[0;36mfunction_def_to_graph\u001b[1;34m(fdef, input_shapes, copy_functions)\u001b[0m\n\u001b[0;32m     63\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;31m# Add all function nodes to the graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m     \u001b[0mimporter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_graph_def_for_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;31m# Initialize fields specific to FuncGraph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\site-packages\\tensorflow_core\\python\\framework\\importer.py\u001b[0m in \u001b[0;36mimport_graph_def_for_function\u001b[1;34m(graph_def, name)\u001b[0m\n\u001b[0;32m    410\u001b[0m   \u001b[1;34m\"\"\"Like import_graph_def but does not validate colocation constraints.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m   return _import_graph_def_internal(\n\u001b[1;32m--> 412\u001b[1;33m       graph_def, validate_colocation_constraints=False, name=name)\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\site-packages\\tensorflow_core\\python\\framework\\importer.py\u001b[0m in \u001b[0;36m_import_graph_def_internal\u001b[1;34m(graph_def, input_map, return_elements, validate_colocation_constraints, name, op_dict, producer_op_list)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlibrary\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m     \u001b[0mfunctions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_library\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m       \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\site-packages\\tensorflow_core\\python\\framework\\function.py\u001b[0m in \u001b[0;36mfrom_library\u001b[1;34m(lib)\u001b[0m\n\u001b[0;32m   1165\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfunc_to_grad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1166\u001b[0m       \u001b[1;32massert\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1167\u001b[1;33m     \u001b[0mdefined_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_from_definition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfdef\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1168\u001b[0m     \u001b[0minitialized\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefined_func\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cuda\\lib\\site-packages\\tensorflow_core\\python\\framework\\function.py\u001b[0m in \u001b[0;36m_from_definition\u001b[1;34m(fdef, grad_func)\u001b[0m\n\u001b[0;32m   1100\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m   \u001b[0mserialized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfdef\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1102\u001b[1;33m   \u001b[0mc_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FunctionImportFunctionDef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m   \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mScopedTFFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m   \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extra_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 학습된 모델 테스트셋에서 성능 평가\n",
    "# 얼굴 스푸핑 분야에서 평가 metric으로 HTER이랑 EER 두 개 주로 사용\n",
    "# =========================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def main():\n",
    "    inputSize = 224\n",
    "    batchSize = 8\n",
    "    \n",
    "    trainDB = 'protocol_4'\n",
    "    testDB = 'protocol_4'\n",
    "    \n",
    "    dataDir = 'E:\\\\Face_Database\\\\B-Database'\n",
    "    modelPath = 'F:\\\\prlab\\\\ysg\\\\densenet-spoofing\\\\Densenet_result\\\\0.001\\\\result_PR-FASD\\\\protocol_4-16-0.1216_0.01.hdf5'\n",
    "    \n",
    "    print('>> model loaded: {}'.format(os.path.basename(modelPath)))\n",
    "    K.clear_session()\n",
    "    model = load_model(modelPath)\n",
    "        \n",
    "    print(\">>>> evaluating on '{}'\".format(testDB))\n",
    "    test_datagen = ImageDataGenerator()\n",
    "\n",
    "    val_datagen = ImageDataGenerator()\n",
    "    \n",
    "    val_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    val_generator = val_datagen.flow_from_directory(os.path.join(*[dataDir, testDB, 'val']),\n",
    "                                                      target_size=(inputSize, inputSize),\n",
    "                                                      batch_size=batchSize,\n",
    "                                                      class_mode='binary',\n",
    "                                                      interpolation='bilinear',\n",
    "                                                      shuffle=False,)\n",
    "    \n",
    "    test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    test_generator = test_datagen.flow_from_directory(os.path.join(*[dataDir, testDB, 'test']),\n",
    "                                                      target_size=(inputSize, inputSize),\n",
    "                                                      batch_size=batchSize,\n",
    "                                                      class_mode='binary',\n",
    "                                                      interpolation='bilinear',\n",
    "                                                      shuffle=False,)\n",
    "\n",
    "    ''' evaluating EER '''\n",
    "    y_true = val_generator.classes\n",
    "    y_score = model.predict_generator(val_generator, steps=len(val_generator)).ravel()\n",
    "\n",
    "    \n",
    "    fpr, tpr, threshold = roc_curve(y_true, y_score)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr-fpr)))]\n",
    "    val_eer = (fpr[np.nanargmin(np.absolute((fnr - fpr)))] + fnr[np.nanargmin(np.absolute((fnr - fpr)))]) / 2\n",
    "\n",
    "    ''' evaluating HTER '''\n",
    "    y_true = test_generator.classes\n",
    "    y_score = model.predict_generator(test_generator, steps = len(test_generator)).ravel()\n",
    "\n",
    "    # Calculate EER threshold\n",
    "    fpr, tpr, threshold = roc_curve(y_true, y_score)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr-fpr)))]\n",
    "\n",
    "    # HTER\n",
    "    y_pred = y_score > eer_threshold\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    labels = test_generator.class_indices\n",
    "    print('                  pred_fake({})   pred_real({})\\nactural_fake({})    {:12d}   {:12d}\\nactual_real({})    {:12d}   {:12d}\\n'.format(labels['Fake'], labels['Real'], labels['Fake'], tn, fp, labels['Real'], fn, tp))\n",
    "    hter = (fp/(tn+fp) + fn/(fn+tp)) * 0.5\n",
    "\n",
    "    # ROC curve\n",
    "    Accuracy = ((tn+tp) / (tn+fp+fn+tp)) * 100.0\n",
    "\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "\n",
    "    plt.plot(fpr, tpr, 'r--', label='ROC_curve (acc_1st = %0.2f%%)' % Accuracy)\n",
    "    plt.plot([0, 1], [0, 1], color = 'orange', lw=lw, linestyle='--')\n",
    "\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc = \"lower right\")\n",
    "\n",
    "    plt.savefig('protocol_4_001.png')\n",
    "    plt.figure()\n",
    "    \n",
    "    print('EER: {:.4f}\\tHTER: {:.4f}'.format(val_eer, hter))\n",
    "\n",
    "    print('>> finished')\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 230, 230, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1/conv (Conv2D)             (None, 112, 112, 64) 9408        zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1/bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1/conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1/relu (Activation)         (None, 112, 112, 64) 0           conv1/bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 114, 114, 64) 0           conv1/relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 56, 56, 64)   0           zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 56, 56, 64)   256         pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_relu (Activation (None, 56, 56, 64)   0           conv2_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 128)  8192        conv2_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 128)  0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_concat (Concatenat (None, 56, 56, 96)   0           pool1[0][0]                      \n",
      "                                                                 conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_bn (BatchNormali (None, 56, 56, 96)   384         conv2_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_relu (Activation (None, 56, 56, 96)   0           conv2_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 128)  12288       conv2_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 128)  0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_concat (Concatenat (None, 56, 56, 128)  0           conv2_block1_concat[0][0]        \n",
      "                                                                 conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_0_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_0_relu (Activation (None, 56, 56, 128)  0           conv2_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 128)  16384       conv2_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 128)  0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_concat (Concatenat (None, 56, 56, 160)  0           conv2_block2_concat[0][0]        \n",
      "                                                                 conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_0_bn (BatchNormali (None, 56, 56, 160)  640         conv2_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_0_relu (Activation (None, 56, 56, 160)  0           conv2_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_conv (Conv2D)    (None, 56, 56, 128)  20480       conv2_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_relu (Activation (None, 56, 56, 128)  0           conv2_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_concat (Concatenat (None, 56, 56, 192)  0           conv2_block3_concat[0][0]        \n",
      "                                                                 conv2_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_0_bn (BatchNormali (None, 56, 56, 192)  768         conv2_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_0_relu (Activation (None, 56, 56, 192)  0           conv2_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_conv (Conv2D)    (None, 56, 56, 128)  24576       conv2_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_relu (Activation (None, 56, 56, 128)  0           conv2_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_concat (Concatenat (None, 56, 56, 224)  0           conv2_block4_concat[0][0]        \n",
      "                                                                 conv2_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_0_bn (BatchNormali (None, 56, 56, 224)  896         conv2_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_0_relu (Activation (None, 56, 56, 224)  0           conv2_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_conv (Conv2D)    (None, 56, 56, 128)  28672       conv2_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_relu (Activation (None, 56, 56, 128)  0           conv2_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_concat (Concatenat (None, 56, 56, 256)  0           conv2_block5_concat[0][0]        \n",
      "                                                                 conv2_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool2_bn (BatchNormalization)   (None, 56, 56, 256)  1024        conv2_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool2_relu (Activation)         (None, 56, 56, 256)  0           pool2_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool2_conv (Conv2D)             (None, 56, 56, 128)  32768       pool2_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool2_pool (AveragePooling2D)   (None, 28, 28, 128)  0           pool2_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 28, 28, 128)  512         pool2_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_relu (Activation (None, 28, 28, 128)  0           conv3_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  16384       conv3_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_concat (Concatenat (None, 28, 28, 160)  0           pool2_pool[0][0]                 \n",
      "                                                                 conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_bn (BatchNormali (None, 28, 28, 160)  640         conv3_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_relu (Activation (None, 28, 28, 160)  0           conv3_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  20480       conv3_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_concat (Concatenat (None, 28, 28, 192)  0           conv3_block1_concat[0][0]        \n",
      "                                                                 conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_0_bn (BatchNormali (None, 28, 28, 192)  768         conv3_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_0_relu (Activation (None, 28, 28, 192)  0           conv3_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  24576       conv3_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_concat (Concatenat (None, 28, 28, 224)  0           conv3_block2_concat[0][0]        \n",
      "                                                                 conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_0_bn (BatchNormali (None, 28, 28, 224)  896         conv3_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_0_relu (Activation (None, 28, 28, 224)  0           conv3_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  28672       conv3_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_concat (Concatenat (None, 28, 28, 256)  0           conv3_block3_concat[0][0]        \n",
      "                                                                 conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_0_bn (BatchNormali (None, 28, 28, 256)  1024        conv3_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_0_relu (Activation (None, 28, 28, 256)  0           conv3_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_conv (Conv2D)    (None, 28, 28, 128)  32768       conv3_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_relu (Activation (None, 28, 28, 128)  0           conv3_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_concat (Concatenat (None, 28, 28, 288)  0           conv3_block4_concat[0][0]        \n",
      "                                                                 conv3_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_0_bn (BatchNormali (None, 28, 28, 288)  1152        conv3_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_0_relu (Activation (None, 28, 28, 288)  0           conv3_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_conv (Conv2D)    (None, 28, 28, 128)  36864       conv3_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_relu (Activation (None, 28, 28, 128)  0           conv3_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_concat (Concatenat (None, 28, 28, 320)  0           conv3_block5_concat[0][0]        \n",
      "                                                                 conv3_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_0_bn (BatchNormali (None, 28, 28, 320)  1280        conv3_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_0_relu (Activation (None, 28, 28, 320)  0           conv3_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_conv (Conv2D)    (None, 28, 28, 128)  40960       conv3_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_relu (Activation (None, 28, 28, 128)  0           conv3_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_concat (Concatenat (None, 28, 28, 352)  0           conv3_block6_concat[0][0]        \n",
      "                                                                 conv3_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_0_bn (BatchNormali (None, 28, 28, 352)  1408        conv3_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_0_relu (Activation (None, 28, 28, 352)  0           conv3_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_conv (Conv2D)    (None, 28, 28, 128)  45056       conv3_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_relu (Activation (None, 28, 28, 128)  0           conv3_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_concat (Concatenat (None, 28, 28, 384)  0           conv3_block7_concat[0][0]        \n",
      "                                                                 conv3_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_0_bn (BatchNormali (None, 28, 28, 384)  1536        conv3_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_0_relu (Activation (None, 28, 28, 384)  0           conv3_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_conv (Conv2D)    (None, 28, 28, 128)  49152       conv3_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_relu (Activation (None, 28, 28, 128)  0           conv3_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_concat (Concatenat (None, 28, 28, 416)  0           conv3_block8_concat[0][0]        \n",
      "                                                                 conv3_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_0_bn (BatchNormal (None, 28, 28, 416)  1664        conv3_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_0_relu (Activatio (None, 28, 28, 416)  0           conv3_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_conv (Conv2D)   (None, 28, 28, 128)  53248       conv3_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_concat (Concatena (None, 28, 28, 448)  0           conv3_block9_concat[0][0]        \n",
      "                                                                 conv3_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_0_bn (BatchNormal (None, 28, 28, 448)  1792        conv3_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_0_relu (Activatio (None, 28, 28, 448)  0           conv3_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_conv (Conv2D)   (None, 28, 28, 128)  57344       conv3_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_concat (Concatena (None, 28, 28, 480)  0           conv3_block10_concat[0][0]       \n",
      "                                                                 conv3_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_0_bn (BatchNormal (None, 28, 28, 480)  1920        conv3_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_0_relu (Activatio (None, 28, 28, 480)  0           conv3_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_conv (Conv2D)   (None, 28, 28, 128)  61440       conv3_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_concat (Concatena (None, 28, 28, 512)  0           conv3_block11_concat[0][0]       \n",
      "                                                                 conv3_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3_bn (BatchNormalization)   (None, 28, 28, 512)  2048        conv3_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3_relu (Activation)         (None, 28, 28, 512)  0           pool3_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool3_conv (Conv2D)             (None, 28, 28, 256)  131072      pool3_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool3_pool (AveragePooling2D)   (None, 14, 14, 256)  0           pool3_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 256)  1024        pool3_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_relu (Activation (None, 14, 14, 256)  0           conv4_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 128)  32768       conv4_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 128)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_concat (Concatenat (None, 14, 14, 288)  0           pool3_pool[0][0]                 \n",
      "                                                                 conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_bn (BatchNormali (None, 14, 14, 288)  1152        conv4_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_relu (Activation (None, 14, 14, 288)  0           conv4_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 128)  36864       conv4_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 128)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_concat (Concatenat (None, 14, 14, 320)  0           conv4_block1_concat[0][0]        \n",
      "                                                                 conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_0_bn (BatchNormali (None, 14, 14, 320)  1280        conv4_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_0_relu (Activation (None, 14, 14, 320)  0           conv4_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 128)  40960       conv4_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 128)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_concat (Concatenat (None, 14, 14, 352)  0           conv4_block2_concat[0][0]        \n",
      "                                                                 conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_0_bn (BatchNormali (None, 14, 14, 352)  1408        conv4_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_0_relu (Activation (None, 14, 14, 352)  0           conv4_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 128)  45056       conv4_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 128)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_concat (Concatenat (None, 14, 14, 384)  0           conv4_block3_concat[0][0]        \n",
      "                                                                 conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_0_bn (BatchNormali (None, 14, 14, 384)  1536        conv4_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_0_relu (Activation (None, 14, 14, 384)  0           conv4_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 128)  49152       conv4_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 128)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_concat (Concatenat (None, 14, 14, 416)  0           conv4_block4_concat[0][0]        \n",
      "                                                                 conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_0_bn (BatchNormali (None, 14, 14, 416)  1664        conv4_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_0_relu (Activation (None, 14, 14, 416)  0           conv4_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 128)  53248       conv4_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 128)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_concat (Concatenat (None, 14, 14, 448)  0           conv4_block5_concat[0][0]        \n",
      "                                                                 conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_0_bn (BatchNormali (None, 14, 14, 448)  1792        conv4_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_0_relu (Activation (None, 14, 14, 448)  0           conv4_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_conv (Conv2D)    (None, 14, 14, 128)  57344       conv4_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_relu (Activation (None, 14, 14, 128)  0           conv4_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_concat (Concatenat (None, 14, 14, 480)  0           conv4_block6_concat[0][0]        \n",
      "                                                                 conv4_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_0_bn (BatchNormali (None, 14, 14, 480)  1920        conv4_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_0_relu (Activation (None, 14, 14, 480)  0           conv4_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_conv (Conv2D)    (None, 14, 14, 128)  61440       conv4_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_relu (Activation (None, 14, 14, 128)  0           conv4_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_concat (Concatenat (None, 14, 14, 512)  0           conv4_block7_concat[0][0]        \n",
      "                                                                 conv4_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_0_bn (BatchNormali (None, 14, 14, 512)  2048        conv4_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_0_relu (Activation (None, 14, 14, 512)  0           conv4_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_conv (Conv2D)    (None, 14, 14, 128)  65536       conv4_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_relu (Activation (None, 14, 14, 128)  0           conv4_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_concat (Concatenat (None, 14, 14, 544)  0           conv4_block8_concat[0][0]        \n",
      "                                                                 conv4_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_0_bn (BatchNormal (None, 14, 14, 544)  2176        conv4_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_0_relu (Activatio (None, 14, 14, 544)  0           conv4_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_conv (Conv2D)   (None, 14, 14, 128)  69632       conv4_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_concat (Concatena (None, 14, 14, 576)  0           conv4_block9_concat[0][0]        \n",
      "                                                                 conv4_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_0_bn (BatchNormal (None, 14, 14, 576)  2304        conv4_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_0_relu (Activatio (None, 14, 14, 576)  0           conv4_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_conv (Conv2D)   (None, 14, 14, 128)  73728       conv4_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_concat (Concatena (None, 14, 14, 608)  0           conv4_block10_concat[0][0]       \n",
      "                                                                 conv4_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_0_bn (BatchNormal (None, 14, 14, 608)  2432        conv4_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_0_relu (Activatio (None, 14, 14, 608)  0           conv4_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_conv (Conv2D)   (None, 14, 14, 128)  77824       conv4_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_concat (Concatena (None, 14, 14, 640)  0           conv4_block11_concat[0][0]       \n",
      "                                                                 conv4_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_0_bn (BatchNormal (None, 14, 14, 640)  2560        conv4_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_0_relu (Activatio (None, 14, 14, 640)  0           conv4_block13_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_conv (Conv2D)   (None, 14, 14, 128)  81920       conv4_block13_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_concat (Concatena (None, 14, 14, 672)  0           conv4_block12_concat[0][0]       \n",
      "                                                                 conv4_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_0_bn (BatchNormal (None, 14, 14, 672)  2688        conv4_block13_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_0_relu (Activatio (None, 14, 14, 672)  0           conv4_block14_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_conv (Conv2D)   (None, 14, 14, 128)  86016       conv4_block14_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_concat (Concatena (None, 14, 14, 704)  0           conv4_block13_concat[0][0]       \n",
      "                                                                 conv4_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_0_bn (BatchNormal (None, 14, 14, 704)  2816        conv4_block14_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_0_relu (Activatio (None, 14, 14, 704)  0           conv4_block15_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_conv (Conv2D)   (None, 14, 14, 128)  90112       conv4_block15_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_concat (Concatena (None, 14, 14, 736)  0           conv4_block14_concat[0][0]       \n",
      "                                                                 conv4_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_0_bn (BatchNormal (None, 14, 14, 736)  2944        conv4_block15_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_0_relu (Activatio (None, 14, 14, 736)  0           conv4_block16_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_conv (Conv2D)   (None, 14, 14, 128)  94208       conv4_block16_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_concat (Concatena (None, 14, 14, 768)  0           conv4_block15_concat[0][0]       \n",
      "                                                                 conv4_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_0_bn (BatchNormal (None, 14, 14, 768)  3072        conv4_block16_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_0_relu (Activatio (None, 14, 14, 768)  0           conv4_block17_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_conv (Conv2D)   (None, 14, 14, 128)  98304       conv4_block17_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block17_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block17_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block17_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_concat (Concatena (None, 14, 14, 800)  0           conv4_block16_concat[0][0]       \n",
      "                                                                 conv4_block17_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_0_bn (BatchNormal (None, 14, 14, 800)  3200        conv4_block17_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_0_relu (Activatio (None, 14, 14, 800)  0           conv4_block18_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_conv (Conv2D)   (None, 14, 14, 128)  102400      conv4_block18_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block18_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block18_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block18_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_concat (Concatena (None, 14, 14, 832)  0           conv4_block17_concat[0][0]       \n",
      "                                                                 conv4_block18_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_0_bn (BatchNormal (None, 14, 14, 832)  3328        conv4_block18_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_0_relu (Activatio (None, 14, 14, 832)  0           conv4_block19_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_conv (Conv2D)   (None, 14, 14, 128)  106496      conv4_block19_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block19_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block19_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block19_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_concat (Concatena (None, 14, 14, 864)  0           conv4_block18_concat[0][0]       \n",
      "                                                                 conv4_block19_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_0_bn (BatchNormal (None, 14, 14, 864)  3456        conv4_block19_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_0_relu (Activatio (None, 14, 14, 864)  0           conv4_block20_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_conv (Conv2D)   (None, 14, 14, 128)  110592      conv4_block20_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block20_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block20_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block20_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_concat (Concatena (None, 14, 14, 896)  0           conv4_block19_concat[0][0]       \n",
      "                                                                 conv4_block20_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_0_bn (BatchNormal (None, 14, 14, 896)  3584        conv4_block20_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_0_relu (Activatio (None, 14, 14, 896)  0           conv4_block21_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_conv (Conv2D)   (None, 14, 14, 128)  114688      conv4_block21_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block21_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block21_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block21_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_concat (Concatena (None, 14, 14, 928)  0           conv4_block20_concat[0][0]       \n",
      "                                                                 conv4_block21_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_0_bn (BatchNormal (None, 14, 14, 928)  3712        conv4_block21_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_0_relu (Activatio (None, 14, 14, 928)  0           conv4_block22_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_conv (Conv2D)   (None, 14, 14, 128)  118784      conv4_block22_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block22_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block22_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block22_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_concat (Concatena (None, 14, 14, 960)  0           conv4_block21_concat[0][0]       \n",
      "                                                                 conv4_block22_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_0_bn (BatchNormal (None, 14, 14, 960)  3840        conv4_block22_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_0_relu (Activatio (None, 14, 14, 960)  0           conv4_block23_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_conv (Conv2D)   (None, 14, 14, 128)  122880      conv4_block23_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block23_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block23_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block23_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_concat (Concatena (None, 14, 14, 992)  0           conv4_block22_concat[0][0]       \n",
      "                                                                 conv4_block23_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_0_bn (BatchNormal (None, 14, 14, 992)  3968        conv4_block23_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_0_relu (Activatio (None, 14, 14, 992)  0           conv4_block24_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_conv (Conv2D)   (None, 14, 14, 128)  126976      conv4_block24_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block24_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block24_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block24_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_concat (Concatena (None, 14, 14, 1024) 0           conv4_block23_concat[0][0]       \n",
      "                                                                 conv4_block24_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool4_bn (BatchNormalization)   (None, 14, 14, 1024) 4096        conv4_block24_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool4_relu (Activation)         (None, 14, 14, 1024) 0           pool4_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool4_conv (Conv2D)             (None, 14, 14, 512)  524288      pool4_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool4_pool (AveragePooling2D)   (None, 7, 7, 512)    0           pool4_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 512)    2048        pool4_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_relu (Activation (None, 7, 7, 512)    0           conv5_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 128)    65536       conv5_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 128)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_concat (Concatenat (None, 7, 7, 544)    0           pool4_pool[0][0]                 \n",
      "                                                                 conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_0_bn (BatchNormali (None, 7, 7, 544)    2176        conv5_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_0_relu (Activation (None, 7, 7, 544)    0           conv5_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 128)    69632       conv5_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 128)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_concat (Concatenat (None, 7, 7, 576)    0           conv5_block1_concat[0][0]        \n",
      "                                                                 conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_0_bn (BatchNormali (None, 7, 7, 576)    2304        conv5_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_0_relu (Activation (None, 7, 7, 576)    0           conv5_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 128)    73728       conv5_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 128)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_concat (Concatenat (None, 7, 7, 608)    0           conv5_block2_concat[0][0]        \n",
      "                                                                 conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_0_bn (BatchNormali (None, 7, 7, 608)    2432        conv5_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_0_relu (Activation (None, 7, 7, 608)    0           conv5_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_conv (Conv2D)    (None, 7, 7, 128)    77824       conv5_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_relu (Activation (None, 7, 7, 128)    0           conv5_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_concat (Concatenat (None, 7, 7, 640)    0           conv5_block3_concat[0][0]        \n",
      "                                                                 conv5_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_0_bn (BatchNormali (None, 7, 7, 640)    2560        conv5_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_0_relu (Activation (None, 7, 7, 640)    0           conv5_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_conv (Conv2D)    (None, 7, 7, 128)    81920       conv5_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_relu (Activation (None, 7, 7, 128)    0           conv5_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_concat (Concatenat (None, 7, 7, 672)    0           conv5_block4_concat[0][0]        \n",
      "                                                                 conv5_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_0_bn (BatchNormali (None, 7, 7, 672)    2688        conv5_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_0_relu (Activation (None, 7, 7, 672)    0           conv5_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_conv (Conv2D)    (None, 7, 7, 128)    86016       conv5_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_relu (Activation (None, 7, 7, 128)    0           conv5_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_concat (Concatenat (None, 7, 7, 704)    0           conv5_block5_concat[0][0]        \n",
      "                                                                 conv5_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_0_bn (BatchNormali (None, 7, 7, 704)    2816        conv5_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_0_relu (Activation (None, 7, 7, 704)    0           conv5_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_conv (Conv2D)    (None, 7, 7, 128)    90112       conv5_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_relu (Activation (None, 7, 7, 128)    0           conv5_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_concat (Concatenat (None, 7, 7, 736)    0           conv5_block6_concat[0][0]        \n",
      "                                                                 conv5_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_0_bn (BatchNormali (None, 7, 7, 736)    2944        conv5_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_0_relu (Activation (None, 7, 7, 736)    0           conv5_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_conv (Conv2D)    (None, 7, 7, 128)    94208       conv5_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_relu (Activation (None, 7, 7, 128)    0           conv5_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_concat (Concatenat (None, 7, 7, 768)    0           conv5_block7_concat[0][0]        \n",
      "                                                                 conv5_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_0_bn (BatchNormali (None, 7, 7, 768)    3072        conv5_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_0_relu (Activation (None, 7, 7, 768)    0           conv5_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_conv (Conv2D)    (None, 7, 7, 128)    98304       conv5_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_relu (Activation (None, 7, 7, 128)    0           conv5_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_concat (Concatenat (None, 7, 7, 800)    0           conv5_block8_concat[0][0]        \n",
      "                                                                 conv5_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_0_bn (BatchNormal (None, 7, 7, 800)    3200        conv5_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_0_relu (Activatio (None, 7, 7, 800)    0           conv5_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_conv (Conv2D)   (None, 7, 7, 128)    102400      conv5_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_concat (Concatena (None, 7, 7, 832)    0           conv5_block9_concat[0][0]        \n",
      "                                                                 conv5_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_0_bn (BatchNormal (None, 7, 7, 832)    3328        conv5_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_0_relu (Activatio (None, 7, 7, 832)    0           conv5_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_conv (Conv2D)   (None, 7, 7, 128)    106496      conv5_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_concat (Concatena (None, 7, 7, 864)    0           conv5_block10_concat[0][0]       \n",
      "                                                                 conv5_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_0_bn (BatchNormal (None, 7, 7, 864)    3456        conv5_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_0_relu (Activatio (None, 7, 7, 864)    0           conv5_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_conv (Conv2D)   (None, 7, 7, 128)    110592      conv5_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_concat (Concatena (None, 7, 7, 896)    0           conv5_block11_concat[0][0]       \n",
      "                                                                 conv5_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_0_bn (BatchNormal (None, 7, 7, 896)    3584        conv5_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_0_relu (Activatio (None, 7, 7, 896)    0           conv5_block13_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_conv (Conv2D)   (None, 7, 7, 128)    114688      conv5_block13_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_concat (Concatena (None, 7, 7, 928)    0           conv5_block12_concat[0][0]       \n",
      "                                                                 conv5_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_0_bn (BatchNormal (None, 7, 7, 928)    3712        conv5_block13_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_0_relu (Activatio (None, 7, 7, 928)    0           conv5_block14_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_conv (Conv2D)   (None, 7, 7, 128)    118784      conv5_block14_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_concat (Concatena (None, 7, 7, 960)    0           conv5_block13_concat[0][0]       \n",
      "                                                                 conv5_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_0_bn (BatchNormal (None, 7, 7, 960)    3840        conv5_block14_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_0_relu (Activatio (None, 7, 7, 960)    0           conv5_block15_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_conv (Conv2D)   (None, 7, 7, 128)    122880      conv5_block15_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_concat (Concatena (None, 7, 7, 992)    0           conv5_block14_concat[0][0]       \n",
      "                                                                 conv5_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_0_bn (BatchNormal (None, 7, 7, 992)    3968        conv5_block15_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_0_relu (Activatio (None, 7, 7, 992)    0           conv5_block16_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_conv (Conv2D)   (None, 7, 7, 128)    126976      conv5_block16_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_concat (Concatena (None, 7, 7, 1024)   0           conv5_block15_concat[0][0]       \n",
      "                                                                 conv5_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bn (BatchNormalization)         (None, 7, 7, 1024)   4096        conv5_block16_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "relu (Activation)               (None, 7, 7, 1024)   0           bn[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 1024)         0           relu[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            1025        global_average_pooling2d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 7,038,529\n",
      "Trainable params: 6,954,881\n",
      "Non-trainable params: 83,648\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "#keras.applications.densenet.DenseNet121(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n",
    "from tensorflow.keras.applications.densenet import DenseNet121\n",
    "#from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.densenet import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Activation\n",
    "\n",
    "def Densenet121(show_layers,weights,input_shape):\n",
    "        base_model = DenseNet121(include_top=False, weights=weights, input_shape=input_shape)\n",
    "        x = GlobalAveragePooling2D()(base_model.output)\n",
    "        pred = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "        model = Model(inputs=base_model.input, outputs=pred)\n",
    "        model.compile(optimizer=SGD(lr=1e-3, decay=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "        if show_layers:\n",
    "            for i, layer in enumerate(model.layers):\n",
    "                print(i, layer.name, layer.trainable)\n",
    "        return model\n",
    "    \n",
    "model = Densenet121(False,weights=None,input_shape=(224, 224, 3))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>  protocol_4\n",
      "============================================Densenet121=============================================\n",
      "Found 41040 images belonging to 2 classes.\n",
      "Found 30240 images belonging to 2 classes.\n",
      "train shape : (41040,)\n",
      "Epoch 1/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.2873 - accuracy: 0.8822\n",
      "Epoch 00001: val_loss improved from inf to 0.54901, saving model to .\\result_cuda\\protocol_4-01-0.5490.hdf5\n",
      "5130/5130 [==============================] - 3424s 667ms/step - loss: 0.2872 - accuracy: 0.8822 - val_loss: 0.5490 - val_accuracy: 0.7596\n",
      "Epoch 2/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.1386 - accuracy: 0.9477\n",
      "Epoch 00002: val_loss did not improve from 0.54901\n",
      "5130/5130 [==============================] - 3391s 661ms/step - loss: 0.1386 - accuracy: 0.9477 - val_loss: 10.2199 - val_accuracy: 0.3333\n",
      "Epoch 3/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0970 - accuracy: 0.9649\n",
      "Epoch 00003: val_loss improved from 0.54901 to 0.16065, saving model to .\\result_cuda\\protocol_4-03-0.1606.hdf5\n",
      "5130/5130 [==============================] - 3395s 662ms/step - loss: 0.0969 - accuracy: 0.9649 - val_loss: 0.1606 - val_accuracy: 0.9436\n",
      "Epoch 4/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0696 - accuracy: 0.9760\n",
      "Epoch 00004: val_loss improved from 0.16065 to 0.12134, saving model to .\\result_cuda\\protocol_4-04-0.1213.hdf5\n",
      "5130/5130 [==============================] - 3377s 658ms/step - loss: 0.0696 - accuracy: 0.9760 - val_loss: 0.1213 - val_accuracy: 0.9539\n",
      "Epoch 5/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0539 - accuracy: 0.9813\n",
      "Epoch 00005: val_loss did not improve from 0.12134\n",
      "5130/5130 [==============================] - 3379s 659ms/step - loss: 0.0539 - accuracy: 0.9813 - val_loss: 0.5855 - val_accuracy: 0.8112\n",
      "Epoch 6/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0442 - accuracy: 0.9865\n",
      "Epoch 00006: val_loss did not improve from 0.12134\n",
      "5130/5130 [==============================] - 3395s 662ms/step - loss: 0.0442 - accuracy: 0.9865 - val_loss: 1.1177 - val_accuracy: 0.6940\n",
      "Epoch 7/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0349 - accuracy: 0.9893\n",
      "Epoch 00007: val_loss did not improve from 0.12134\n",
      "5130/5130 [==============================] - 3462s 675ms/step - loss: 0.0349 - accuracy: 0.9893 - val_loss: 0.3253 - val_accuracy: 0.9118\n",
      "Epoch 8/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0294 - accuracy: 0.9916\n",
      "Epoch 00008: val_loss did not improve from 0.12134\n",
      "5130/5130 [==============================] - 3427s 668ms/step - loss: 0.0294 - accuracy: 0.9916 - val_loss: 0.4676 - val_accuracy: 0.8266\n",
      "Epoch 9/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0243 - accuracy: 0.9934\n",
      "Epoch 00009: val_loss did not improve from 0.12134\n",
      "5130/5130 [==============================] - 3387s 660ms/step - loss: 0.0243 - accuracy: 0.9934 - val_loss: 0.4342 - val_accuracy: 0.8338\n",
      "Epoch 10/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0208 - accuracy: 0.9942\n",
      "Epoch 00010: val_loss did not improve from 0.12134\n",
      "5130/5130 [==============================] - 3367s 656ms/step - loss: 0.0208 - accuracy: 0.9942 - val_loss: 0.2037 - val_accuracy: 0.9454\n",
      "Epoch 11/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0191 - accuracy: 0.9948\n",
      "Epoch 00011: val_loss did not improve from 0.12134\n",
      "5130/5130 [==============================] - 3404s 664ms/step - loss: 0.0191 - accuracy: 0.9948 - val_loss: 0.2764 - val_accuracy: 0.9103\n",
      "Epoch 12/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0195 - accuracy: 0.9951\n",
      "Epoch 00012: val_loss improved from 0.12134 to 0.10699, saving model to .\\result_cuda\\protocol_4-12-0.1070.hdf5\n",
      "5130/5130 [==============================] - 3369s 657ms/step - loss: 0.0195 - accuracy: 0.9951 - val_loss: 0.1070 - val_accuracy: 0.9628\n",
      "Epoch 13/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0141 - accuracy: 0.9967\n",
      "Epoch 00013: val_loss did not improve from 0.10699\n",
      "5130/5130 [==============================] - 3382s 659ms/step - loss: 0.0141 - accuracy: 0.9967 - val_loss: 0.7947 - val_accuracy: 0.8609\n",
      "Epoch 14/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0167 - accuracy: 0.9955\n",
      "Epoch 00014: val_loss did not improve from 0.10699\n",
      "5130/5130 [==============================] - 3377s 658ms/step - loss: 0.0167 - accuracy: 0.9955 - val_loss: 0.2722 - val_accuracy: 0.9087\n",
      "Epoch 15/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0138 - accuracy: 0.9970\n",
      "Epoch 00015: val_loss improved from 0.10699 to 0.09540, saving model to .\\result_cuda\\protocol_4-15-0.0954.hdf5\n",
      "5130/5130 [==============================] - 3396s 662ms/step - loss: 0.0138 - accuracy: 0.9970 - val_loss: 0.0954 - val_accuracy: 0.9665\n",
      "Epoch 16/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0122 - accuracy: 0.9968\n",
      "Epoch 00016: val_loss did not improve from 0.09540\n",
      "5130/5130 [==============================] - 3366s 656ms/step - loss: 0.0122 - accuracy: 0.9968 - val_loss: 0.1232 - val_accuracy: 0.9595\n",
      "Epoch 17/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0096 - accuracy: 0.9980\n",
      "Epoch 00017: val_loss improved from 0.09540 to 0.08814, saving model to .\\result_cuda\\protocol_4-17-0.0881.hdf5\n",
      "5130/5130 [==============================] - 3362s 655ms/step - loss: 0.0096 - accuracy: 0.9980 - val_loss: 0.0881 - val_accuracy: 0.9625\n",
      "Epoch 18/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0089 - accuracy: 0.9984\n",
      "Epoch 00018: val_loss did not improve from 0.08814\n",
      "5130/5130 [==============================] - 3385s 660ms/step - loss: 0.0089 - accuracy: 0.9984 - val_loss: 0.1214 - val_accuracy: 0.9636\n",
      "Epoch 19/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0123 - accuracy: 0.9972\n",
      "Epoch 00019: val_loss improved from 0.08814 to 0.07550, saving model to .\\result_cuda\\protocol_4-19-0.0755.hdf5\n",
      "5130/5130 [==============================] - 3375s 658ms/step - loss: 0.0123 - accuracy: 0.9972 - val_loss: 0.0755 - val_accuracy: 0.9728\n",
      "Epoch 20/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0110 - accuracy: 0.9980\n",
      "Epoch 00020: val_loss did not improve from 0.07550\n",
      "5130/5130 [==============================] - 3386s 660ms/step - loss: 0.0110 - accuracy: 0.9980 - val_loss: 0.0940 - val_accuracy: 0.9675\n",
      "Epoch 21/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0077 - accuracy: 0.9983\n",
      "Epoch 00021: val_loss improved from 0.07550 to 0.07447, saving model to .\\result_cuda\\protocol_4-21-0.0745.hdf5\n",
      "5130/5130 [==============================] - 3402s 663ms/step - loss: 0.0077 - accuracy: 0.9983 - val_loss: 0.0745 - val_accuracy: 0.9727\n",
      "Epoch 22/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0100 - accuracy: 0.9983\n",
      "Epoch 00022: val_loss did not improve from 0.07447\n",
      "5130/5130 [==============================] - 3437s 670ms/step - loss: 0.0100 - accuracy: 0.9983 - val_loss: 0.0904 - val_accuracy: 0.9675\n",
      "Epoch 23/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0076 - accuracy: 0.9983\n",
      "Epoch 00023: val_loss did not improve from 0.07447\n",
      "5130/5130 [==============================] - 3381s 659ms/step - loss: 0.0076 - accuracy: 0.9983 - val_loss: 0.1095 - val_accuracy: 0.9607\n",
      "Epoch 24/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0078 - accuracy: 0.9984\n",
      "Epoch 00024: val_loss did not improve from 0.07447\n",
      "5130/5130 [==============================] - 3393s 661ms/step - loss: 0.0078 - accuracy: 0.9984 - val_loss: 0.0913 - val_accuracy: 0.9689\n",
      "Epoch 25/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0056 - accuracy: 0.9989\n",
      "Epoch 00025: val_loss did not improve from 0.07447\n",
      "5130/5130 [==============================] - 3510s 684ms/step - loss: 0.0056 - accuracy: 0.9989 - val_loss: 0.1024 - val_accuracy: 0.9645\n",
      "Epoch 26/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0102 - accuracy: 0.9980\n",
      "Epoch 00026: val_loss did not improve from 0.07447\n",
      "5130/5130 [==============================] - 3623s 706ms/step - loss: 0.0102 - accuracy: 0.9980 - val_loss: 0.0972 - val_accuracy: 0.9703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0046 - accuracy: 0.9992\n",
      "Epoch 00027: val_loss did not improve from 0.07447\n",
      "5130/5130 [==============================] - 3475s 677ms/step - loss: 0.0046 - accuracy: 0.9992 - val_loss: 0.0784 - val_accuracy: 0.9710\n",
      "Epoch 28/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0069 - accuracy: 0.9989\n",
      "Epoch 00028: val_loss did not improve from 0.07447\n",
      "5130/5130 [==============================] - 3432s 669ms/step - loss: 0.0069 - accuracy: 0.9989 - val_loss: 0.0823 - val_accuracy: 0.9709\n",
      "Epoch 29/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0078 - accuracy: 0.9983\n",
      "Epoch 00029: val_loss did not improve from 0.07447\n",
      "5130/5130 [==============================] - 3445s 672ms/step - loss: 0.0078 - accuracy: 0.9983 - val_loss: 0.1092 - val_accuracy: 0.9610\n",
      "Epoch 30/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0113 - accuracy: 0.9976\n",
      "Epoch 00030: val_loss did not improve from 0.07447\n",
      "5130/5130 [==============================] - 3473s 677ms/step - loss: 0.0113 - accuracy: 0.9976 - val_loss: 0.1108 - val_accuracy: 0.9667\n",
      "Epoch 31/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0044 - accuracy: 0.9993\n",
      "Epoch 00031: val_loss did not improve from 0.07447\n",
      "5130/5130 [==============================] - 3425s 668ms/step - loss: 0.0044 - accuracy: 0.9993 - val_loss: 0.1282 - val_accuracy: 0.9581\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEKCAYAAADaa8itAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deXzU9bX/8ddJWBJC2EUUtIBKFdmsqNxSEatXwQ0tanHDpVeu19ZKey8Xb29vpaW9tZb2WlurouVqq3WpW/1V6npFsG4IRUVxQUAJIpssCQmBJOf3x2fGDCHLTDJLZub9fDy+j5n5rp9vZjJnPru5OyIiIslQkOkEiIhI7lBQERGRpFFQERGRpFFQERGRpFFQERGRpFFQERGRpFFQERHJU2Y2z8w2mtnyJrabmd1sZivN7E0z+1JL51RQERHJX3cBE5rZPhE4LLJMA25t6YQKKiIiecrdFwKfNbPLJOD3HrwC9DCzA5o7Z4dkJjBVCgoKvLi4ONPJEBHJKpWVlQ4sjVk1193nJnCK/sDamNdlkXXrmzogK4JKcXExO3fuzHQyRESyiplVufvotpyikXXNju2l4i8REWlKGXBQzOsBwCfNHaCgIiIiTXkcmBppBTYG2O7uTRZ9QZYUf4mISPKZ2X3AeKCPmZUB1wMdAdz9NmA+cBqwEqgELm/xnNkw9H1JSYk3rFPZs2cPZWVl7Nq1K0Opyn5FRUUMGDCAjh07ZjopIpICZlbp7iXpvGbW5lTKysooLS1l4MCBmDVWlyTNcXe2bNlCWVkZgwYNynRyRCRHZG2dyq5du+jdu7cCSiuZGb1791ZOT0SSKmuDCqCA0kb6+4lIsmV1UEmaPXvgs+Y6lYqISDwUVABWrQrL7t1xH7Jt2zZ++9vftupyp512Gtu2bYt7/1mzZjFnzpxWXUtEJJ1SFlQaG/3SzHqZ2TNm9kHksWeqrp+Qzp3DY4f42y00F1Rqa2ubPXb+/Pn06NEj7muJiGSLVOZU7mLf0S+vA55z98OA5yKvM6+gAAoLw2OcrrvuOj788ENGjRrFjBkzWLBgASeeeCIXXnghw4cPB+Dss8/m6KOP5sgjj2Tu3PrhdgYOHMjmzZtZs2YNRxxxBFdeeSVHHnkkp5xyClVVVc1ed9myZYwZM4YRI0ZwzjnnsHXrVgBuvvlmhg4dyogRI5gyZQoAL7zwAqNGjWLUqFEcddRRlJeXJ/qXERFJSMqaFLv7QjMb2GD1JEJHG4C7gQXAzLZea/p0WLasDSeo6gM1PaFrHVgILKNGwU03NX3IDTfcwPLly1kWufCCBQt47bXXWL58+edNdOfNm0evXr2oqqrimGOOYfLkyfTu3Xuv83zwwQfcd9993HHHHZx//vk8/PDDXHzxxU1ed+rUqfz617/mhBNO4Ac/+AE//OEPuemmm7jhhhtYvXo1nTt3/rxobc6cOdxyyy2MHTuWiooKioqK2vBHEhFpWbrrVPaPdvGPPPZtakczm2Zmr5vZ6zU1NelJXW1dmw4/9thj9+rzcfPNNzNy5EjGjBnD2rVr+eCDD/Y5ZtCgQYwaNQqAo48+mjVr1jR5/u3bt7Nt2zZOOOEEAC699FIWLlwIwIgRI7jooou455576BApxhs7dizf/e53ufnmm9m2bdvn60VEUqXdfstEhmeeC6FHfXP7NpejiEuVwdvvweDB0KtXq09TUlLfcXXBggU8++yzvPzyy3Tp0oXx48c32iekc7Q+BygsLGyx+KspTzzxBAsXLuTxxx9n9uzZvP3221x33XWcfvrpzJ8/nzFjxvDss89y+OGHt+r8IiLxSHdOZUN0gpfI48Y0X79xhYXhsYUK9lilpaXN1lFs376dnj170qVLF959911eeeWVtqaS7t2707NnTxYtWgTAH/7wB0444QTq6upYu3YtJ554IjfeeCPbtm2joqKCDz/8kOHDhzNz5kxGjx7Nu+++2+Y0iIg0J905lceBS4EbIo9/TvP1G7dqVXisi7/4q3fv3owdO5Zhw4YxceJETj/99L22T5gwgdtuu40RI0bwxS9+kTFjxiQlqXfffTdXXXUVlZWVDB48mP/93/+ltraWiy++mO3bt+PufOc736FHjx7813/9F88//zyFhYUMHTqUiRMnJiUNIiJNSdmAkrGjXwIbCKNfPgY8CBwMfAyc5+4t9jpsbEDJFStWcMQRRyQnsW+8ETpAHnhgWPJIUv+OItKu5NSAku5+QRObTkrVNVutrg5KS2G//TKdEhGRrKYe9e6hLqVrV9AQ8CIibaKgEq1H2b4dEhg6RURE9qWg4g7dukFlJWzalOnUiIhkNQWVDh1gyJBQp5JAk2IREdmXgkpUQUFCTYpFRGRfCirl5fDmm1BdnfKcSteuXRNaLyKSbRRUamrCPCqFhSr+EhFpIwWVaCA5+GA48si4D5s5c+Ze86nMmjWLX/ziF1RUVHDSSSfxpS99ieHDh/PnP8c/aIC7M2PGDIYNG8bw4cN54IEHAFi/fj3jxo1j1KhRDBs2jEWLFlFbW8tll132+b7/8z//E/d1RERSpd0OKJmw8eP3XXf++XD11aFl12mn7bv9sstg4sTQlPi00yB2zvYFC5q93JQpU5g+fTpXX301AA8++CBPPvkkRUVFPProo3Tr1o3NmzczZswYzjrrrLjmg3/kkUdYtmwZb7zxBps3b+aYY45h3Lhx/PGPf+TUU0/lP//zP6mtraWyspJly5axbt06li8Pc6AlMpOkiEiq5E5Qaa1oTqWuLhSFdeq0d3BpwlFHHcXGjRv55JNP2LRpEz179uTggw9mz549fO9732PhwoUUFBSwbt06NmzYQL9+/Vo854svvsgFF1xAYWEh+++/PyeccAKLFy/mmGOO4YorrmDPnj2cffbZjBo1isGDB7Nq1SquueYaTj/9dE455ZS2/iVERNosd4JKczmLLl2a3v7ZZzBoEDz6KKxZA8OH108v3IJzzz2Xhx56iE8//fTz2RbvvfdeNm3axJIlS+jYsSMDBw5sdMj7xjQ1Dtu4ceNYuHAhTzzxBJdccgkzZsxg6tSpvPHGGzz11FPccsstPPjgg8ybNy+u64iIpIrqVHr1gkMOqZ9KOIFmxVOmTOH+++/noYce4txzzwXCkPd9+/alY8eOPP/883z00Udxn2/cuHE88MAD1NbWsmnTJhYuXMixxx7LRx99RN++fbnyyiv5xje+wdKlS9m8eTN1dXVMnjyZ2bNns3Tp0oRuW0QkFXInp9JWrZhT5cgjj6S8vJz+/ftzwAEHAHDRRRdx5plnMnr0aEaNGpXQpFjnnHMOL7/8MiNHjsTMuPHGG+nXrx933303P//5z+nYsSNdu3bl97//PevWrePyyy+nLhIEf/rTn8Z/ryIiKZKyoe+TKaVD33/4YahL6d8f3n0XDjsMundv+3mzhIa+F8ldmRj6XsVfe/aEx2jxl/qqiIi0moq/6upCi6/iYjjqqPrgIiIiCcvqb9CkFN3V1oZAYhbqVeJoTpwrsqHoU0SyS9YGlaKiIrZs2dL2L8ba2hBM3GHt2jCvSh5wd7Zs2UJRUVGmkyIiOSRri78GDBhAWVkZm9o6B8rOnaGivrISPv44DIHfs2dyEtnOFRUVMWDAgEwnQ0RySNa2/kqJvn1h8mS49dbUX0tEJMXU+ivdovPTR5WWhqHwRUSkVfI7qKxeHWZ+vPfe8LpbNwUVEckbZjbBzN4zs5Vmdl0j27ub2f8zszfM7G0zu7ylc+Z3UNmxIzyWRHKHpaVQVZW59IiIpImZFQK3ABOBocAFZja0wW7fBN5x95HAeOAXZtapufNmbUV9UkSDSrdu4fH55+uHaxERyW3HAivdfRWAmd0PTALeidnHgVILc3d0BT4Dapo7qXIqEHIooIAiIrmmg5m9HrNMi9nWH1gb87ossi7Wb4AjgE+At4Br3b3ZUXeVU4H6nMof/wgvvQS/+U3m0iQikjw17j66iW2N9fRu2Bz4VGAZ8FXgEOAZM1vk7juaumB+51SGDIHvfCc0JQZYsgTuuiujSRIRSZMy4KCY1wMIOZJYlwOPeLASWA00O/R6fudURo8OS1RpaegMGe1lLyKSuxYDh5nZIGAdMAW4sME+HwMnAYvMbH/gi8Cq5k6a30GloiKM9RXb+iu6Po+GvxeR/OPuNWb2LeApoBCY5+5vm9lVke23AbOBu8zsLUJx2Ux339zcefM7qHzve/CHP8DWreF1tG6lvFxBRURynrvPB+Y3WHdbzPNPgFMSOWdG6lTM7DuRjjTLzew+M8vMqIY7dtTnTiCM+dWrl/qqiIi0UtqDipn1B74NjHb3YYRs15R0pwMIOZJo7gTg3HNhy5Yw+6OIiCQsU62/OgDFZtYB6MK+LQ7SY8eOvYOKiIi0SdqDiruvA+YQWhWsB7a7+9MN9zOzadEOOzU1zXbgbL2GQWXt2pBb+dvfUnM9EZEcl4nir56EoQAGAQcCJWZ2ccP93H2uu49299EdOqSoPcG0aXDppfWvd++Ghx+GlStTcz0RkRyXidZfJwOr3X0TgJk9AnwZuCftKfnGN/Z+Hdv6S0REEpaJOpWPgTFm1iUySNlJwIoMpCMMfR8bQKItwXY0OQKBiIg0IxN1Kq8CDwFLCQOUFQBz050Oamth8GCYM6d+XefOYX4V5VRERFolI50f3f164PpMXPtzFRXhMbai3gwOPxy6dMlMmkREslz+9qiP5kYaNil+6630p0VEJEfk7yjFDYe9FxGRNlNQaRhU/u3fYPr09KdHRCQH5G/x18EHw803w7Bhe69fvhw++ywzaRIRyXL5G1QOPBCuuWbf9aWl8NFH6U+PiEgOyN/ir40bQ66k4RAw3bqpn4qISCvlb1D54x9h+PB9A0hpqfqpiIi0Uv4GlWjgiJ1PBUKHyMMPB/f0p0lEJMvlb1DZsQOKi6Fjx73Xf/vb8NproSOkiIgkJL+DivqoiIgklYJKQ88+C0cfDatWpT9NIiJZLn+bFF99dZg6uKGqKli6NGwbPDj96RIRyWL5G1SOP77x9dGKe7UAExFJWP4Wf73yCrz//r7rNVGXiEir5W9QufBC+PGP912vibpERFotf4NKUxX1PXvCuHHQq1f60yQikuXys07Fvemg0qcPvPBC+tMkIpID8jOnUl0Ne/aon4qISJLlZ1BpaYKu446DH/wgfekREckR+Vn8VVoKf/kLDB3a+Pb162Ht2vSmSUQkB+RnUCkuhtNPb3p7aalaf4mItEJ+Fn9t2ACPPw5btza+XcPfi4i0Sn4Glddeg0mTmh7fq1s3BRURkVbIz+Kvlirqx43TPPUiIq2goNKY738/fWkREckh+Vn81VJQERHJA2Y2wczeM7OVZnZdE/uMN7NlZva2mbXYMzx/g0qHDlBU1Pj2G28MPes1pbCI5CgzKwRuASYCQ4ELzGxog316AL8FznL3I4HzWjpvfgaVadPgueeanjLYPcynUlWV3nSJiKTPscBKd1/l7ruB+4FJDfa5EHjE3T8GcPeNLZ00P4PKF74QKuObopGKRSQ3dDCz12OWaTHb+gOxvbzLIutiDQF6mtkCM1tiZlNbvGDb05yFnnkGamthwoTGt8dO1NWvX/rSJSKSXDXuPrqJbY0V1TQs8+8AHA2cBBQDL5vZK+7eyGRU9QekXaSc7k5gGOEmrnD3l9OWgJ/9DHbtajqoaKIuEcl9ZcBBMa8HAJ80ss9md98J7DSzhcBIoMmgkqnir18BT7r74YQErkjr1Zsa9j5q8GC49NL6HIuISO5ZDBxmZoPMrBMwBXi8wT5/Bo43sw5m1gU4jha+r9OeUzGzbsA44DKASAXR7rQmYseOEDiaMnw43HVX2pIjIpJu7l5jZt8CngIKgXnu/raZXRXZfpu7rzCzJ4E3gTrgTndf3tx5zdPcbNbMRgFzgXcIuZQlwLWR7FXsftOAaQCdOnU6urq6OnmJOPBAOOMMmDu3+f3cm24hJiLSzplZpbuXpPOamSj+6gB8CbjV3Y8CdgL7dLpx97nuPtrdR3fokOQM1Y4dzRdtffopdO4Mt9+e3OuKiOS4TFTUlwFl7v5q5PVDNBJUUmrx4ubrVEpKYPduVdSLiCQo7UHF3T81s7Vm9kV3f4/QVO2dtCbiiCOa315SEoq9FFRERBKSqdZf1wD3mtmbwCjgv9N25W3b4Fe/gvebbBEHBQXQtas6P4qIJCgj/VTcfRnQVIec1Corg+nTQ2X9kCFN76eJukREEpZ/PerjHaF42jQ49NDUp0dEJIcoqDTl+utTnxYRkRyTfwNKRoNKS73la2qgoiL16RERySH5G1Rayqmcdx78wz+kPj0iIjkk/4LKhRfC6tWhor45qqgXEUlYXEHFzK41s24W/M7MlprZKalOXEp06QIDB4aZH5vTrZuCiohIguLNqVzh7juAU4D9gMuBG1KWqlR64gn45S9b3k85FRGRhMUbVKKjKp4G/K+7v0HjE7y0f489BnPmtLxfaSns2QPJHMhSRCTHxRtUlpjZ04Sg8pSZlRKGQc4+LQ0mGTVuHMyaFUYqFhGRuMQ19L2ZFRCGU1nl7tvMrBcwwN3fTHUCAUpKSnznzp0t7xiPiRNh8+YwqKSISA5rz0Pf/wPwXiSgXAx8H9ieumSlUHl5y82JIRR9ffKJir9ERBIQb1C5Fag0s5HAvwMfAb9PWapSqaWphKNeeAH691eORkQkAfEO01Lj7m5mk4BfufvvzOzSVCYsZV59NeRCWhKtd9FIxSIicYs3qJSb2X8AlwDHm1kh0DF1yUqh4uKwtCQaVNSsWEQkbvEWf30dqCb0V/kU6A/8PGWpShX3MOz9s8+2vG+0iExBRUQkbnEFlUgguRfobmZnALvcPfvqVKqqwgRdS5a0vK+Kv0REEhbvMC3nA68B5wHnA6+a2bmpTFhKxDuYJISZH2+8MfRXERGRuMRbp/KfwDHuvhHAzPYDngUeSlXCUiJalBVPUCkshBkzUpseEZEcE2+dSkE0oERsSeDY9iPeuVSiVq+Gjz9OXXpERHJMvDmVJ83sKeC+yOuvA/NTk6QU2rkTzOLLqQCceiocfTTcd1/L+4qISHxBxd1nmNlkYCxhIMm57v5oSlOWCuPGhRkd49WtmyrqRUQSEPcc9e7+MPBwCtOSHgUJlNpp+HsRkYQ0+w1rZuVmtqORpdzMsu8n/NNPw5VXhmKweGiiLhGRhDQbVNy91N27NbKUunucFRPtyJIlcOedoWVXPEpLVfwlIpKAuIu/ckJ5OXTsCJ07x7f/lVfCpEmpTZOISA7Jr6ASHaHY4py08oQTUpseEZEck319Tdoi3mHvozZsgBdfhNra1KVJRCSH5FdQKSiA/fePf//774fjj4ft2TkfmYhIuuVX8ddddyW2f+xIxb16JT05IiK5JmM5FTMrNLO/m9lfMpWGFmmkYhGRhGSy+OtaYEVar3jVVXDLLfHvr4m6RCSHmdkEM3vPzFaa2XXN7HeMmdXGMzp9RoKKmQ0ATgfuTOuF//xnePPN+PfXRF0ikqMiM/jeAkwEhgIXmNnQJvb7GfBUPOfNVE7lJuDfgbq0XjXR1l+HHw4PPQQjR6YuTSIimXEssNLdV7n7buB+oLGOedcQhuja2Mi2faQ9qERmjtzo7s1Ov2hm08zsdTN7vSaRQSCbUlMDlZWJBZWePWHyZOjXr+3XFxFJvw7R79HIMi1mW39gbczrssi6z5lZf+Ac4La4L9iW1LbSWOAsMzsNKAK6mdk97n5x7E7uPheYC1BSUuJtvmoiE3RF1dbCc8/BoEFw2GFtToKISJrVuPvoJrY11gu84XftTcBMd6+1ODuNpz2n4u7/4e4D3H0gMAX4v4YBJSV27YJDDoG+feM/xj3MqXL//alLl4hIZpQBB8W8HgB80mCf0cD9ZrYGOBf4rZmd3dxJ86efygEHwMqViR3ToQMUFamiXkRy0WLgMDMbBKwj/Mi/MHYHdx8UfW5mdwF/cffHmjtpRoOKuy8AFmQyDS3S8PcikoPcvcbMvkVo1VUIzHP3t83sqsj2uOtRYuXPMC0vvQQnnwzvv5/YcRr+XkRylLvPd/ch7n6Iu/8ksu62xgKKu1/m7g+1dM78CSpr14ZK90Rbkmn2RxGRuOVPnUo0t5FI6y+AW2+FkpLkp0dEJAcpqLRkzJjkp0VEJEflT/FXtAira9fEjnv9dXj00eSnR0QkB+VPUOnVC449Nsypkog77oB/+ZfUpElEJMfkT1D59rfh1VcTP04V9SIiccufoNJa3bqFMcM0pbCISIvyJ6hcdRX80z8lfpzmVBERiVv+tP56883WNQ2ODSo9eiQ3TSIiOSZ/ciqJzqUSNWkSLF0K+++f/DSJiOSY/MmptDao7LdfWEREpEXKqbRk0ya4/XZYsybpSRIRyTX5E1SOOw6OOCLx49avD5X8S5cmP00iIjkmf4q/nnqqdcdFK+o1UrGISIvyJ6fSWmpSLCISt/wIKitXwuDBMH9+4scqqIiIxC0/gsrWrbB6NdTVJX5s587QqZOKv0RE4pAfdSqtHfY+6q231KxYRCQOCirxGDIkeWkREclh+VH8FQ0q0fqRRN1zD9x/f/LSIyKSo/Ijp3LAAXDmmWFOlda4/fZQrzJlSnLTJSKSY/IjqJxySlhaq7Q09KwXEZFm5UfxV1tpoi4RkbjkR1D57ndh2LDWH9+tm4KKiEgc8iOobNwIu3a1/vjSUvVTERGJQ34ElR07Wt/yC2DWLPj446QlR0QkV+VHRX1rh72PasuxIiJ5JH9yKm0JDH//O8ycGYZ7ERGRJuVHUDn99LY1KX73XbjxRtiwIXlpEhHJQWkv/jKzg4DfA/2AOmCuu/8qpRedPbttx2ukYhGRuGSiTqUG+Fd3X2pmpcASM3vG3d9J2RXdwaz1x2uiLhGRuKS9+Mvd17v70sjzcmAF0D9lF9y9GwoLQ/FVa0XrY5RTERFpVkZbf5nZQOAo4NVGtk0DpgF06tSp9RcpLw85laKi1p9DxV8iInHJWFAxs67Aw8B0d9+nXMnd5wJzAUpKSrzVF2rrsPcQZo3ctStM2CUiIk3KSOsvM+tICCj3uvsjKb1YNHfRlqBSUKCAIiI5x8wmmNl7ZrbSzK5rZPtFZvZmZHnJzEa2dM60BxUzM+B3wAp3/2XKL5iMnArAd74DDz3U9vSIiLQDZlYI3AJMBIYCF5jZ0Aa7rQZOcPcRwGwipUfNyUROZSxwCfBVM1sWWU5L2dX69oVrrw1FWG1x113wwgtJSZKISDtwLLDS3Ve5+27gfmBS7A7u/pK7R3t9vwIMaOmkaa9TcfcXgTa0703QkCFw001tP49GKhaR7NPBzF6PeT03Ul8NodXt2phtZcBxzZzrG8BfW7xgwknMNrt21bf+amtfFfVTEZHsUuPuo5vY1tgXYqONoszsREJQ+UpLF8z9YVp+8xvo0gUqKtp2Hk3UJSK5pQw4KOb1AOCThjuZ2QjgTmCSu29p6aS5H1R27Ag5lJKStp2nV6+Q4xERyQ2LgcPMbJCZdQKmAI/H7mBmBwOPAJe4+/vxnDT3i7/Ky0Muo6CN8fOJJ5KTHhGRdsDda8zsW8BTQCEwz93fNrOrIttvA34A9AZ+GxruNlucBoB5Fvz6Likp8Z07d7bu4G98A55+GtaubXlfEZEcYmaV7t7GYprE5EfxVzIm2brvPrj00rafR0Qkh+V+UJk8Gb75zbaf5+234Z57VK8iItKM3K9TmTIlOecpLYW6OqisbHulv4hIjsr9nMq6dcnpX6Lh70VEWpT7QWXMGJg+ve3n0URdIiItyv2gkqyK+l694MADw6RfIiLSqNyuU6mrC8VVyQgqp50WitIyITod8oYN4V6KizOTDhGRFuR2TmXnzvCFnIygkil/+QuceCKsWgVHHgnXX5/pFImINCm3g0oyJuiKWr8eTj8dnnmm7eeK11//GppE79wZit++9jX4xS/g1X1mXxYRaRdyO6iUlIRh77/85eScb/58+PDD5JyrJU8/DeecA8OGhec9esCcOdC/P1xxRRh9WUSkncntoNK9e5iga9iwtp8rna2/nn8eJk2CI44IOaOePcP6bt3gjjvgnXfgRz9KfTpERBKU20Fl+/bQEz4Zv+pLSkJleTr6qRx8MJx0UggovXrtve3UU+Hyy+Gjj9S7X0TandwOKs89F3Ip773X9nOZpX6irg8+CIHikENCBX2fPo3vd/vtcO+9bZt0LNe8+CJs25bpVIjkvdwOKtEAkKzWXyNG1BdFJdvf/gZHHQX//d8t79uxY3hcsQLmzUtNerJBXV0YfXrr1tCI4h//MTwXkYxRUEnEokUwa1ZyzhXrlVdg4sT6Svh4zZkD06bB3/+e/DRlgzlzQjPrLVtCzu3NN+GUUxRYRDIoP4JKtJK9PXrttVBP0q9fqKA/4ID4j/35z2G//UIdS7719F+4EL73PZgwIRQXnnEGPPxwCCzKsYhkTO4HlaIi6NQpOeebMSNM+pUslZVw1lmh7uT//i8MA5OIXr3gttvgjTfghhuSl672bsOGMPr04MFw5531dUtnnAGPPAJvvRX+LiKSdrk98+Prr4fmt1OnJichX/taqEx/663knA9CB8dhw+Cgg1p/josuggcfhKVLYfjw5KWtPaqtDUVcL78cOoE2dr/LloX6r7ZOIS2S5TIx82Nuj/01enRYkiUZrb/cQ13AfvvBZZeFupS2uvnm8Kv90ENb3veTT0JT5WeeCR05p06Ff/mXUHx2/fWhk2XPnvWPhx/etoCXbDU14V4vuaTpADpqVHhcswa++92Qm2nYNFtEUiK3g0qylZa2rZ9KbW0Yhv83v4GLLw7TEyejWXDv3jB7dnheV7f3L/ToYJR1dXDMMSE3AyGoDR8OhYXh9bZtIdjV1Ox97p/+FK67DqqqQgu1k07KXFNmd+jcOXQAjcfKlWEUhJNPhmefVWARSQOVDySiW7cQVFpTZFhVBeedFwLKv/4r3H138r+c33wzFKU9+mioY/nqV8MXKoRA89Wvws9+FlqLffpp6MczbVrY3rdvyK1UVEBZGSxfHlq7XXBB2P7AA6ECfPjw8Mu/qiq5aW/Jxx/D2LGhM2u8Tj4ZHnssFIGedEuT7jgAAA5QSURBVFJoJSYiqeXu7X7p0qWLt8bf/+6+YIH77t2tOnxfv/ud+1e+4l5dndhx1dXuY8e6m7nfdFOSEtOIjRvd+/RxD2HPfcQI95kz3evq2n7uXbvc77rLfeTIcO4+fdy///3E/7h1de5btyaWpupq9+OOcy8tdX///cSu5+7+5JPunTuHtG/alPjxIlkK2Olp/r7OeMCIZ2ltUJk6Ndxhaan7Oee4z53r/vHHrTpV282e7f6nP6X+OkuWuN97r/v69ak5f12d+/PPu591lvsxx9QHh3Xr6vepqXFfs8b9s8/C67fecp88OXypd+0a3pRDD3V/6aX4rnntteGYtvz9nnwy/CCIpundd91ra1t/PpEskImgktOtv3bsCCU8f/0rPPlk6HwNob/chAmhjvwrXwnF9CmxdCns2QPHHZeiC2RYdXX44332WajM/+IXQzPp1atDUdodd8A//VMolvv610N/kkMPhf33D31y7rorNKN++OFQ1DZ5chhROlrPA/CnP8H558O3vw2/+lXb0uuR+qWdO0NxX48eoUjy/PPDtNNqLSY5JhOtv3I6qMRyD0XrTz4ZgsyiReF7r6QkVDWMHx++3/r0CXXYffqEZa+A88ILcOaZ0LVraGHUu3dY/vmfw4jCGzaEuog+fcKkWlOnhi/axYtze5yuioowHtljj4WAEQ0eJ54YX4u0H/84LNXV4fizzw4B5uSTQ+Tfti10dkxWf6Pq6tCf5cEHw4ehujoExVtvDcO9tMQ91Cnt2BGO7d491LclKyjV1u4dWCW3RX/sQPiBtm1b+Hzt2hW+jA4+uNWnzpugYmYTgF8BhcCd7t5sz71kBJWGKirCj+VokFm9uvH9SkvrA8zg7lu4evUM9tu9jtI9W+havYUuVVt4fvqfqTruRA55/QGG/2TK58fWDBvJ7kfn03nQgfqOaEl5eWip9cgj8MQTMGRIyOnt3h1Gm95vv9Rcd8cOePzxEGB+9KPwY2HRIpg7N/xTl5eHfcrLw6CV3buH1nA/+9ne5ykoCPt06RLm8Jk/PzTJji777x9a/kGYaO2558KXx9at4bFnz/CrB0InzkWLwjF9+4Zl2LD66Q4WLar/IqqqCkvPnjBuXNg+dy5s3Fi/raAARo4MzbAB7rknHF9cHNJbXAwDBsBhh4Xtzz4b7j26VFeHpuVjx4b3Y/bs+nNXVobHSZNCo44tW0I/osrKkEvv3Dmc/5prQmvHTZvgW98K66JL587hh8Rxx4UGGbNnh2P37AnX27Mn/O3Gjw8/2mbMCEG3oKD+ceZMOPbY0BF4zpz660bvcerU0BR9zZrQijG6rbAw/C3GjAnv7Zo14Rx1dXsvZ5wRfoH+7W/hvS0vr1927Ag56u7d4Ze/hF//Ovwdo7WbAO++G643a1bIwdfV1f99a2vrR8S4/PKQg4+aObNNHZvzop+KmRUCtwD/CJQBi83scXd/J53p6No1ZDrOPDO875s3h897Y4/R5x9s7s0le+ZRXhE+R7W1kZNF3vNenMwwFtCbLRRTxePLz6LisDBETIcO4XNeVFT/GH3eqVPY3rFj/RL7uqXnDR8LC+uX6P9dPM8bPsY+NwvPCwqaf97YEt73fdfFci+F474Ox30d+2EVhevLqFkDZp0oLNyPgk+aT2M4x95LXd2+66JpqU9vNwrOuRj72sVhfTUUvrqEwhdeCB+S0m7hl0W/A6CmFoMwrE7PniF30qlTCHrbt4cvDQgXLi8P0xNs3RqWDh3qg8rGjeED1bNn+DLv0SOM+xZ17rkht7dxY1jef3/v6RuuuSZ88cUaPz78SoIwfM/KleGPU1wcbnzixPqgMn36vi3hpk4NLRIh5NYaDvtz9dUhqJjBT35SH4yiS7SIt3PnkOUvLg4fxurqEHS6dAnbKytD59Rdu+oDU3U1fOEL4Rzl5eFHRceO4W8b/bBXVITj9+wJaa+rC/+A0cfo9s8+C1/8u3fXB7xdu8LfZ/BgeOml0Jy/ocWLQ5+2p58OJQ8NrVgRAutrr4UfFKWlYekW+XxUV4f9vvAFOP74fT/40Q/p4YeH96KgIHwBFBeHx+iPhCuuCGmNrh8yZN+0tHNpz6mY2T8As9z91Mjr/wBw9582dUwqcipt5V7/Qzb2x2zs8+iPvNgffA1/AO7aFT7/NTX1P85inzd8HX0eu07SL/Y7o2FAjX1dUACGU2TV7C4o2uf4xl7HBsToD+XY50NqV3CgrwOgimJ2WTFbrRcf2UDMoISd7KYTtQUd90lTQQH08/V0tZ10sSq6UEkXq2JrYR/e6zgMdzi6+iWqvRPVdGYXReyiiO10Zwfd8LrwfWEFts8PhcZ+WDTU3NdNU+eIXRebeYiNKQ3XmcX8uLK68OOjQwGlVkE/X08Xq6LEKimkllov4L1Ow9lpXelRs5l+e9ZSRwG1XkCdG3UU8FHhYKqtCKurpY4CPPy8+Px+Yh+be+9it0fT16HD3o8N191+e4hTrZEXORWgP7A25nUZsE9NtplNA6YBdEpWWXoSmdX/SOvbN7Npqa3dO9jU1Oz9DxfP84aPDZ/H/vpv6Z+mYc6g4brYIuRYja1r6gukYRqjxzf3BRc9f2M5maaeN3UPDdc1PGbv8xnu9QGl4Rdr7Gv35nOD4fURkaXx9EBJo3/z+vfqgH3es751MOTza3+5yVynmX1+3cbe74br4n2fW8phRtfF5lSjS8PXBQX1x4TPSUHM867U1R1W/7kGCg2O/Pz++kSWve/7C59/fgr3+kHR2OPe71Xjr6H+Mxz9/419jH3ensfDbUwmgkpjNdb7/H5x97nAXAg5lVQnKptFf92krBWbiEicMtGGsgyIHUxqAPBJBtIhIiJJlomgshg4zMwGmVknYArweAbSISIiSZb24i93rzGzbwFPEZoUz3P3BAZ0EhGR9ipvOj+KiOSbTLT+0rgUIiKSNAoqIiJ5yswmmNl7ZrbSzK5rZLuZ2c2R7W+a2ZdaOqeCiohIHooZ3WQiMBS4wMyGNthtInBYZJkG3NrSeRVURETy07HASndf5e67gfuBSQ32mQT8PjKS/itADzM7oLmTZsV0wpWVlW5mrZ1qsANQ0+Je2UH30v7kyn2A7qW9asu9FJvZ6zGv50Y6lkN8o5s0tk9/YH1ziW333L3VOSoze93dRyczPZmie2l/cuU+QPfSXqXwXuIZ3SSuEVBiqfhLRCQ/xTO6ScIjoCioiIjkp3hGN3kcmBppBTYG2O7uTRZ9QZYUf7XR3JZ3yRq6l/YnV+4DdC/tVUrupanRTczsqsj224D5wGnASqASuLyl82ZFj3oREckOKv4SEZGkUVAREZGkyemg0tIQBNnEzNaY2VtmtqxBu/N2zczmmdlGM1ses66XmT1jZh9EHntmMo3xauJeZpnZusj7sszMTstkGuNhZgeZ2fNmtsLM3jazayPrs+59aeZesvF9KTKz18zsjci9/DCyPqvel5ytU4kMQfA+8I+EZnGLgQvc/Z2MJqyVzGwNMNrdN2c6LYkws3FABaFX7rDIuhuBz9z9hkiw7+nuMzOZzng0cS+zgAp3n5PJtCUi0iP6AHdfamalwBLgbOAysux9aeZezif73hcDSty9wsw6Ai8C1wJfI4vel1zOqcQzBIGkmLsvBD5rsHoScHfk+d2EL4F2r4l7yTruvt7dl0aelwMrCL2ks+59aeZesk5kKJSKyMuOkcXJsvcll4NKU8MLZCsHnjazJWY2LdOJaaP9o23dI499M5yetvpWZATXee29aKIhMxsIHAW8Spa/Lw3uBbLwfTGzQjNbBmwEnnH3rHtfcjmoJDy8QDs31t2/RBg19JuRohjJvFuBQ4BRhPGQfpHZ5MTPzLoCDwPT3X1HptPTFo3cS1a+L+5e6+6jCD3XjzWzYZlOU6JyOagkPLxAe+bun0QeNwKPEor3stWG6EinkceNGU5Pq7n7hsgXQR1wB1nyvkTK7B8G7nX3RyKrs/J9aexesvV9iXL3bcACYAJZ9r7kclCJZwiCrGBmJZFKSMysBDgFWN78Ue3a48ClkeeXAn/OYFrapMEw4OeQBe9LpEL4d8AKd/9lzKase1+aupcsfV/2M7MekefFwMnAu2TZ+5Kzrb8AIs0Ib6J+CIKfZDhJrWJmgwm5EwhD6/wxW+7FzO4DxgN9gA3A9cBjwIPAwcDHwHnu3u4rwJu4l/GEIhYH1gD/3NLYSJlmZl8BFgFvAXWR1d8j1EVk1fvSzL1cQPa9LyMIFfGFhB/8D7r7j8ysN1n0vuR0UBERkfTK5eIvERFJMwUVERFJGgUVERFJGgUVERFJGgUVERFJGgUVkRQzs/Fm9pdMp0MkHRRUREQkaRRURCLM7OLIfBbLzOz2yOB+FWb2CzNbambPmdl+kX1HmdkrkQELH40OWGhmh5rZs5E5MZaa2SGR03c1s4fM7F0zuzfSE1wk5yioiABmdgTwdcLAnaOAWuAioARYGhnM8wVCL3qA3wMz3X0EoTd3dP29wC3uPhL4MmEwQwij504HhgKDgbEpvymRDOiQ6QSItBMnAUcDiyOZiGLCwH11wAORfe4BHjGz7kAPd38hsv5u4E+R8dn6u/ujAO6+CyByvtfcvSzyehkwkDAJk0hOUVARCQy4293/Y6+VZv/VYL/mxjVqrkirOuZ5Lfrfkxyl4i+R4DngXDPrC5/PC/4Fwv/IuZF9LgRedPftwFYzOz6y/hLghcg8HmVmdnbkHJ3NrEta70Ikw/RrSQRw93fM7PuE2TULgD3AN4GdwJFmtgTYTqh3gTAE+W2RoLEKuDyy/hLgdjP7UeQc56XxNkQyTqMUizTDzCrcvWum0yGSLVT8JSIiSaOcioiIJI1yKiIikjQKKiIikjQKKiIikjQKKiIikjQKKiIikjT/H+xyUzDb+W1IAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9727\tloss: 0.0745\n",
      "========================================protocol_4 finished=========================================\n"
     ]
    }
   ],
   "source": [
    "#### import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "%matplotlib inline\n",
    "SEED = 777\n",
    "\n",
    "def main():\n",
    "    inputSize = 224\n",
    "    batchSize = 8\n",
    "    nEpoch = 100\n",
    "    \n",
    "    # 디렉토리 정보 설정\n",
    "    # dataDir = 'D:\\\\Face_Database\\\\B-Database'\n",
    "    dataDir = 'E:\\\\Face_Database\\\\B-Database'   \n",
    "    # 학습 및 테스트 할 데이터베이스 디렉토리명\n",
    "    trainDB = 'protocol_4'  # D:\\Face_Database\\B-Database\\protocol_4\\train\n",
    "    validDB = 'protocol_4'\n",
    "    \n",
    "    saveDir =  '.\\\\result_cuda'\n",
    "    if not os.path.exists(saveDir):    \n",
    "        os.makedirs(saveDir)\n",
    "    model_path = os.path.join(saveDir, trainDB + '-{epoch:02d}-{val_loss:.4f}.hdf5')\n",
    "    \n",
    "    # 데이터베이스별 학습 수행\n",
    "    print('>> ', trainDB)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    print('Densenet121'.center(100, '='))\n",
    "   \n",
    "    '''\n",
    "    training\n",
    "    '''\n",
    "    K.clear_session()\n",
    "\n",
    "    # 네트워크 정의\n",
    "    model = Densenet121(False,weights=None,input_shape=(224, 224, 3))\n",
    "\n",
    "    # 데이터 generator 생성\n",
    "    train_datagen = ImageDataGenerator(rescale=1.0/255.0,\n",
    "                                       horizontal_flip=True,)#이미지augmentation > 이미지를 회전,확대,축소 등 여러변형해보는것.\n",
    "    train_generator = train_datagen.flow_from_directory(os.path.join(*[dataDir,trainDB,'train']),\n",
    "                                                        target_size=(inputSize, inputSize),\n",
    "                                                        batch_size=batchSize,\n",
    "                                                        class_mode='binary',\n",
    "                                                        interpolation='bilinear',\n",
    "                                                        shuffle=True,)\n",
    "\n",
    "    valid_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    valid_generator = valid_datagen.flow_from_directory(os.path.join(*[dataDir, validDB,'val']),\n",
    "                                                        target_size=(inputSize, inputSize),\n",
    "                                                        batch_size=batchSize,\n",
    "                                                        class_mode='binary',\n",
    "                                                        interpolation='bilinear',\n",
    "                                                        shuffle=False,)\n",
    "\n",
    "    print('train shape :',train_generator.classes.shape)\n",
    "    # unbalanced class를 해결하기 위한 class_weight 설정 #np.unique는 중복원소제거하는거\n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.unique(train_generator.classes), train_generator.classes) \n",
    "\n",
    "    # callback 함수 정의 dropout함수\n",
    "    early_stop = EarlyStopping(patience=10, monitor='val_loss')\n",
    "    cb_checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "    # 모델 학습\n",
    "    hist = model.fit_generator(train_generator, \n",
    "                               epochs=nEpoch,\n",
    "                               steps_per_epoch=len(train_generator),\n",
    "                               class_weight=class_weights,\n",
    "                               validation_data=valid_generator,\n",
    "                               validation_steps=len(valid_generator),\n",
    "                               verbose=1,\n",
    "                               callbacks=[early_stop, cb_checkpoint])\n",
    "\n",
    "    \n",
    "    # 학습 결과 그래프 그리기\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, loss_ax = plt.subplots()\n",
    "    acc_ax = loss_ax.twinx()\n",
    "\n",
    "    loss_ax.plot(hist.history['loss'], 'b', label='train loss')\n",
    "    loss_ax.plot(hist.history['val_loss'], '--r', label='val loss')\n",
    "    loss_ax.set_xlabel('epoch')\n",
    "    loss_ax.set_ylabel('loss')\n",
    "    loss_ax.legend(loc='upper left')\n",
    "\n",
    "    plt.show()\n",
    "    print('acc: {:.4f}\\tloss: {:.4f}'.format(hist.history['val_accuracy'][-11], hist.history['val_loss'][-11]))\n",
    "\n",
    "    print(('{} finished'.format(trainDB)).center(100,'='))\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> model loaded: protocol_4-21-0.0745_0.001.hdf5\n",
      ">>>> evaluating on 'protocol_4'\n",
      "Found 30240 images belonging to 2 classes.\n",
      "Found 56160 images belonging to 2 classes.\n",
      "                  pred_fake(0)   pred_real(1)\n",
      "actural_fake(0)           36565            875\n",
      "actual_real(1)             438          18282\n",
      "\n",
      "EER: 0.0285\tHTER: 0.0234\n",
      ">> finished\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3dd3wVZdbA8d9JQgihBKRJJyAIgRCECLKAgKAgouK+64rr2pW1YF1cbLvKqqtrl4UVXTuviu9aQBSwIKKIdCnSuwSQ3lvaef94bsIljRvIzeRmzvfzuZ/cmWfuzJnk5p47z8ycR1QVY4wx/hXldQDGGGO8ZYnAGGN8zhKBMcb4nCUCY4zxOUsExhjjczFeB1BctWrV0qZNm3odhjHGRJR58+btUNXaBbVFXCJo2rQpc+fO9ToMY4yJKCKyobA26xoyxhifs0RgjDE+Z4nAGGN8zhKBMcb4nCUCY4zxubAlAhF5Q0S2icjPhbSLiIwQkdUiskhEOoQrFmOMMYUL5xHBW0C/ItovBFoEHoOBl8MYizHGmEKE7T4CVf1ORJoWscilwDvq6mDPFJHqIlJPVbeEK6ZyQxWys93P4OcVKkB0NGRmwr59x9pzHgkJULEiHDkCO3e6eTnrU4U6dSAuDg4cgK1bj83PWaZxY9e+ezds3gxHj7rt5SzTqpVr37oVNm48ft0A7dtDbCykpRXcfs45bn1r1xbc3quX+7lsGWzadHxs0dHQu7ebXrDAxZfzewG33xdc4J7PmuVizHktQJUqx17//fewY8fx7dWrw3nnuedTprjfQXAJ99q1oWdP93ziRNi///j2+vXh3HPd808+gcOHj29v0gS6dXPPx451f8Pg9hYt3O8H4O23j83PWaZNGzj7bMjIgDFjyOess9zj4EF4//387Z07Q3Iy7NkD//d/x68boHt3SEqCbdvg44/zt/fp42LctAnGj8/fftFF0LQprFsHn32Wf/uXXQYNG8KKFTB5cv72QYOgbl1YtMj9/vOu//rroUYNmDsXpk3L337LLe5v/MMPMGNG/va773bvzW++gdmz82///vvdz4kT3fsrWGwsDB3qnn/yCSxZcvz6q1SBe+5xz99/H1atOr69Zk0YMsQ9f/NNWL/++PXXrw9X9oQj26BO9/yxlQRVDdsDaAr8XEjbZ0C3oOkpQGohyw4G5gJzGzdurGXCxo2q332n+vzzqv/8p+rdd6u+/bZry8hQ7dlTNTVVtXZt1VatVFu0UH3mGde+a5dqbKz7iIuPV61Sxf3MaV+7VrVCBdWYGNXoaNWoKFUR1ZEjXfuCBXk/4t3jrbdc+7RpBbePG+faP/us4PYpU1z7++8X3D5rlmt/9dWC21escO3PPFNw++bNrv1vfyu4ff9+137vvQW357j55vxtVaocax80KH97vXrH2i+6KH97y5bH2nv0yN/eseOx9g4d8rf36HGsvUWL/O0DBhxrP/30/O1XXnmsvXLl/O2DBx9rL+h38+c/u7Z9+wpuf+QR175pU8Htzz7r2pcvL7j9lVdc+5w5Bbe/955rnzq14PYJE1z7p58W3D51qmt/992C2+fOde2jR5/ae++RRyLvvXdTQ9X3K6p+3ED16B49WcBc1YI/q8W1h0fgiOAzVW1bQNvnwJOqOj0wPQX4i6rOK2qdqampWmp3Fqu6b5/Tp8N337lvPI88Ar/+CvXq5V9+4ED3bS42Fnr0cN+Os7LceurWhUsugSuvhEOH3DeInTvdN8mKFUEE+vd33zp37YJnnnHzgh8XXeS+FW7dCq+84uZFRR1rHzDAfavbtAk+/DD/6/v3h8RE+OUX960rZz64nxde6L59rF/v9jf4tQB9+0KtWu4b+9y57hv3kSPuSEPEfaOuWhXWrIGlS49ft4jbt7g4941ozZr87b16QUyMa9+4MX97jx5uetWqY9/oc5aJjj72jXnVKvc7DP7dVKgA7dq59tWr3RFTDhH3N0hKOtZ+8ODx7ZUqub9/TvuRI8dvPz7e/W5z2jMyjm+vUsV94wW371lZ+dtz3lNr1rj3THB7tWruvQLu958zP2eZatXgtNPc3yQtjXwSEtwjKwu2FHDQnZDg/naZme5bf/C6c9rj491+7dyZv71aNfc7Sk93RxUFtVes6Nr378+//WrV3N/o6FH3/5FX1aruvXH06LHfffD6K1d274GjR9028rbHx7v3Q3r6sb9NcHulSu55Rsaxv02wuDj3MzPT/Y7zio11P3P+3/OuPzra/cz72uD3eGFW/hvm3g7NrocOz0Ns9cKXLYKIzFPV1ALbPEwErwDfqur7gekVQE89QddQqSWCV1+Fxx8/1kUB0KXLscPK0aNdd8FZZ7kPzypViv5jGmNMKLKOwK75UPs3blqzYccsqN3llFZbVCLw8vLRT4FrAlcPnQPsPVESCKtFi+CKK459m9q71/W7jhjh+vyyso4lAXB9joMGwZlnum8rlgSMMadq23SYmAJTL4CDgdJAEnXKSeBEwnayWETeB3oCtUQkDXgEqACgqqOBiUB/YDVwCLg+XLGc0Lx57kRdpUouETRsCPfd5x7GGBNuGfthwQOwapSbrtYK0vdA5SalsvlwXjV05QnaFbg9XNsP2aZNcPHF7sz9d99Bs2ZeR2SM8ZPNX8DswXDoF5AYSBoGbR+G6LhSCyHiylCXuDvucN1A33xjScAYU7qWPAkLH3TPa3SAc16HGu1LPQx/l5hQdVdD3HGHu47aGGNKU4MBEFMV2j8FfWd5kgTA70cEIvD66+4GKmOMCbfDW2Dd/0Lroe7zp3oyDNwIsQmehuXfI4LsbHe9dlSUu4bZGGPCRRXWvAmfJcGCv8Av/z3W5nESAD8nggUL4Iwzjt1Ob4wx4XBgHUztC7NugIw9UK8f1DrH66iO49+uoZx7As4+29s4jDHlU3aWuxx0wQOQdQhiT4OOL0HTq8rcfUf+TQRz58Lppx8rC2CMMSVp1SiYd5d73vj3kPoviKvjbUyF8G8iWLkSWrb0OgpjTHnV/Cb45UNodS80Guh1NEXy5zmCo0ddqdlOnbyOxBhTXuyaB99eBBmBgoYx8dBnWplPAuDXI4KYGBg3ztXfN8aYU5F5GBY/CsufA82Cpf+ElCdcWxk7F1AYfyaC6GhXstkYY07Ftu9g1k2wfxUgcOY90OZBr6MqNn8mgilTXO3znBGjjDGmODL2wYL7YVVghN2EJOj8epm7LDRU/kwETz7p7iaeOdPrSIwxkWj7DJcEJMYdAbR5EKIreh3VSfNnIli71moLGWOKJ+vIsYqg9ftBu8egwSVQo523cZUA/101lJ3tRh1r2tTrSIwxkUAVNnwA4xNhR9DA9m0fLhdJAPyYCLZsceOONm7sdSTGmLLu0Gb4biD8MAiO/Apr3/A6orDwX9fQ5s3uZ/363sZhjCm7VGHN6/DTUMjY60pFd3jW3SRWDvkvESQnw+LF0KiR15EYY8qig7/AzOth6zduuv5F0Gk0xDf0Nq4w8l8iiIuDtm29jsIYU1ZFVXB3CVesBR1HQJNBEXNj2MnyXyL45htYtgxuvdWNRWCMMftWQJXmEBUDlerBuZ9AQluIq+11ZKXCf5+E774Lw4dbEjDGQFY6LB4OE5NhxYvH5tft5ZskAH48ItiwwUpPG2Ng5xyYeQPs/dlNH9rkbTwe8l8i2L0b6tb1OgpjjFcyD8Giv8GKF0CzXZdQ5/+4owCf8l8i2LoVWrf2OgpjjBcOpcHXPeHAGpAoN4h88nBXMtrH/JcINm1y1UeNMf5TqT5UOh2iKwWKxNmYJODHRLB3rys4Z4zxh02fQ/VkqNzYHQV0+9CNHxwd63VkZYb/Lp2pVs3uKjbGD45shx+ugmkDYPYt7m5hCBwRWBII5q9EsHcv3H23lZ82pjxThfXvw+dJsOE91w1U73xAvY6szPJX19CePfDSS9CuHZwTmQNIGGOKcCgNZt8Kmz9z03XPc1cEVWnmbVxlnL8SwdGj7mfFyB1AwhhTiIz9MKk9HN0JFarBWc9B8xvLfXmIkuCvRLB/v/tpbwxjyp8KVeGMW2DPYjj73xDfwOuIIkZYzxGISD8RWSEiq0Xk/gLaE0RkgogsFJElInJ9OOMhK8v9jIsL62aMMaUgOwuWPQcbxx2blzwczh1nSaCYwnZEICLRwCjgfCANmCMin6rq0qDFbgeWqurFIlIbWCEi76pqeliCSk93NYaqVw/L6o0xpWTPYph5I+yaA3F13cngmMoQZfcInYxwdg11Alar6loAERkLXAoEJwIFqoqIAFWAXUBm2CLq1s0dFahdPWBMRMo6Ckv+4R6a6cYIOPsVlwTMSQtnImgAbAyaTgPyjhg/EvgU2AxUBa5Q1ey8KxKRwcBggMYlMcSknSMwJvLsmAWzboS9S9x0i1uh/VPuxLA5JeE8R1DQp23er+J9gQVAfaA9MFJE8v1VVfVVVU1V1dTatU+hNOysWXDdda7MhDEmcmRnwow/uiRQtQX0meZOCFsSKBHhTARpQPB4kA1x3/yDXQ98rM5qYB3QKmwRrVkDb79tJSaMiRQ5HQRRMW64yNZ/gQsXQp1zvY2rnAlnIpgDtBCRRBGJBQbhuoGC/QL0BhCRusCZwNqwRZQeOAcda7eXG1Ompe+BWTfDvLuPzTu9N5z1T4ip5F1c5VTYzhGoaqaIDAG+AKKBN1R1iYjcEmgfDTwGvCUii3FdScNUdUe4YmLLFvfTEoExZVfaeJhzKxzeAtFxkHQ/xFt9sHAK6w1lqjoRmJhn3uig55uBC8IZw3HiAzXHK1QotU0aY0J0ZBvMvRN++cBN1+riSkVbEgg7f91ZHBsLNWtCZbvUzJgyZd3/wry7IH0XRMdD+yehxe12X0Ap8Vf10VtvhR07LBEYU9Zs/twlgdP7wEU/w5l3WhIoRf46IjDGlA2a7cYLqBQYP7zjCKjXDxKvsft8POCvI4L//Af++EevozDG3/athCm9YOoFkJ3h5sXVhmbXWhLwiL8SwU8/wRdfeB2FMf6UnQlLn4ZJKbDtOzjyK+xf5XVUBr91DR09apeOGuOF3Qth5g2we76bTrwWOjwPFU/zNi4D+C0RrF5th57GlLal/4SFDweKxDWGTq9C/b5eR2WC+KtrqFYtOHzY6yiM8ZfY00CzoOUQd0WQJYEyx19HBLVru1LUxpjwyTgAu+ZC3Z5uuvlNUPNsqNHe07BM4UJKBIFaQY0DheEi1+jRJ17GGHPytnwFswfDka3u23+VZq471pJAmXbCriERuQhYDHwVmG4vIp+EOzBjTARJ3+1GDJt6ARxcD9XOhKwjXkdlQhTKOYK/4waU2QOgqguAM8IZVNjceivcd5/XURhTvmz8GD5LgrVvQFRFSPkH9J0NCUleR2ZCFErXUIaq7pHjr7aJzLEe58+H0+xyNWNKzKJH4efh7nntrtDpNUgI35AiJjxCOSJYJiK/B6ICYwu8CMwMc1zhkZ3tBq83xpSMJr93VwV1/Bf0+c6SQIQK5VNxCNARyAY+Bo4Ad4UzqLDJyrJEYMypOLgBFj8GGugUSEiCgb/AmUNA7H8rUoXSNdRXVYcBw3JmiMhvcUkhsmRnQ7RVNDSm2DQbVr0MC+6HzANQ9QxoeqVri7FqvpEulBT+cAHzHirpQEpFUhKcEZnnuY3xzL4V8PW5MHeISwKNfgd1z/M6KlOCCj0iEJG+QD+ggYg8H9RUDddNFHnee8/rCIyJHNkZsOxZWDwcso9C3Olw9iho9FuvIzMlrKiuoW3Az7hzAkuC5u8H7g9nUMaYMmDlKFj4oHve7Hro8BzE1vA2JhMWhSYCVf0J+ElE3lXV8nFnyMCBrnvoH//wOhJjyr4z/gRbJkOrP0O9872OxoRRKOcIGojIWBFZJCIrcx5hjywcliyBDRu8jsKYsmnbdJjSG9L3uOmYStBrsiUBHwglEbwFvAkIcCHwf8DYMMYUPnb5qDH5ZeyHOUPg6+6w9Rt3XsD4SiifivGq+gWAqq5R1YeBXuENK0zS06FCBa+jMKbs2DwZPm8Lq0aBxECbh6HtX72OypSyUO4jOCquvsQaEbkF2ATUCW9YYZKVBTH+qrxtTIGO7oT598K6d9z0aR2h8+tQI8XbuIwnQvlUvAeoAtwJPAEkADeEM6iw+c1voHVrr6Mwxnu75rskEB0HycOh1b0QZV+S/OqEf3lVnRV4uh+4GkBEGoYzqLD56COvIzDGO5kHj90FXO98OOsZaHAJVGvpbVzGc0WeIxCRs0VkoIjUCky3EZF3iNSic8b4kSqseRPGNYbtM47Nbz3UkoABikgEIvIk8C5wFTBZRB4CpgILgch896SkwJNPeh2FMaXnwDo3WMysGyB9F2yIzAv+THgV1TV0KZCiqodF5DRgc2B6RemEFgarVsGuXV5HYUz4ZWe5K4EWPABZh6BiTejwEjT9g9eRmTKoqERwRFUPA6jqLhFZHtFJANxVQ1Z91JR3B9bCjD/Cjh/ddJNB0PEliIvMi/1M+BWVCJqJSE6paQGaBk2jqiesPCUi/YCXgGjgNVV9qoBlegIvAhWAHaraI/Twi8kSgfGD6MquYmil+nD2y9DwEq8jMmVcUYngf/JMjyzOikUkGhgFnA+kAXNE5FNVXRq0THXg30A/Vf1FRML3lUXVJYKsrLBtwhjP7F4ECa0hqgJUqgs9JrhBY2Krex2ZiQBFFZ2bcorr7gSsVtW1ACIyFnfeYWnQMn8APlbVXwLb3HaK2yxcdjYMGACNGoVtE8aUuszDsPhRWP4ctHsc2gQKA9f+jadhmcgSzjtIGgAbg6bTgM55lmkJVBCRb4GqwEuq+k7eFYnIYGAwQOPGjU8umuhomDDh5F5rTFm07TuYdRPsX+WGiczY53VEJkKFMxFIAfO0gO13BHoDlYAfRWSmqh5X3VRVXwVeBUhNTc27DmP8JWOfGzJy1ctuOiEJOr8BtfJ+zzImNCGX4hSRisVcdxoQ3A/TEHcJat5lJqvqQVXdAXwHhKfYyd69UKsWvPJKWFZvTKk4uAE+b+OSgMRA20eg33xLAuaUnDARiEgnEVkMrApMp4jIv0JY9xyghYgkikgsMAj4NM8y44HuIhIjIvG4rqNlxdqDUGVkwM6drgKpMZEqvhFUaQ6npcKF86HdoxBd3O9oxhwvlK6hEcAAYByAqi4UkROWoVbVTBEZAnyBu3z0DVVdEqhgiqqOVtVlIjIZWIQbB/k1Vf35JPelaJmZ7qeVoTaRRBV++S/UPBuqJLpzAd0+dFcDWZE4U0JCeSdFqeoGV4k6V0jXYKrqRGBinnmj80w/AzwTyvpOyZHAaJu7d4d9U8aUiEObYe5tkDYeTu8Dvb4EEYir5XVkppwJJRFsFJFOgAbuDbgDiLyhKjVwjrlePW/jMOZEVGHtGzD/z5CxFypUg8aXex2VKcdCSQS34rqHGgNbga8D8yJL5cpw7bXQooXXkRhTuANrYdbNbshIgPoDoNPLEB+Zld9NZAglEWSq6qCwRxJuderAW295HYUxhUvfC5M6QsYeqFgLOo5wdYKkoCuxjSk5oSSCOSKyAvgAdxfw/jDHZIw/xSbAmXe5G8Q6vghxtb2OyPjECS8fVdXmwOO4G78Wi8g4EYm8I4QlS9wVQzZKmSkrstJh8d/hlw+PzUt+BLq+a0nAlKqQbihT1RmqeifQAdiHG7AmsmRmuocdZpuyYOccmNwRFj8Cc2+HzENuvr0/jQdCuaGsiohcJSITgNnAdiDyKlrlVB21MtTGS5mHYP5Q+PIc2Puzuzms6wcQE+91ZMbHQjlH8DMwAXhaVb8PczzhY4nAeG3rt65I3IE17saw1kMhebglAeO5UBJBM1XNDnsk4bY/cI47KuTySsaUnOxMmD3YJYHqydD5dXe3sDFlQKGJQESeU9U/Ax+JSL6Kn6GMUFamJCbCH/8Ite0knClF2VkQFe3KQXT6D2ybBkn3Q3Ss15EZk6uoI4IPAj+LNTJZmZWYCGPGeB2F8Ysj22HeXe6u4E6Bqip1e7iHMWVMUSOUzQ48ba2qxyWDQDG5Ux3BrHRlZbnKo7Gxdp7AhI8qbBgL8+6EozsgpjIkPwqVTvc6MmMKFUqH+Q0FzLuxpAMJux9/hPh4mDrV60hMeXUoDaZdAjP+4JJA3d7Qf5ElAVPmFXWO4ArcGAKJIvJxUFNVYE+4AzMmoqx+FX66z40eViEBOjwPza63+wJMRCjqHMFsYCduZLFRQfP3Az+FMyhjIs626S4JNLwUUv8N8fW9jsiYkBV1jmAdsA5XbdQYEyw7E478eqwqaMcXoOEl0Oh/7CjARJxCzxGIyLTAz90isivosVtEdpVeiMaUMXsWw5e/gal9Ieuom1exJjT+nSUBE5GK6hrKGY6yfAyH1KgRPPQQNG3qdSQmUmUdhSX/cA/NdOMHH1gHCa28jsyYU1JU11DO3cSNgM2qmi4i3YB2wP/iis9FjiZN4PHHvY7CRKods2DWjbB3iZtucRu0f9LdJ2BMhAvl8tFxuGEqmwPvAK2B98IaVTikp8O2be6nMcWxeDh82cUlgaotoM80OHuUJQFTboSSCLJVNQP4LfCiqt4BNAhvWGEwcybUrQvTp3sdiYk0lZu4InFJw+DChVDnXK8jMqZEhTRUpYhcDlwNDAzMqxC+kIzxWPoe2DET6vdz04nXQs1z7FyAKbdCvbO4F64M9VoRSQTeD29YxngkbTx8ngTfXwb7Vrp5IpYETLl2wiMCVf1ZRO4EzhCRVsBqVX0i/KEZU4qObIO5d8IvgVqLtboAdimo8YcTJgIR6Q6MATbh/jNOF5GrVfWHcAdnTNipwvp3XaXQ9F2uSFzKk+6qoCgrTmj8IZRzBC8A/VV1KYCItMYlhtRwBlbiEhPhqaegeXOvIzFlyaKH3X0BAKefD51ehSpNPQ3JmNIWyjmC2JwkAKCqy4DIG1WjUSMYNszdT2BMjsRroFI9OOdN6PWFJQHjS6EcEcwXkVdwRwEAVxGJRecOH4YtW6BePahUyetojFf2rYS1b0HKE+4kcLUz4ZJ1EF3R68iM8UwoRwS3AGuAvwDDgLXAn8IZVFjMmuW6hWbN8joS44XsTFj6NExKgaVPuvMCOSwJGJ8r8ohARJKB5sAnqvp06YRkTAnbvRBm3gC757vpxGuhfn9vYzKmDCmq+uiDuPISVwFfiUhBI5UZU3ZlHYGFD8PkVJcE4htDz8nQ5S2oeJrX0RlTZhTVNXQV0E5VLwfOBm4t7spFpJ+IrBCR1SJyfxHLnS0iWSLyu+Juw5hCrfw3LHkCNAta3gEX/Qz1+3odlTFlTlFdQ0dV9SCAqm4XkVDOJ+QSkWjcyGbnA2nAHBH5NPgKpKDl/gl8UazIjSmI6rExAVreDtu/h9ZDoXZXb+MypgwrKhE0CxqrWIDmwWMXq+pvT7DuTri7kNcCiMhY4FJgaZ7l7gA+wh11hE+LFjBqlPtpyqctX8Kiv0HPia7rJ7oinPuJ11EZU+YVlQj+J8/0yGKuuwGwMWg6DegcvICINAAuA86jiEQgIoOBwQCNGzcuZhg50TSA2247udeasi19N8y/110WCrDiJWg33NOQjIkkRQ1MM+UU111QoRbNM/0iMExVs6SIIf5U9VXgVYDU1NS86wjN/v2wdq27hLRKlZNahSmDNn4Mc2534wdHVXQJoNW9XkdlTEQJ5Yayk5WGG90sR0Ngc55lUoGxgSRQC+gvIpmqOq7Eo5kzB3r3hmnT4FyrJx/xDv8Kc4fAxo/cdO1u0Pk1d4OYMaZYwpkI5gAtAmWrNwGDgD8EL6CqiTnPReQt4LOwJAFT/uxd6pJATBVo/09ocYsbPMYYU2whJwIRqaiqR0NdXlUzRWQI7mqgaOANVV0iIrcE2kcXO1rjb+l7ILa6e376eZA6EhoMcCOIGWNOWihlqDsBrwMJQGMRSQFuCgxZWSRVnQhMzDOvwASgqteFErDxIc2GlaNg4UPQ83Oo093Nb3m7t3EZU06Eciw9AhgA7ARQ1YW4EcuMCb+9y+Hrc2HenZC5HzZN8DoiY8qdULqGolR1Q56rerLCFE/4tG4Nb78NZ9rJxIiQnQHLnoHFwyE7HeJOh7NfhkYDT/xaY0yxhJIINga6hzRwF/AdwMrwhhUG9erBNdd4HYUJxb5V8MPvYfcCN93sBujwLMTW8DYuY8qpULqGbgXuBRoDW4FzOIm6Q57bswemT4e9e72OxJxIbHU4lAaVm8J5X8E5r1sSMCaMQhm8fhvu0s/INncunH8+fP89dOvmdTQmrx2zoMZZEB0LcbWh5ySo1goq2M1/xoRbKFcN/Yf8dwSjqoPDEpHxl4z9sOABWDUKkodD8t/c/JqRNSS2MZEslHMEXwc9j8PVBtpYyLLGhG7zZJj9Jzj0C0gMBVclMcaEWyhdQx8ET4vIGOCrsEVkyr+jO12RuHXvuOnTOkLn16FGirdxGeNTJ1NiIhGwWznNyTmwHr7sDEe2QXQcJP8dWt0DUeGsdmKMKUoo5wh2c+wcQRSwCyh0tLEyq107+Phjdz+B8U7lJpCQDNUyoNN/oFpLryMyxvdONHi9ACm4onEA2ap6cmWgvVanDlx2mddR+I+qGyegTneoeoYbPaz7h1ChmhWJM6aMKPI/MfCh/4mqZgUekZkEAHbsgEmTYPduryPxjwPrYOoFMOsGmHWzqxkE7j4BSwLGlBmh/DfOFpEOYY8k3ObPh/79YdkyryMp/7KzYPlL8Hlb+PVrqFgTmt+EXRVkTNlUaNeQiMSoaibQDbhZRNYAB3H/zaqqkZ8cTMnbuxRm3QQ7fnTTTQZBx5cgro63cRljClXUOYLZQAfAqnyZ0KTvhS/OcVVCK9V3ReIaXuJ1VMaYEygqEQiAqq4ppVhMpItNgDb3u0tEz3rGTRtjyryiEkFtESl0FHBVfT4M8ZhIknkYFj8KNdpD0yvdvKQH3JVBxpiIUVQiiAaqUF7O8HXsCF99BUlJXkdSPmyd5s4FHFjt+v8bDoSYSpYEjIlARSWCLar691KLJNxq1oQ+fbyOIvJl7IOfhsHqwIijCW1ceYiYSt7GZYw5aSc8R1BubNniSlD37u2Sgim+TRNhzp/cWAFRFaDNQ64rKDrW68iMMaegqPNLTWcAABqSSURBVETQu9SiKA0LF8IVV8CPP1oiOBnZGfDTvS4J1OzkjgKqt/U6KmNMCSg0EajqrtIMxJRBqi4BRMe6I4DOr7sBZM68C6KivY7OGFNC/FPyMYKrY3ji0CaYc5sbLazza25e7a7uYYwpV/xX8MWuaimaKqz+D3yeBJs+hV8+hMNbvY7KGBNG/jkiMCe2fw3Mvhm2TnXTDS52dwdXquttXMaYsPJPIujSBWbOhDZtvI6k7FGFFS/Cwocg6zBUrAUd/wVNrrAjKGN8wD+JoHp16NzZ6yjKJhHY87NLAk3+ECgSV8vrqIwxpcQ/5wg2boQ33oDt272OpGzISnfjBeTo8Cz0+By6vmtJwBif8U8iWLwYbrwR1q078bLl3c45MLkjfHshZB1x82JrQIP+3sZljPGEfxKBgcxDMH8ofHkO7P3ZDSBzcKPXURljPBbWRCAi/URkhYisFpF8A96LyFUisijwmCEiKWELxu/3EWydChOTYflzbrr1fdB/IVRr4W1cxhjPhe1ksYhEA6OA84E0YI6IfKqqS4MWWwf0UNXdInIh8CoQ3jO6frwKZsH9sPSf7nn1ZOj8BtRM9TYmY0yZEc6rhjoBq1V1LYCIjAUuBXITgarOCFp+JtAwjPH4V0LbQJG4v0LSMCsSZ4w5TjgTQQMguAM6jaK/7d8ITCqoQUQGA4MBGjdufHLRnHsuLFkCiYkn9/pIcmQ77JgBDS91002vcqUhqvhg340xxRbOcwQF9cEU2FEvIr1wiWBYQe2q+qqqpqpqau3atU8umqpV3aA0lcpx3XxVWP8efN4apv8e9i5z80UsCRhjChXORJAGNAqabghszruQiLQDXgMuVdWdYYtm3Tp46SXYWk7r5hzcCNMuhhlXwdGdULs7RJfjpGeMKTHhTARzgBYikigiscAg4NPgBUSkMfAxcLWqrgxjLK5b6O673Y1l5Ylmw6pX4PM2sPlzqJDgykWf9xVUaep1dMaYCBC2cwSqmikiQ4AvcOMfv6GqS0TklkD7aOBvQE3g3+Ku5slUVbucpTgWDINlz7rnDQdC6iiIr+9tTMaYiBLWWkOqOhGYmGfe6KDnNwE3hTOGoA2XymZK3Rl/cqWiz3oaGv3On5fHGmNOif/uLI70D8rdi2De3ccSW9Uz4OJV0PjyyN83Y4wn/FN9NNJlHYUlT8CSJ0Ez4bSOkHi1a4uyP6Mx5uT55xOkd2/YsAFOP93rSIpvx0yYdSPsDdyL1+J2dz7AGGNKgH8SQXw8nOzNaF7JPAgLH4YVLwEKVVu68YPrdPc6MmNMOeKfcwQrV8Ljj8OWLV5HErpVr7iRwyQKku53ReIsCRhjSph/EsHy5fDXv5b9RBB8dVPLIW7EsL6zof2TEB3nXVzGmHLLP4kgEmwcB5POgiM73HR0rBsx7LQO3sZljCnX/JMIyvJ9BIe3utpA318GexbCqn97HZExxkf8c7I4R1m61l4V1v+vuy8gfRfEVIaUp6DlbV5HZozxEf8lgrLi4C8w+xbYEqi8ffoF0OkVqw9kjCl1/kkEF14Iu3a5ctRlwcH1LglUqA4dX4DEa8vW0Yoxxjf8kwhiY93DS0e2Q1xgPIU657oqofX7Q6UIvMnNGFNu+Odk8ZIlMGwYbM43JEL4ZWe6MYPHN4Zfvzk2v/kNlgSMMZ7zTyJYtQqefhq2bSvd7e5eAF90dgPIZx2Brd+c+DXGGFOK/NM1VNqyjsDPj7kjAc2Cyk2g06tQ7wKvIzPGmOP4JxGU5n0Ee5fC9/8D+5YDAi3vgJR/QIUqpReDMcaEyD+JIEdpXJkTd7q7L6BaK1ckrnbX8G/TGGNOkv8SQbhs/RZqdYHoilDxNOj1FVRrafWBjDFlnn9OFl96KWRkQLt2Jbveo7tg5vUwpZcbOCZHjXaWBIwxEcE/RwRRUe5Rkn75CObeDke2QlRFqJBQsus3xphS4J9EsGABvPoqPPggNGx4aus6/CvMHQIbP3LTtbtD5/9AtTNPPU5TIjIyMkhLS+PIkSNeh2JMqYqLi6Nhw4ZUqFAh5Nf4JxGsXQsvvwy33HJqieDAWpicCum7IaYKtP8ntLjFDR5jyoy0tDSqVq1K06ZNESvdYXxCVdm5cydpaWkkJiaG/Dr/JIKSUjkRanYCxBWJqxxhw1/6xJEjRywJGN8REWrWrMn27duL9Tr/JIKTvY9As2HlKHcjWLUz3eWn3T50JaPtQ6ZMsyRg/Ohk3vf+SQQ5ivNL2rsMZt0EO2ZA7W7Q5zv3ersxzBhTjvgnEURFQVxcaIkgOwOWPQOLh0N2OlSqB63+bEcAxphyyT9nOC+7DA4fhrZti15u13z4ohMsfMglgeY3wkVLodHA0onTlBvR0dG0b9+etm3bcvHFF7Nnz57ctiVLlnDeeefRsmVLWrRowWOPPYYGdV9OmjSJ1NRUWrduTatWrRg6dKgXuxCScePG8fe//z2s21i+fDldunShYsWKPPvssydc/sUXX+TQoUOnvN309HSuv/56kpOTSUlJ4dtvvwVg//79tG/fPvdRq1Yt7r777gLXsWjRIrp06UKbNm1ITk7OvZItPT2dwYMH07JlS1q1asVHH7mrEP/1r3/Rtm1b+vfvT3p6OgDTp0/n3nvvzV3n9u3b6dev3ynvXy5VjahHx44dNWyO7lb9oIrqu6iOS1Td8nX4tmXCaunSpcfP6NEj/2PUKNd28GDB7W++6dq3b8/fFoLKlSvnPr/mmmv08ccfV1XVQ4cOabNmzfSLL74IbP6g9uvXT0eOHKmqqosXL9ZmzZrpsmXLVFU1IyNDR+XEWsIyMjJOeR1dunTR7du3l0A0hdu6davOnj1bH3zwQX3mmWdOuHyTJk1KJKaRI0fqddddlxtDhw4dNCsrK99yHTp00GnTpuWbn5GRocnJybpgwQJVVd2xY4dmZmaqqurf/vY3feihh1RVNSsrKzfedu3aaVZWlj744IP66aefanZ2tl5wwQW6a9eu49Z93XXX6fTp0wuMO9/7X1WBuVrI56p/jghmz4ZrroG0tMKXia0OyY/AmXfDRYvh9N6lF58p17p06cKmTZsAeO+99+jatSsXXOAq0cbHxzNy5EieeuopAJ5++mkeeughWrVqBUBMTAy33Vb4ONZbt27lsssuIyUlhZSUFGbMmMH69etpG3T0++yzz/Loo48C0LNnTx588EF69OjBE088QdOmTcnOzgbg0KFDNGrUiIyMDNasWUO/fv3o2LEj3bt3Z/ny5fm2vXLlSipWrEitWrUAmDBhAp07d+ass86iT58+bN26FYADBw7kfrNu165d7rffyZMn06FDB1JSUujdu/D/tzp16nD22Wfnuzb+4MGDXHTRRaSkpNC2bVs++OADRowYwebNm+nVqxe9evUqdJ2hWLp0aW5cderUoXr16sydO/e4ZVatWsW2bdvo3r17vtd/+eWXtGvXjpSUFABq1qxJdHQ0AG+88QYPPPAAAFFRUbm/Q3D3wRw6dIgKFSowZswY+vfvT40aNY5b98CBA3n33XdPaf9y+OccwYYNMGaMG5wm5z6CjP1unICanaHZNW5e67J7CG5OQeCQvkDx8UW316pVdPsJZGVlMWXKFG688UbAdQt17NjxuGWaN2/OgQMH2LdvHz///DN//vOfQ17/nXfeSY8ePfjkk0/IysriwIED7N69u8jX7Nmzh2nTpgEwf/58pk2bRq9evZgwYQJ9+/alQoUKDB48mNGjR9OiRQtmzZrFbbfdxjffHD+exg8//ECHDh1yp7t168bMmTMREV577TWefvppnnvuOR577DESEhJYvHgxALt372b79u3cfPPNfPfddyQmJrJr166Q9znH5MmTqV+/Pp9//jkAe/fuJSEhgeeff56pU6ce9+Ga45577mHq1Kn55g8aNIj777//uHkpKSmMHz+eQYMGsXHjRubNm8fGjRvp1KlT7jLvv/8+V1xxRYFX66xcuRIRoW/fvmzfvp1Bgwbxl7/8Jbeb8K9//SvffvstzZs3Z+TIkdStW5ehQ4dyzjnn0KZNG7p27crAgQOZPHlyvnWnpqby8MMPF+8XVgj/JIK8Nk+C2X+CQxth44fQ5PdWG8iUqMOHD9O+fXvWr19Px44dOf/88wHXHVvYJX4nc+nfN998wzvvvAO48xIJCQknTARXXHHFcc8/+OADevXqxdixY7nttts4cOAAM2bM4PLLL89d7ujRo/nWs2XLFmrXrp07nZaWxhVXXMGWLVtIT0/Pvanp66+/ZuzYsbnL1ahRgwkTJnDuuefmLnPaaacVe9+Tk5MZOnQow4YNY8CAAQV+K8/rhRdeCHn9N9xwA8uWLSM1NZUmTZrwm9/8hpiY4z82x44dy5gxYwp8fWZmJtOnT2fOnDnEx8fTu3dvOnbsSEpKCmlpaXTt2pXnn3+e559/nqFDhzJmzBiuvvpqrr76agCGDx/OnXfeyaRJk3jnnXdo1KgRzz33HFFRUdSpU4fNJTTiYli7hkSkn4isEJHVInJ/Ae0iIiMC7YtEpENB6ylRmXtgxjXwbX+XBE5LdZVCLQmYElapUiUWLFjAhg0bSE9PZ9SoUQC0adMmX/fC2rVrqVKlClWrVqVNmzbMmzfvlLYdExOT290D5Cu1Ubly5dznl1xyCZMmTWLXrl3MmzeP8847j+zsbKpXr86CBQtyH8uWLStwH4PXfccddzBkyBAWL17MK6+8kttWUPIrKiGGqmXLlsybN4/k5GQeeOCBkE5a33PPPced6M155HTNBYuJieGFF15gwYIFjB8/nj179tCiRYvc9oULF5KZmZnvCC9Hw4YN6dGjB7Vq1SI+Pp7+/fszf/58atasSXx8PJdddhkAl19+OfPnzz/utZs3b2bOnDlceumlPP7443zwwQdUrFiRKVOmAO5vWqlSpZB/V0UJWyIQkWhgFHAhkARcKSJJeRa7EGgReAwGXg5XPKhCJ2DFJbB+jPvgP+sZuOBHVynUmDBJSEhgxIgRPPvss2RkZHDVVVcxffp0vv76a8AdOdx555385S9/AeC+++7jH//4BytXrgQgOzub559/vtD19+7dm5dfdv86WVlZ7Nu3j7p167Jt2zZ27tzJ0aNH+eyzzwp9fZUqVejUqRN33XUXAwYMIDo6mmrVqpGYmMh///tfwH1oL1y4MN9rW7duzerVq3On9+7dS4MGDQB4++23c+dfcMEFjBw5Mnd69+7ddOnShWnTprFu3TqAk+oa2rx5M/Hx8fzxj39k6NChuR+mVatWZf/+/QW+JueDPe8jb7cQuHMmBw8eBOCrr74iJiaGpKRjH2Pvv/8+V155ZaHx9e3bl0WLFnHo0CEyMzOZNm0aSUlJiAgXX3xx7lVIU6ZMOW694LqNHnvsMcC9R0SEqKio3KuhVq5cedx5oFNS2FnkU30AXYAvgqYfAB7Is8wrwJVB0yuAekWt96SvGhr3keqz0e6KoK96qO5bdXLrMRGhoKsmSlvwVUOqqgMGDNB33nlHVVUXLVqkPXr00JYtW2rz5s310Ucf1ezs7NxlJ0yYoB06dNBWrVpp69atdejQoYVu59dff9VLLrlE27ZtqykpKTpjxgxVVX3ppZe0efPm2qdPH7322mv1kUceUVXVHj166Jw5c45bx3//+18F9Ntvv82dt3btWu3bt6+2a9dOW7durcOHD8+37YMHD2pSUlJu7OPGjdPExETt1q2bDh06VHsErrDav3+/XnPNNdqmTRtt166dfvTRR6qqOnHiRG3fvr22a9dO+/TpU+g+btmyRRs0aKBVq1bVhIQEbdCgge7du1cnT56sycnJmpKSoqmpqbn7NWLECD3zzDO1Z8+eha4zFOvWrdOWLVtqq1attHfv3rp+/frj2hMTE3Ov7soxfvx4/etf/5o7PWbMGE1KStI2bdrofffdlzt//fr12r17d01OTtbzzjtPN2zYkNs2f/58veGGG3KnX3jhBU1KStK+ffvqkSNHVFX1mWee0REjRhQYd3GvGhIN0xCOIvI7oJ+q3hSYvhrorKpDgpb5DHhKVacHpqcAw1R1bp51DcYdMdC4ceOOGzZsOLmgdsxyg8mfcbMViSvnli1bRuvWrb0OwxfuuusuLr74Yvr06eN1KL5y7rnnMn78+HxXE0HB738RmaeqqQWtK5yfhgV1/uXNOqEsg6q+qqqpqpoafGKq2Gp1hhZ/siRgTAl68MEHS+TmLRO67du3c++99xaYBE5GOK8aSgMaBU03BPKe4g5lGWMM8MQTT+T22ee4/PLLeeihhzyKyKlbty6XXHJJiazrzTff5KWXXjpuXteuXXNPtBundu3aDBxYctUOwtk1FAOsBHoDm4A5wB9UdUnQMhcBQ4D+QGdghKp2KmB1uVJTUzXvFRfG5LVs2TJatWplFUiN76gqy5cvL1bXUNiOCFQ1U0SGAF8A0cAbqrpERG4JtI8GJuKSwGrgEHB9uOIx/hIXF8fOnTupWbOmJQPjGxoYmCYurniXw4ftiCBc7IjAhMKGqjR+VdhQlZ4cERjjpQoVKhRrqD5j/MwunzHGGJ+zRGCMMT5nicAYY3wu4k4Wi8h24CRvLaYWsKMEw4kEts/+YPvsD6eyz01UtcA7ciMuEZwKEZlb2Fnz8sr22R9sn/0hXPtsXUPGGONzlgiMMcbn/JYIXvU6AA/YPvuD7bM/hGWffXWOwBhjTH5+OyIwxhiThyUCY4zxuXKZCESkn4isEJHVIpJvIFJxRgTaF4lIBy/iLEkh7PNVgX1dJCIzRCTFizhL0on2OWi5s0UkKzBqXkQLZZ9FpKeILBCRJSIyrbRjLGkhvLcTRGSCiCwM7HNEVzEWkTdEZJuI/FxIe8l/fhU2hmWkPnAlr9cAzYBYYCGQlGeZ/sAk3Ahp5wCzvI67FPb5N0CNwPML/bDPQct9gyt5/juv4y6Fv3N1YCnQODBdx+u4S2GfHwT+GXheG9gFxHod+yns87lAB+DnQtpL/POrPB4RdAJWq+paVU0HxgKX5lnmUsCNIq46E6guIvVKO9ASdMJ9VtUZqro7MDkTNxpcJAvl7wxwB/ARsK00gwuTUPb5D8DHqvoLgKpG+n6Hss8KVBU38EQVXCLILN0wS46qfofbh8KU+OdXeUwEDYCNQdNpgXnFXSaSFHd/bsR9o4hkJ9xnEWkAXAaMLsW4wimUv3NLoIaIfCsi80TkmlKLLjxC2eeRQGvcMLeLgbtUNbt0wvNEiX9+lcfxCAoajirvNbKhLBNJQt4fEemFSwTdwhpR+IWyzy8Cw1Q1q5yMUhbKPscAHXFDxFYCfhSRmaq6MtzBhUko+9wXWACcBzQHvhKR71V1X7iD80iJf36Vx0SQBjQKmm6I+6ZQ3GUiSUj7IyLtgNeAC1V1ZynFFi6h7HMqMDaQBGoB/UUkU1XHlU6IJS7U9/YOVT0IHBSR74AU3PjhkSiUfb4eeEpdB/pqEVkHtAJml06Ipa7EP7/KY9fQHKCFiCSKSCwwCPg0zzKfAtcEzr6fA+xV1S2lHWgJOuE+i0hj4GPg6gj+dhjshPusqomq2lRVmwIfArdFcBKA0N7b44HuIhIjIvFAZ2BZKcdZkkLZ519wR0CISF3gTGBtqUZZukr886vcHRGoaqaIDAG+wF1x8IaqLhGRWwLto3FXkPQHVgOHcN8oIlaI+/w3oCbw78A35EyN4MqNIe5zuRLKPqvqMhGZDCwCsoHXVLXAyxAjQYh/58eAt0RkMa7bZJiqRmx5ahF5H+gJ1BKRNOARoAKE7/PLSkwYY4zPlceuIWOMMcVgicAYY3zOEoExxvicJQJjjPE5SwTGGONzlghMmROoFLog6NG0iGWbFlalsZjb/DZQ4XKhiPwgImeexDpuySnpICLXiUj9oLbXRCSphOOcIyLtQ3jN3YF7CowpkCUCUxYdVtX2QY/1pbTdq1Q1BXgbeKa4Lw5cx/9OYPI6oH5Q202qurREojwW578JLc67AUsEplCWCExECHzz/15E5gcevylgmTYiMjtwFLFIRFoE5v8xaP4rIhJ9gs19B5wReG1vEflJRBYH6sRXDMx/SkSWBrbzbGDeoyIyVNy4B6nAu4FtVgp8k08VkVtF5OmgmK8TkX+dZJw/ElRsTEReFpG54mryDw/MuxOXkKaKyNTAvAtE5MfA7/G/IlLlBNsx5ZwlAlMWVQrqFvokMG8bcL6qdgCuAEYU8LpbgJdUtT3ugzhNRFoHlu8amJ8FXHWC7V8MLBaROOAt4ApVTcbdiX+riJyGq2raRlXbAY8Hv1hVPwTm4r65t1fVw0HNHwK/DZq+AvjgJOPsBwSXzHgocLd4O6CHiLRT1RG4OjS9VLWXiNQCHgb6BH6Xc4F7T7AdU86VuxITplw4HPgwDFYBGBnoE8/ClVvO60fgIRFpiKvJv0pEeuOqcc4JlNaoROFjE7wrIoeB9bhxDM4E1gXVZnobuB1X9vgI8JqIfA58FuqOqep2EVkbqBGzKrCNHwLrLU6clXElF4JHp/q9iAzG/V/XA5JwpSaCnROY/0NgO7G435vxMUsEJlLcA2zFVdKMwn0QH0dV3xORWcBFwBcichOu9szbqvpACNu4SlXn5kyISM2CFgrUv+mEK3Q2CBiCK4Ecqg+A3wPLgU9UVcV9KoccJ26krqeAUcBvRSQRGAqcraq7ReQtIK6A1wrwlapeWYx4TTlnXUMmUiQAWwIDjlyN+zZ8HBFpBqwNdId8iusimQL8TkTqBJY5TUSahLjN5UBTETkjMH01MC3Qp56gqhNxJ2ILunJnP1C1kPV+DAwErsQlBYobp6pm4Lp4zgl0K1UDDgJ7xVXgvLCQWGYCXXP2SUTiRaSgoyvjI5YITKT4N3CtiMzEdQsdLGCZK4CfRWQBrh79O4ErdR4GvhSRRcBXuG6TE1LVI7jKjv8NVLbMxo12VhX4LLC+abijlbzeAkbnnCzOs97duHGFm6jq7MC8YscZOPfwHDBUVRcCPwFLgDdw3U05XgUmichUVd2Ou6Lp/cB2ZuJ+V8bHrPqoMcb4nB0RGGOMz1kiMMYYn7NEYIwxPmeJwBhjfM4SgTHG+JwlAmOM8TlLBMYY43P/D4Ym8AI+md25AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학습된 모델 테스트셋에서 성능 평가\n",
    "# 얼굴 스푸핑 분야에서 평가 metric으로 HTER이랑 EER 두 개 주로 사용\n",
    "# =========================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def main():\n",
    "    inputSize = 224\n",
    "    batchSize = 8\n",
    "    \n",
    "    trainDB = 'protocol_4'\n",
    "    testDB = 'protocol_4'\n",
    "    \n",
    "    dataDir = 'E:\\\\Face_Database\\\\B-Database'\n",
    "    modelPath = 'C:\\\\Users\\\\ysk00\\\\OneDrive\\\\바탕 화면\\\\prlab\\\\ysg\\\\densenet-spoofing\\\\result_cuda\\\\protocol_4-21-0.0745_0.001.hdf5'\n",
    "    \n",
    "    print('>> model loaded: {}'.format(os.path.basename(modelPath)))\n",
    "    K.clear_session()\n",
    "    model = load_model(modelPath)\n",
    "        \n",
    "    print(\">>>> evaluating on '{}'\".format(testDB))\n",
    "    test_datagen = ImageDataGenerator()\n",
    "\n",
    "    val_datagen = ImageDataGenerator()\n",
    "    \n",
    "    val_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    val_generator = val_datagen.flow_from_directory(os.path.join(*[dataDir, testDB, 'val']),\n",
    "                                                      target_size=(inputSize, inputSize),\n",
    "                                                      batch_size=batchSize,\n",
    "                                                      class_mode='binary',\n",
    "                                                      interpolation='bilinear',\n",
    "                                                      shuffle=False,)\n",
    "    \n",
    "    test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    test_generator = test_datagen.flow_from_directory(os.path.join(*[dataDir, testDB, 'test']),\n",
    "                                                      target_size=(inputSize, inputSize),\n",
    "                                                      batch_size=batchSize,\n",
    "                                                      class_mode='binary',\n",
    "                                                      interpolation='bilinear',\n",
    "                                                      shuffle=False,)\n",
    "\n",
    "    ''' evaluating EER '''\n",
    "    y_true = val_generator.classes\n",
    "    y_score = model.predict_generator(val_generator, steps=len(val_generator)).ravel()\n",
    "\n",
    "    fpr, tpr, threshold = roc_curve(y_true, y_score)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr-fpr)))]\n",
    "    val_eer = (fpr[np.nanargmin(np.absolute((fnr - fpr)))] + fnr[np.nanargmin(np.absolute((fnr - fpr)))]) / 2\n",
    "\n",
    "    ''' evaluating HTER '''\n",
    "    y_true = test_generator.classes\n",
    "    y_score = model.predict_generator(test_generator, steps = len(test_generator)).ravel()\n",
    "\n",
    "    # Calculate EER threshold\n",
    "    fpr, tpr, threshold = roc_curve(y_true, y_score)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr-fpr)))]\n",
    "\n",
    "    # HTER\n",
    "    y_pred = y_score > eer_threshold\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    labels = test_generator.class_indices\n",
    "    print('                  pred_fake({})   pred_real({})\\nactural_fake({})    {:12d}   {:12d}\\nactual_real({})    {:12d}   {:12d}\\n'.format(labels['Fake'], labels['Real'], labels['Fake'], tn, fp, labels['Real'], fn, tp))\n",
    "    hter = (fp/(tn+fp) + fn/(fn+tp)) * 0.5\n",
    "\n",
    "    # ROC curve\n",
    "    Accuracy = ((tn+tp) / (tn+fp+fn+tp)) * 100.0\n",
    "\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "\n",
    "    plt.plot(fpr, tpr, 'r--', label='ROC_curve (acc_1st = %0.2f%%)' % Accuracy)\n",
    "    plt.plot([0, 1], [0, 1], color = 'orange', lw=lw, linestyle='--')\n",
    "\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc = \"lower right\")\n",
    "\n",
    "    plt.savefig('protocol_4_001.png')\n",
    "    plt.figure()\n",
    "    \n",
    "    print('EER: {:.4f}\\tHTER: {:.4f}'.format(val_eer, hter))\n",
    "\n",
    "    print('>> finished')\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.005\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 230, 230, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1/conv (Conv2D)             (None, 112, 112, 64) 9408        zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1/bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1/conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1/relu (Activation)         (None, 112, 112, 64) 0           conv1/bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 114, 114, 64) 0           conv1/relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 56, 56, 64)   0           zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 56, 56, 64)   256         pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_relu (Activation (None, 56, 56, 64)   0           conv2_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 128)  8192        conv2_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 128)  0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_concat (Concatenat (None, 56, 56, 96)   0           pool1[0][0]                      \n",
      "                                                                 conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_bn (BatchNormali (None, 56, 56, 96)   384         conv2_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_relu (Activation (None, 56, 56, 96)   0           conv2_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 128)  12288       conv2_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 128)  0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_concat (Concatenat (None, 56, 56, 128)  0           conv2_block1_concat[0][0]        \n",
      "                                                                 conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_0_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_0_relu (Activation (None, 56, 56, 128)  0           conv2_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 128)  16384       conv2_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 128)  0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_concat (Concatenat (None, 56, 56, 160)  0           conv2_block2_concat[0][0]        \n",
      "                                                                 conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_0_bn (BatchNormali (None, 56, 56, 160)  640         conv2_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_0_relu (Activation (None, 56, 56, 160)  0           conv2_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_conv (Conv2D)    (None, 56, 56, 128)  20480       conv2_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_relu (Activation (None, 56, 56, 128)  0           conv2_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_concat (Concatenat (None, 56, 56, 192)  0           conv2_block3_concat[0][0]        \n",
      "                                                                 conv2_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_0_bn (BatchNormali (None, 56, 56, 192)  768         conv2_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_0_relu (Activation (None, 56, 56, 192)  0           conv2_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_conv (Conv2D)    (None, 56, 56, 128)  24576       conv2_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_relu (Activation (None, 56, 56, 128)  0           conv2_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_concat (Concatenat (None, 56, 56, 224)  0           conv2_block4_concat[0][0]        \n",
      "                                                                 conv2_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_0_bn (BatchNormali (None, 56, 56, 224)  896         conv2_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_0_relu (Activation (None, 56, 56, 224)  0           conv2_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_conv (Conv2D)    (None, 56, 56, 128)  28672       conv2_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_relu (Activation (None, 56, 56, 128)  0           conv2_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_concat (Concatenat (None, 56, 56, 256)  0           conv2_block5_concat[0][0]        \n",
      "                                                                 conv2_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool2_bn (BatchNormalization)   (None, 56, 56, 256)  1024        conv2_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool2_relu (Activation)         (None, 56, 56, 256)  0           pool2_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool2_conv (Conv2D)             (None, 56, 56, 128)  32768       pool2_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool2_pool (AveragePooling2D)   (None, 28, 28, 128)  0           pool2_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 28, 28, 128)  512         pool2_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_relu (Activation (None, 28, 28, 128)  0           conv3_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  16384       conv3_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_concat (Concatenat (None, 28, 28, 160)  0           pool2_pool[0][0]                 \n",
      "                                                                 conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_bn (BatchNormali (None, 28, 28, 160)  640         conv3_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_relu (Activation (None, 28, 28, 160)  0           conv3_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  20480       conv3_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_concat (Concatenat (None, 28, 28, 192)  0           conv3_block1_concat[0][0]        \n",
      "                                                                 conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_0_bn (BatchNormali (None, 28, 28, 192)  768         conv3_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_0_relu (Activation (None, 28, 28, 192)  0           conv3_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  24576       conv3_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_concat (Concatenat (None, 28, 28, 224)  0           conv3_block2_concat[0][0]        \n",
      "                                                                 conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_0_bn (BatchNormali (None, 28, 28, 224)  896         conv3_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_0_relu (Activation (None, 28, 28, 224)  0           conv3_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  28672       conv3_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_concat (Concatenat (None, 28, 28, 256)  0           conv3_block3_concat[0][0]        \n",
      "                                                                 conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_0_bn (BatchNormali (None, 28, 28, 256)  1024        conv3_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_0_relu (Activation (None, 28, 28, 256)  0           conv3_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_conv (Conv2D)    (None, 28, 28, 128)  32768       conv3_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_relu (Activation (None, 28, 28, 128)  0           conv3_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_concat (Concatenat (None, 28, 28, 288)  0           conv3_block4_concat[0][0]        \n",
      "                                                                 conv3_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_0_bn (BatchNormali (None, 28, 28, 288)  1152        conv3_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_0_relu (Activation (None, 28, 28, 288)  0           conv3_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_conv (Conv2D)    (None, 28, 28, 128)  36864       conv3_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_relu (Activation (None, 28, 28, 128)  0           conv3_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_concat (Concatenat (None, 28, 28, 320)  0           conv3_block5_concat[0][0]        \n",
      "                                                                 conv3_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_0_bn (BatchNormali (None, 28, 28, 320)  1280        conv3_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_0_relu (Activation (None, 28, 28, 320)  0           conv3_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_conv (Conv2D)    (None, 28, 28, 128)  40960       conv3_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_relu (Activation (None, 28, 28, 128)  0           conv3_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_concat (Concatenat (None, 28, 28, 352)  0           conv3_block6_concat[0][0]        \n",
      "                                                                 conv3_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_0_bn (BatchNormali (None, 28, 28, 352)  1408        conv3_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_0_relu (Activation (None, 28, 28, 352)  0           conv3_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_conv (Conv2D)    (None, 28, 28, 128)  45056       conv3_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_relu (Activation (None, 28, 28, 128)  0           conv3_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_concat (Concatenat (None, 28, 28, 384)  0           conv3_block7_concat[0][0]        \n",
      "                                                                 conv3_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_0_bn (BatchNormali (None, 28, 28, 384)  1536        conv3_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_0_relu (Activation (None, 28, 28, 384)  0           conv3_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_conv (Conv2D)    (None, 28, 28, 128)  49152       conv3_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_relu (Activation (None, 28, 28, 128)  0           conv3_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_concat (Concatenat (None, 28, 28, 416)  0           conv3_block8_concat[0][0]        \n",
      "                                                                 conv3_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_0_bn (BatchNormal (None, 28, 28, 416)  1664        conv3_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_0_relu (Activatio (None, 28, 28, 416)  0           conv3_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_conv (Conv2D)   (None, 28, 28, 128)  53248       conv3_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_concat (Concatena (None, 28, 28, 448)  0           conv3_block9_concat[0][0]        \n",
      "                                                                 conv3_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_0_bn (BatchNormal (None, 28, 28, 448)  1792        conv3_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_0_relu (Activatio (None, 28, 28, 448)  0           conv3_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_conv (Conv2D)   (None, 28, 28, 128)  57344       conv3_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_concat (Concatena (None, 28, 28, 480)  0           conv3_block10_concat[0][0]       \n",
      "                                                                 conv3_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_0_bn (BatchNormal (None, 28, 28, 480)  1920        conv3_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_0_relu (Activatio (None, 28, 28, 480)  0           conv3_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_conv (Conv2D)   (None, 28, 28, 128)  61440       conv3_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_concat (Concatena (None, 28, 28, 512)  0           conv3_block11_concat[0][0]       \n",
      "                                                                 conv3_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3_bn (BatchNormalization)   (None, 28, 28, 512)  2048        conv3_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3_relu (Activation)         (None, 28, 28, 512)  0           pool3_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool3_conv (Conv2D)             (None, 28, 28, 256)  131072      pool3_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool3_pool (AveragePooling2D)   (None, 14, 14, 256)  0           pool3_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 256)  1024        pool3_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_relu (Activation (None, 14, 14, 256)  0           conv4_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 128)  32768       conv4_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 128)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_concat (Concatenat (None, 14, 14, 288)  0           pool3_pool[0][0]                 \n",
      "                                                                 conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_bn (BatchNormali (None, 14, 14, 288)  1152        conv4_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_relu (Activation (None, 14, 14, 288)  0           conv4_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 128)  36864       conv4_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 128)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_concat (Concatenat (None, 14, 14, 320)  0           conv4_block1_concat[0][0]        \n",
      "                                                                 conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_0_bn (BatchNormali (None, 14, 14, 320)  1280        conv4_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_0_relu (Activation (None, 14, 14, 320)  0           conv4_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 128)  40960       conv4_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 128)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_concat (Concatenat (None, 14, 14, 352)  0           conv4_block2_concat[0][0]        \n",
      "                                                                 conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_0_bn (BatchNormali (None, 14, 14, 352)  1408        conv4_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_0_relu (Activation (None, 14, 14, 352)  0           conv4_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 128)  45056       conv4_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 128)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_concat (Concatenat (None, 14, 14, 384)  0           conv4_block3_concat[0][0]        \n",
      "                                                                 conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_0_bn (BatchNormali (None, 14, 14, 384)  1536        conv4_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_0_relu (Activation (None, 14, 14, 384)  0           conv4_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 128)  49152       conv4_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 128)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_concat (Concatenat (None, 14, 14, 416)  0           conv4_block4_concat[0][0]        \n",
      "                                                                 conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_0_bn (BatchNormali (None, 14, 14, 416)  1664        conv4_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_0_relu (Activation (None, 14, 14, 416)  0           conv4_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 128)  53248       conv4_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 128)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_concat (Concatenat (None, 14, 14, 448)  0           conv4_block5_concat[0][0]        \n",
      "                                                                 conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_0_bn (BatchNormali (None, 14, 14, 448)  1792        conv4_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_0_relu (Activation (None, 14, 14, 448)  0           conv4_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_conv (Conv2D)    (None, 14, 14, 128)  57344       conv4_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_relu (Activation (None, 14, 14, 128)  0           conv4_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_concat (Concatenat (None, 14, 14, 480)  0           conv4_block6_concat[0][0]        \n",
      "                                                                 conv4_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_0_bn (BatchNormali (None, 14, 14, 480)  1920        conv4_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_0_relu (Activation (None, 14, 14, 480)  0           conv4_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_conv (Conv2D)    (None, 14, 14, 128)  61440       conv4_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_relu (Activation (None, 14, 14, 128)  0           conv4_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_concat (Concatenat (None, 14, 14, 512)  0           conv4_block7_concat[0][0]        \n",
      "                                                                 conv4_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_0_bn (BatchNormali (None, 14, 14, 512)  2048        conv4_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_0_relu (Activation (None, 14, 14, 512)  0           conv4_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_conv (Conv2D)    (None, 14, 14, 128)  65536       conv4_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_relu (Activation (None, 14, 14, 128)  0           conv4_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_concat (Concatenat (None, 14, 14, 544)  0           conv4_block8_concat[0][0]        \n",
      "                                                                 conv4_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_0_bn (BatchNormal (None, 14, 14, 544)  2176        conv4_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_0_relu (Activatio (None, 14, 14, 544)  0           conv4_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_conv (Conv2D)   (None, 14, 14, 128)  69632       conv4_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_concat (Concatena (None, 14, 14, 576)  0           conv4_block9_concat[0][0]        \n",
      "                                                                 conv4_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_0_bn (BatchNormal (None, 14, 14, 576)  2304        conv4_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_0_relu (Activatio (None, 14, 14, 576)  0           conv4_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_conv (Conv2D)   (None, 14, 14, 128)  73728       conv4_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_concat (Concatena (None, 14, 14, 608)  0           conv4_block10_concat[0][0]       \n",
      "                                                                 conv4_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_0_bn (BatchNormal (None, 14, 14, 608)  2432        conv4_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_0_relu (Activatio (None, 14, 14, 608)  0           conv4_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_conv (Conv2D)   (None, 14, 14, 128)  77824       conv4_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_concat (Concatena (None, 14, 14, 640)  0           conv4_block11_concat[0][0]       \n",
      "                                                                 conv4_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_0_bn (BatchNormal (None, 14, 14, 640)  2560        conv4_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_0_relu (Activatio (None, 14, 14, 640)  0           conv4_block13_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_conv (Conv2D)   (None, 14, 14, 128)  81920       conv4_block13_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_concat (Concatena (None, 14, 14, 672)  0           conv4_block12_concat[0][0]       \n",
      "                                                                 conv4_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_0_bn (BatchNormal (None, 14, 14, 672)  2688        conv4_block13_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_0_relu (Activatio (None, 14, 14, 672)  0           conv4_block14_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_conv (Conv2D)   (None, 14, 14, 128)  86016       conv4_block14_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_concat (Concatena (None, 14, 14, 704)  0           conv4_block13_concat[0][0]       \n",
      "                                                                 conv4_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_0_bn (BatchNormal (None, 14, 14, 704)  2816        conv4_block14_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_0_relu (Activatio (None, 14, 14, 704)  0           conv4_block15_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_conv (Conv2D)   (None, 14, 14, 128)  90112       conv4_block15_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_concat (Concatena (None, 14, 14, 736)  0           conv4_block14_concat[0][0]       \n",
      "                                                                 conv4_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_0_bn (BatchNormal (None, 14, 14, 736)  2944        conv4_block15_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_0_relu (Activatio (None, 14, 14, 736)  0           conv4_block16_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_conv (Conv2D)   (None, 14, 14, 128)  94208       conv4_block16_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_concat (Concatena (None, 14, 14, 768)  0           conv4_block15_concat[0][0]       \n",
      "                                                                 conv4_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_0_bn (BatchNormal (None, 14, 14, 768)  3072        conv4_block16_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_0_relu (Activatio (None, 14, 14, 768)  0           conv4_block17_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_conv (Conv2D)   (None, 14, 14, 128)  98304       conv4_block17_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block17_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block17_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block17_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_concat (Concatena (None, 14, 14, 800)  0           conv4_block16_concat[0][0]       \n",
      "                                                                 conv4_block17_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_0_bn (BatchNormal (None, 14, 14, 800)  3200        conv4_block17_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_0_relu (Activatio (None, 14, 14, 800)  0           conv4_block18_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_conv (Conv2D)   (None, 14, 14, 128)  102400      conv4_block18_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block18_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block18_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block18_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_concat (Concatena (None, 14, 14, 832)  0           conv4_block17_concat[0][0]       \n",
      "                                                                 conv4_block18_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_0_bn (BatchNormal (None, 14, 14, 832)  3328        conv4_block18_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_0_relu (Activatio (None, 14, 14, 832)  0           conv4_block19_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_conv (Conv2D)   (None, 14, 14, 128)  106496      conv4_block19_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block19_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block19_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block19_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_concat (Concatena (None, 14, 14, 864)  0           conv4_block18_concat[0][0]       \n",
      "                                                                 conv4_block19_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_0_bn (BatchNormal (None, 14, 14, 864)  3456        conv4_block19_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_0_relu (Activatio (None, 14, 14, 864)  0           conv4_block20_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_conv (Conv2D)   (None, 14, 14, 128)  110592      conv4_block20_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block20_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block20_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block20_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_concat (Concatena (None, 14, 14, 896)  0           conv4_block19_concat[0][0]       \n",
      "                                                                 conv4_block20_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_0_bn (BatchNormal (None, 14, 14, 896)  3584        conv4_block20_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_0_relu (Activatio (None, 14, 14, 896)  0           conv4_block21_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_conv (Conv2D)   (None, 14, 14, 128)  114688      conv4_block21_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block21_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block21_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block21_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_concat (Concatena (None, 14, 14, 928)  0           conv4_block20_concat[0][0]       \n",
      "                                                                 conv4_block21_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_0_bn (BatchNormal (None, 14, 14, 928)  3712        conv4_block21_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_0_relu (Activatio (None, 14, 14, 928)  0           conv4_block22_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_conv (Conv2D)   (None, 14, 14, 128)  118784      conv4_block22_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block22_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block22_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block22_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_concat (Concatena (None, 14, 14, 960)  0           conv4_block21_concat[0][0]       \n",
      "                                                                 conv4_block22_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_0_bn (BatchNormal (None, 14, 14, 960)  3840        conv4_block22_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_0_relu (Activatio (None, 14, 14, 960)  0           conv4_block23_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_conv (Conv2D)   (None, 14, 14, 128)  122880      conv4_block23_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block23_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block23_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block23_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_concat (Concatena (None, 14, 14, 992)  0           conv4_block22_concat[0][0]       \n",
      "                                                                 conv4_block23_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_0_bn (BatchNormal (None, 14, 14, 992)  3968        conv4_block23_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_0_relu (Activatio (None, 14, 14, 992)  0           conv4_block24_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_conv (Conv2D)   (None, 14, 14, 128)  126976      conv4_block24_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block24_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block24_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block24_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_concat (Concatena (None, 14, 14, 1024) 0           conv4_block23_concat[0][0]       \n",
      "                                                                 conv4_block24_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool4_bn (BatchNormalization)   (None, 14, 14, 1024) 4096        conv4_block24_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool4_relu (Activation)         (None, 14, 14, 1024) 0           pool4_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool4_conv (Conv2D)             (None, 14, 14, 512)  524288      pool4_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool4_pool (AveragePooling2D)   (None, 7, 7, 512)    0           pool4_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 512)    2048        pool4_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_relu (Activation (None, 7, 7, 512)    0           conv5_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 128)    65536       conv5_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 128)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_concat (Concatenat (None, 7, 7, 544)    0           pool4_pool[0][0]                 \n",
      "                                                                 conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_0_bn (BatchNormali (None, 7, 7, 544)    2176        conv5_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_0_relu (Activation (None, 7, 7, 544)    0           conv5_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 128)    69632       conv5_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 128)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_concat (Concatenat (None, 7, 7, 576)    0           conv5_block1_concat[0][0]        \n",
      "                                                                 conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_0_bn (BatchNormali (None, 7, 7, 576)    2304        conv5_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_0_relu (Activation (None, 7, 7, 576)    0           conv5_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 128)    73728       conv5_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 128)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_concat (Concatenat (None, 7, 7, 608)    0           conv5_block2_concat[0][0]        \n",
      "                                                                 conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_0_bn (BatchNormali (None, 7, 7, 608)    2432        conv5_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_0_relu (Activation (None, 7, 7, 608)    0           conv5_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_conv (Conv2D)    (None, 7, 7, 128)    77824       conv5_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_relu (Activation (None, 7, 7, 128)    0           conv5_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_concat (Concatenat (None, 7, 7, 640)    0           conv5_block3_concat[0][0]        \n",
      "                                                                 conv5_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_0_bn (BatchNormali (None, 7, 7, 640)    2560        conv5_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_0_relu (Activation (None, 7, 7, 640)    0           conv5_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_conv (Conv2D)    (None, 7, 7, 128)    81920       conv5_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_relu (Activation (None, 7, 7, 128)    0           conv5_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_concat (Concatenat (None, 7, 7, 672)    0           conv5_block4_concat[0][0]        \n",
      "                                                                 conv5_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_0_bn (BatchNormali (None, 7, 7, 672)    2688        conv5_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_0_relu (Activation (None, 7, 7, 672)    0           conv5_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_conv (Conv2D)    (None, 7, 7, 128)    86016       conv5_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_relu (Activation (None, 7, 7, 128)    0           conv5_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_concat (Concatenat (None, 7, 7, 704)    0           conv5_block5_concat[0][0]        \n",
      "                                                                 conv5_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_0_bn (BatchNormali (None, 7, 7, 704)    2816        conv5_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_0_relu (Activation (None, 7, 7, 704)    0           conv5_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_conv (Conv2D)    (None, 7, 7, 128)    90112       conv5_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_relu (Activation (None, 7, 7, 128)    0           conv5_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_concat (Concatenat (None, 7, 7, 736)    0           conv5_block6_concat[0][0]        \n",
      "                                                                 conv5_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_0_bn (BatchNormali (None, 7, 7, 736)    2944        conv5_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_0_relu (Activation (None, 7, 7, 736)    0           conv5_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_conv (Conv2D)    (None, 7, 7, 128)    94208       conv5_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_relu (Activation (None, 7, 7, 128)    0           conv5_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_concat (Concatenat (None, 7, 7, 768)    0           conv5_block7_concat[0][0]        \n",
      "                                                                 conv5_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_0_bn (BatchNormali (None, 7, 7, 768)    3072        conv5_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_0_relu (Activation (None, 7, 7, 768)    0           conv5_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_conv (Conv2D)    (None, 7, 7, 128)    98304       conv5_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_relu (Activation (None, 7, 7, 128)    0           conv5_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_concat (Concatenat (None, 7, 7, 800)    0           conv5_block8_concat[0][0]        \n",
      "                                                                 conv5_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_0_bn (BatchNormal (None, 7, 7, 800)    3200        conv5_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_0_relu (Activatio (None, 7, 7, 800)    0           conv5_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_conv (Conv2D)   (None, 7, 7, 128)    102400      conv5_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_concat (Concatena (None, 7, 7, 832)    0           conv5_block9_concat[0][0]        \n",
      "                                                                 conv5_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_0_bn (BatchNormal (None, 7, 7, 832)    3328        conv5_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_0_relu (Activatio (None, 7, 7, 832)    0           conv5_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_conv (Conv2D)   (None, 7, 7, 128)    106496      conv5_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_concat (Concatena (None, 7, 7, 864)    0           conv5_block10_concat[0][0]       \n",
      "                                                                 conv5_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_0_bn (BatchNormal (None, 7, 7, 864)    3456        conv5_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_0_relu (Activatio (None, 7, 7, 864)    0           conv5_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_conv (Conv2D)   (None, 7, 7, 128)    110592      conv5_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_concat (Concatena (None, 7, 7, 896)    0           conv5_block11_concat[0][0]       \n",
      "                                                                 conv5_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_0_bn (BatchNormal (None, 7, 7, 896)    3584        conv5_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_0_relu (Activatio (None, 7, 7, 896)    0           conv5_block13_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_conv (Conv2D)   (None, 7, 7, 128)    114688      conv5_block13_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_concat (Concatena (None, 7, 7, 928)    0           conv5_block12_concat[0][0]       \n",
      "                                                                 conv5_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_0_bn (BatchNormal (None, 7, 7, 928)    3712        conv5_block13_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_0_relu (Activatio (None, 7, 7, 928)    0           conv5_block14_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_conv (Conv2D)   (None, 7, 7, 128)    118784      conv5_block14_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_concat (Concatena (None, 7, 7, 960)    0           conv5_block13_concat[0][0]       \n",
      "                                                                 conv5_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_0_bn (BatchNormal (None, 7, 7, 960)    3840        conv5_block14_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_0_relu (Activatio (None, 7, 7, 960)    0           conv5_block15_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_conv (Conv2D)   (None, 7, 7, 128)    122880      conv5_block15_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_concat (Concatena (None, 7, 7, 992)    0           conv5_block14_concat[0][0]       \n",
      "                                                                 conv5_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_0_bn (BatchNormal (None, 7, 7, 992)    3968        conv5_block15_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_0_relu (Activatio (None, 7, 7, 992)    0           conv5_block16_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_conv (Conv2D)   (None, 7, 7, 128)    126976      conv5_block16_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_concat (Concatena (None, 7, 7, 1024)   0           conv5_block15_concat[0][0]       \n",
      "                                                                 conv5_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bn (BatchNormalization)         (None, 7, 7, 1024)   4096        conv5_block16_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "relu (Activation)               (None, 7, 7, 1024)   0           bn[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 1024)         0           relu[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            1025        global_average_pooling2d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 7,038,529\n",
      "Trainable params: 6,954,881\n",
      "Non-trainable params: 83,648\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "#keras.applications.densenet.DenseNet121(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n",
    "from tensorflow.keras.applications.densenet import DenseNet121\n",
    "#from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.densenet import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Activation\n",
    "\n",
    "def Densenet121(show_layers,weights,input_shape):\n",
    "        base_model = DenseNet121(include_top=False, weights=weights, input_shape=input_shape)\n",
    "        x = GlobalAveragePooling2D()(base_model.output)\n",
    "        pred = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "        model = Model(inputs=base_model.input, outputs=pred)\n",
    "        model.compile(optimizer=SGD(lr=5e-3, decay=5e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "        if show_layers:\n",
    "            for i, layer in enumerate(model.layers):\n",
    "                print(i, layer.name, layer.trainable)\n",
    "        return model\n",
    "    \n",
    "model = Densenet121(False,weights=None,input_shape=(224, 224, 3))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>  protocol_4\n",
      "============================================Densenet121=============================================\n",
      "Found 41040 images belonging to 2 classes.\n",
      "Found 30240 images belonging to 2 classes.\n",
      "train shape : (41040,)\n",
      "Epoch 1/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.2183 - accuracy: 0.9115\n",
      "Epoch 00001: val_loss improved from inf to 0.26982, saving model to .\\result_cuda\\protocol_4-01-0.2698.hdf5\n",
      "5130/5130 [==============================] - 3743s 730ms/step - loss: 0.2182 - accuracy: 0.9115 - val_loss: 0.2698 - val_accuracy: 0.9080\n",
      "Epoch 2/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0756 - accuracy: 0.9735\n",
      "Epoch 00002: val_loss improved from 0.26982 to 0.15320, saving model to .\\result_cuda\\protocol_4-02-0.1532.hdf5\n",
      "5130/5130 [==============================] - 3620s 706ms/step - loss: 0.0756 - accuracy: 0.9735 - val_loss: 0.1532 - val_accuracy: 0.9500\n",
      "Epoch 3/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0414 - accuracy: 0.9871\n",
      "Epoch 00003: val_loss did not improve from 0.15320\n",
      "5130/5130 [==============================] - 3579s 698ms/step - loss: 0.0414 - accuracy: 0.9871 - val_loss: 0.1826 - val_accuracy: 0.9521\n",
      "Epoch 4/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0274 - accuracy: 0.9919\n",
      "Epoch 00004: val_loss improved from 0.15320 to 0.11390, saving model to .\\result_cuda\\protocol_4-04-0.1139.hdf5\n",
      "5130/5130 [==============================] - 3555s 693ms/step - loss: 0.0274 - accuracy: 0.9919 - val_loss: 0.1139 - val_accuracy: 0.9648\n",
      "Epoch 5/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0144 - accuracy: 0.9964\n",
      "Epoch 00005: val_loss did not improve from 0.11390\n",
      "5130/5130 [==============================] - 3614s 704ms/step - loss: 0.0144 - accuracy: 0.9964 - val_loss: 0.1390 - val_accuracy: 0.9559\n",
      "Epoch 6/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0120 - accuracy: 0.9974\n",
      "Epoch 00006: val_loss improved from 0.11390 to 0.11197, saving model to .\\result_cuda\\protocol_4-06-0.1120.hdf5\n",
      "5130/5130 [==============================] - 3587s 699ms/step - loss: 0.0120 - accuracy: 0.9974 - val_loss: 0.1120 - val_accuracy: 0.9640\n",
      "Epoch 7/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9985\n",
      "Epoch 00007: val_loss did not improve from 0.11197\n",
      "5130/5130 [==============================] - 3592s 700ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.1362 - val_accuracy: 0.9514\n",
      "Epoch 8/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0079 - accuracy: 0.9984\n",
      "Epoch 00008: val_loss did not improve from 0.11197\n",
      "5130/5130 [==============================] - 3535s 689ms/step - loss: 0.0079 - accuracy: 0.9984 - val_loss: 0.1578 - val_accuracy: 0.9391\n",
      "Epoch 9/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0088 - accuracy: 0.9984\n",
      "Epoch 00009: val_loss did not improve from 0.11197\n",
      "5130/5130 [==============================] - 3580s 698ms/step - loss: 0.0088 - accuracy: 0.9984 - val_loss: 0.1357 - val_accuracy: 0.9600\n",
      "Epoch 10/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0085 - accuracy: 0.9981\n",
      "Epoch 00010: val_loss did not improve from 0.11197\n",
      "5130/5130 [==============================] - 3553s 693ms/step - loss: 0.0085 - accuracy: 0.9981 - val_loss: 0.1211 - val_accuracy: 0.9571\n",
      "Epoch 11/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9986\n",
      "Epoch 00011: val_loss improved from 0.11197 to 0.10787, saving model to .\\result_cuda\\protocol_4-11-0.1079.hdf5\n",
      "5130/5130 [==============================] - 3875s 755ms/step - loss: 0.0073 - accuracy: 0.9986 - val_loss: 0.1079 - val_accuracy: 0.9660\n",
      "Epoch 12/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9993\n",
      "Epoch 00012: val_loss improved from 0.10787 to 0.10179, saving model to .\\result_cuda\\protocol_4-12-0.1018.hdf5\n",
      "5130/5130 [==============================] - 3799s 741ms/step - loss: 0.0045 - accuracy: 0.9993 - val_loss: 0.1018 - val_accuracy: 0.9651\n",
      "Epoch 13/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0068 - accuracy: 0.9987\n",
      "Epoch 00013: val_loss improved from 0.10179 to 0.09769, saving model to .\\result_cuda\\protocol_4-13-0.0977.hdf5\n",
      "5130/5130 [==============================] - 3535s 689ms/step - loss: 0.0068 - accuracy: 0.9987 - val_loss: 0.0977 - val_accuracy: 0.9640\n",
      "Epoch 14/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0034 - accuracy: 0.9996\n",
      "Epoch 00014: val_loss improved from 0.09769 to 0.09742, saving model to .\\result_cuda\\protocol_4-14-0.0974.hdf5\n",
      "5130/5130 [==============================] - 3557s 693ms/step - loss: 0.0034 - accuracy: 0.9996 - val_loss: 0.0974 - val_accuracy: 0.9647\n",
      "Epoch 15/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9993\n",
      "Epoch 00015: val_loss did not improve from 0.09742\n",
      "5130/5130 [==============================] - 3571s 696ms/step - loss: 0.0045 - accuracy: 0.9993 - val_loss: 0.1228 - val_accuracy: 0.9550\n",
      "Epoch 16/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0070 - accuracy: 0.9988\n",
      "Epoch 00016: val_loss did not improve from 0.09742\n",
      "5130/5130 [==============================] - 3557s 693ms/step - loss: 0.0070 - accuracy: 0.9988 - val_loss: 0.1130 - val_accuracy: 0.9606\n",
      "Epoch 17/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9992\n",
      "Epoch 00017: val_loss did not improve from 0.09742\n",
      "5130/5130 [==============================] - 3544s 691ms/step - loss: 0.0048 - accuracy: 0.9992 - val_loss: 0.1168 - val_accuracy: 0.9632\n",
      "Epoch 18/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9993\n",
      "Epoch 00018: val_loss did not improve from 0.09742\n",
      "5130/5130 [==============================] - 3539s 690ms/step - loss: 0.0045 - accuracy: 0.9993 - val_loss: 0.1320 - val_accuracy: 0.9557\n",
      "Epoch 19/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 0.9997\n",
      "Epoch 00019: val_loss did not improve from 0.09742\n",
      "5130/5130 [==============================] - 3623s 706ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 0.1057 - val_accuracy: 0.9639\n",
      "Epoch 20/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0053 - accuracy: 0.9992\n",
      "Epoch 00020: val_loss did not improve from 0.09742\n",
      "5130/5130 [==============================] - 3549s 692ms/step - loss: 0.0053 - accuracy: 0.9992 - val_loss: 0.1018 - val_accuracy: 0.9644\n",
      "Epoch 21/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0056 - accuracy: 0.9990\n",
      "Epoch 00021: val_loss did not improve from 0.09742\n",
      "5130/5130 [==============================] - 3568s 695ms/step - loss: 0.0056 - accuracy: 0.9990 - val_loss: 0.1221 - val_accuracy: 0.9576\n",
      "Epoch 22/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0064 - accuracy: 0.9991\n",
      "Epoch 00022: val_loss did not improve from 0.09742\n",
      "5130/5130 [==============================] - 3861s 753ms/step - loss: 0.0064 - accuracy: 0.9991 - val_loss: 0.3267 - val_accuracy: 0.8795\n",
      "Epoch 23/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0023 - accuracy: 0.9996\n",
      "Epoch 00023: val_loss did not improve from 0.09742\n",
      "5130/5130 [==============================] - 3958s 772ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.1018 - val_accuracy: 0.9668\n",
      "Epoch 24/100\n",
      "5129/5130 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 0.9994\n",
      "Epoch 00024: val_loss did not improve from 0.09742\n",
      "5130/5130 [==============================] - 3576s 697ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.1020 - val_accuracy: 0.9668\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9647\tloss: 0.0974\n",
      "========================================protocol_4 finished=========================================\n"
     ]
    }
   ],
   "source": [
    "#### import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "%matplotlib inline\n",
    "SEED = 777\n",
    "\n",
    "def main():\n",
    "    inputSize = 224\n",
    "    batchSize = 8\n",
    "    nEpoch = 100\n",
    "    \n",
    "    # 디렉토리 정보 설정\n",
    "    # dataDir = 'D:\\\\Face_Database\\\\B-Database'\n",
    "    dataDir = 'E:\\\\Face_Database\\\\B-Database'   \n",
    "    # 학습 및 테스트 할 데이터베이스 디렉토리명\n",
    "    trainDB = 'protocol_4'  # D:\\Face_Database\\B-Database\\protocol_4\\train\n",
    "    validDB = 'protocol_4'\n",
    "    \n",
    "    saveDir =  '.\\\\result_cuda'\n",
    "    if not os.path.exists(saveDir):    \n",
    "        os.makedirs(saveDir)\n",
    "    model_path = os.path.join(saveDir, trainDB + '-{epoch:02d}-{val_loss:.4f}.hdf5')\n",
    "    \n",
    "    # 데이터베이스별 학습 수행\n",
    "    print('>> ', trainDB)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    print('Densenet121'.center(100, '='))\n",
    "   \n",
    "    '''\n",
    "    training\n",
    "    '''\n",
    "    K.clear_session()\n",
    "\n",
    "    # 네트워크 정의\n",
    "    model = Densenet121(False,weights=None,input_shape=(224, 224, 3))\n",
    "\n",
    "    # 데이터 generator 생성\n",
    "    train_datagen = ImageDataGenerator(rescale=1.0/255.0,\n",
    "                                       horizontal_flip=True,)#이미지augmentation > 이미지를 회전,확대,축소 등 여러변형해보는것.\n",
    "    train_generator = train_datagen.flow_from_directory(os.path.join(*[dataDir,trainDB,'train']),\n",
    "                                                        target_size=(inputSize, inputSize),\n",
    "                                                        batch_size=batchSize,\n",
    "                                                        class_mode='binary',\n",
    "                                                        interpolation='bilinear',\n",
    "                                                        shuffle=True,)\n",
    "\n",
    "    valid_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    valid_generator = valid_datagen.flow_from_directory(os.path.join(*[dataDir, validDB,'val']),\n",
    "                                                        target_size=(inputSize, inputSize),\n",
    "                                                        batch_size=batchSize,\n",
    "                                                        class_mode='binary',\n",
    "                                                        interpolation='bilinear',\n",
    "                                                        shuffle=False,)\n",
    "\n",
    "    print('train shape :',train_generator.classes.shape)\n",
    "    # unbalanced class를 해결하기 위한 class_weight 설정 #np.unique는 중복원소제거하는거\n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.unique(train_generator.classes), train_generator.classes) \n",
    "\n",
    "    # callback 함수 정의 dropout함수\n",
    "    early_stop = EarlyStopping(patience=10, monitor='val_loss')\n",
    "    cb_checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "    # 모델 학습\n",
    "    hist = model.fit_generator(train_generator, \n",
    "                               epochs=nEpoch,\n",
    "                               steps_per_epoch=len(train_generator),\n",
    "                               class_weight=class_weights,\n",
    "                               validation_data=valid_generator,\n",
    "                               validation_steps=len(valid_generator),\n",
    "                               verbose=1,\n",
    "                               callbacks=[early_stop, cb_checkpoint])\n",
    "\n",
    "    \n",
    "    # 학습 결과 그래프 그리기\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, loss_ax = plt.subplots()\n",
    "    acc_ax = loss_ax.twinx()\n",
    "\n",
    "    loss_ax.plot(hist.history['loss'], 'b', label='train loss')\n",
    "    loss_ax.plot(hist.history['val_loss'], '--r', label='val loss')\n",
    "    loss_ax.set_xlabel('epoch')\n",
    "    loss_ax.set_ylabel('loss')\n",
    "    loss_ax.legend(loc='upper left')\n",
    "\n",
    "    plt.show()\n",
    "    print('acc: {:.4f}\\tloss: {:.4f}'.format(hist.history['val_accuracy'][-11], hist.history['val_loss'][-11]))\n",
    "\n",
    "    print(('{} finished'.format(trainDB)).center(100,'='))\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> model loaded: protocol_4-21-0.0745_0.001.hdf5\n",
      ">>>> evaluating on 'protocol_4'\n",
      "Found 30240 images belonging to 2 classes.\n",
      "Found 56160 images belonging to 2 classes.\n",
      "                  pred_fake(0)   pred_real(1)\n",
      "actural_fake(0)           36565            875\n",
      "actual_real(1)             438          18282\n",
      "\n",
      "EER: 0.0285\tHTER: 0.0234\n",
      ">> finished\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학습된 모델 테스트셋에서 성능 평가\n",
    "# 얼굴 스푸핑 분야에서 평가 metric으로 HTER이랑 EER 두 개 주로 사용\n",
    "# =========================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def main():\n",
    "    inputSize = 224\n",
    "    batchSize = 8\n",
    "    \n",
    "    trainDB = 'protocol_4'\n",
    "    testDB = 'protocol_4'\n",
    "    \n",
    "    dataDir = 'E:\\\\Face_Database\\\\B-Database'\n",
    "    modelPath = 'C:\\\\Users\\\\ysk00\\\\OneDrive\\\\바탕 화면\\\\prlab\\\\ysg\\\\densenet-spoofing\\\\result_cuda\\\\protocol_4-21-0.0745_0.001.hdf5'\n",
    "    \n",
    "    print('>> model loaded: {}'.format(os.path.basename(modelPath)))\n",
    "    K.clear_session()\n",
    "    model = load_model(modelPath)\n",
    "        \n",
    "    print(\">>>> evaluating on '{}'\".format(testDB))\n",
    "    test_datagen = ImageDataGenerator()\n",
    "\n",
    "    val_datagen = ImageDataGenerator()\n",
    "    \n",
    "    val_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    val_generator = val_datagen.flow_from_directory(os.path.join(*[dataDir, testDB, 'val']),\n",
    "                                                      target_size=(inputSize, inputSize),\n",
    "                                                      batch_size=batchSize,\n",
    "                                                      class_mode='binary',\n",
    "                                                      interpolation='bilinear',\n",
    "                                                      shuffle=False,)\n",
    "    \n",
    "    test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    test_generator = test_datagen.flow_from_directory(os.path.join(*[dataDir, testDB, 'test']),\n",
    "                                                      target_size=(inputSize, inputSize),\n",
    "                                                      batch_size=batchSize,\n",
    "                                                      class_mode='binary',\n",
    "                                                      interpolation='bilinear',\n",
    "                                                      shuffle=False,)\n",
    "\n",
    "    ''' evaluating EER '''\n",
    "    y_true = val_generator.classes\n",
    "    y_score = model.predict_generator(val_generator, steps=len(val_generator)).ravel()\n",
    "\n",
    "    fpr, tpr, threshold = roc_curve(y_true, y_score)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr-fpr)))]\n",
    "    val_eer = (fpr[np.nanargmin(np.absolute((fnr - fpr)))] + fnr[np.nanargmin(np.absolute((fnr - fpr)))]) / 2\n",
    "\n",
    "    ''' evaluating HTER '''\n",
    "    y_true = test_generator.classes\n",
    "    y_score = model.predict_generator(test_generator, steps = len(test_generator)).ravel()\n",
    "\n",
    "    # Calculate EER threshold\n",
    "    fpr, tpr, threshold = roc_curve(y_true, y_score)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr-fpr)))]\n",
    "\n",
    "    # HTER\n",
    "    y_pred = y_score > eer_threshold\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    labels = test_generator.class_indices\n",
    "    print('                  pred_fake({})   pred_real({})\\nactural_fake({})    {:12d}   {:12d}\\nactual_real({})    {:12d}   {:12d}\\n'.format(labels['Fake'], labels['Real'], labels['Fake'], tn, fp, labels['Real'], fn, tp))\n",
    "    hter = (fp/(tn+fp) + fn/(fn+tp)) * 0.5\n",
    "\n",
    "    # ROC curve\n",
    "    Accuracy = ((tn+tp) / (tn+fp+fn+tp)) * 100.0\n",
    "\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "\n",
    "    plt.plot(fpr, tpr, 'r--', label='ROC_curve (acc_1st = %0.2f%%)' % Accuracy)\n",
    "    plt.plot([0, 1], [0, 1], color = 'orange', lw=lw, linestyle='--')\n",
    "\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc = \"lower right\")\n",
    "\n",
    "    plt.savefig('protocol_4_001.png')\n",
    "    plt.figure()\n",
    "    \n",
    "    print('EER: {:.4f}\\tHTER: {:.4f}'.format(val_eer, hter))\n",
    "\n",
    "    print('>> finished')\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 공개DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## denset model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 230, 230, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1/conv (Conv2D)             (None, 112, 112, 64) 9408        zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1/bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1/conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1/relu (Activation)         (None, 112, 112, 64) 0           conv1/bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 114, 114, 64) 0           conv1/relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 56, 56, 64)   0           zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 56, 56, 64)   256         pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_relu (Activation (None, 56, 56, 64)   0           conv2_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 128)  8192        conv2_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 128)  0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_concat (Concatenat (None, 56, 56, 96)   0           pool1[0][0]                      \n",
      "                                                                 conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_bn (BatchNormali (None, 56, 56, 96)   384         conv2_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_relu (Activation (None, 56, 56, 96)   0           conv2_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 128)  12288       conv2_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 128)  0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_concat (Concatenat (None, 56, 56, 128)  0           conv2_block1_concat[0][0]        \n",
      "                                                                 conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_0_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_0_relu (Activation (None, 56, 56, 128)  0           conv2_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 128)  16384       conv2_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 128)  0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_concat (Concatenat (None, 56, 56, 160)  0           conv2_block2_concat[0][0]        \n",
      "                                                                 conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_0_bn (BatchNormali (None, 56, 56, 160)  640         conv2_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_0_relu (Activation (None, 56, 56, 160)  0           conv2_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_conv (Conv2D)    (None, 56, 56, 128)  20480       conv2_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_relu (Activation (None, 56, 56, 128)  0           conv2_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_concat (Concatenat (None, 56, 56, 192)  0           conv2_block3_concat[0][0]        \n",
      "                                                                 conv2_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_0_bn (BatchNormali (None, 56, 56, 192)  768         conv2_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_0_relu (Activation (None, 56, 56, 192)  0           conv2_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_conv (Conv2D)    (None, 56, 56, 128)  24576       conv2_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_relu (Activation (None, 56, 56, 128)  0           conv2_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_concat (Concatenat (None, 56, 56, 224)  0           conv2_block4_concat[0][0]        \n",
      "                                                                 conv2_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_0_bn (BatchNormali (None, 56, 56, 224)  896         conv2_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_0_relu (Activation (None, 56, 56, 224)  0           conv2_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_conv (Conv2D)    (None, 56, 56, 128)  28672       conv2_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_relu (Activation (None, 56, 56, 128)  0           conv2_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_concat (Concatenat (None, 56, 56, 256)  0           conv2_block5_concat[0][0]        \n",
      "                                                                 conv2_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool2_bn (BatchNormalization)   (None, 56, 56, 256)  1024        conv2_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool2_relu (Activation)         (None, 56, 56, 256)  0           pool2_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool2_conv (Conv2D)             (None, 56, 56, 128)  32768       pool2_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool2_pool (AveragePooling2D)   (None, 28, 28, 128)  0           pool2_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 28, 28, 128)  512         pool2_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_relu (Activation (None, 28, 28, 128)  0           conv3_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  16384       conv3_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_concat (Concatenat (None, 28, 28, 160)  0           pool2_pool[0][0]                 \n",
      "                                                                 conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_bn (BatchNormali (None, 28, 28, 160)  640         conv3_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_relu (Activation (None, 28, 28, 160)  0           conv3_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  20480       conv3_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_concat (Concatenat (None, 28, 28, 192)  0           conv3_block1_concat[0][0]        \n",
      "                                                                 conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_0_bn (BatchNormali (None, 28, 28, 192)  768         conv3_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_0_relu (Activation (None, 28, 28, 192)  0           conv3_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  24576       conv3_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_concat (Concatenat (None, 28, 28, 224)  0           conv3_block2_concat[0][0]        \n",
      "                                                                 conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_0_bn (BatchNormali (None, 28, 28, 224)  896         conv3_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_0_relu (Activation (None, 28, 28, 224)  0           conv3_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  28672       conv3_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_concat (Concatenat (None, 28, 28, 256)  0           conv3_block3_concat[0][0]        \n",
      "                                                                 conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_0_bn (BatchNormali (None, 28, 28, 256)  1024        conv3_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_0_relu (Activation (None, 28, 28, 256)  0           conv3_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_conv (Conv2D)    (None, 28, 28, 128)  32768       conv3_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_relu (Activation (None, 28, 28, 128)  0           conv3_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_concat (Concatenat (None, 28, 28, 288)  0           conv3_block4_concat[0][0]        \n",
      "                                                                 conv3_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_0_bn (BatchNormali (None, 28, 28, 288)  1152        conv3_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_0_relu (Activation (None, 28, 28, 288)  0           conv3_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_conv (Conv2D)    (None, 28, 28, 128)  36864       conv3_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_relu (Activation (None, 28, 28, 128)  0           conv3_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_concat (Concatenat (None, 28, 28, 320)  0           conv3_block5_concat[0][0]        \n",
      "                                                                 conv3_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_0_bn (BatchNormali (None, 28, 28, 320)  1280        conv3_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_0_relu (Activation (None, 28, 28, 320)  0           conv3_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_conv (Conv2D)    (None, 28, 28, 128)  40960       conv3_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_relu (Activation (None, 28, 28, 128)  0           conv3_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_concat (Concatenat (None, 28, 28, 352)  0           conv3_block6_concat[0][0]        \n",
      "                                                                 conv3_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_0_bn (BatchNormali (None, 28, 28, 352)  1408        conv3_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_0_relu (Activation (None, 28, 28, 352)  0           conv3_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_conv (Conv2D)    (None, 28, 28, 128)  45056       conv3_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_relu (Activation (None, 28, 28, 128)  0           conv3_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_concat (Concatenat (None, 28, 28, 384)  0           conv3_block7_concat[0][0]        \n",
      "                                                                 conv3_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_0_bn (BatchNormali (None, 28, 28, 384)  1536        conv3_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_0_relu (Activation (None, 28, 28, 384)  0           conv3_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_conv (Conv2D)    (None, 28, 28, 128)  49152       conv3_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_relu (Activation (None, 28, 28, 128)  0           conv3_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_concat (Concatenat (None, 28, 28, 416)  0           conv3_block8_concat[0][0]        \n",
      "                                                                 conv3_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_0_bn (BatchNormal (None, 28, 28, 416)  1664        conv3_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_0_relu (Activatio (None, 28, 28, 416)  0           conv3_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_conv (Conv2D)   (None, 28, 28, 128)  53248       conv3_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_concat (Concatena (None, 28, 28, 448)  0           conv3_block9_concat[0][0]        \n",
      "                                                                 conv3_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_0_bn (BatchNormal (None, 28, 28, 448)  1792        conv3_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_0_relu (Activatio (None, 28, 28, 448)  0           conv3_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_conv (Conv2D)   (None, 28, 28, 128)  57344       conv3_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_concat (Concatena (None, 28, 28, 480)  0           conv3_block10_concat[0][0]       \n",
      "                                                                 conv3_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_0_bn (BatchNormal (None, 28, 28, 480)  1920        conv3_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_0_relu (Activatio (None, 28, 28, 480)  0           conv3_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_conv (Conv2D)   (None, 28, 28, 128)  61440       conv3_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_concat (Concatena (None, 28, 28, 512)  0           conv3_block11_concat[0][0]       \n",
      "                                                                 conv3_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3_bn (BatchNormalization)   (None, 28, 28, 512)  2048        conv3_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3_relu (Activation)         (None, 28, 28, 512)  0           pool3_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool3_conv (Conv2D)             (None, 28, 28, 256)  131072      pool3_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool3_pool (AveragePooling2D)   (None, 14, 14, 256)  0           pool3_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 256)  1024        pool3_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_relu (Activation (None, 14, 14, 256)  0           conv4_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 128)  32768       conv4_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 128)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_concat (Concatenat (None, 14, 14, 288)  0           pool3_pool[0][0]                 \n",
      "                                                                 conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_bn (BatchNormali (None, 14, 14, 288)  1152        conv4_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_relu (Activation (None, 14, 14, 288)  0           conv4_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 128)  36864       conv4_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 128)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_concat (Concatenat (None, 14, 14, 320)  0           conv4_block1_concat[0][0]        \n",
      "                                                                 conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_0_bn (BatchNormali (None, 14, 14, 320)  1280        conv4_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_0_relu (Activation (None, 14, 14, 320)  0           conv4_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 128)  40960       conv4_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 128)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_concat (Concatenat (None, 14, 14, 352)  0           conv4_block2_concat[0][0]        \n",
      "                                                                 conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_0_bn (BatchNormali (None, 14, 14, 352)  1408        conv4_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_0_relu (Activation (None, 14, 14, 352)  0           conv4_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 128)  45056       conv4_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 128)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_concat (Concatenat (None, 14, 14, 384)  0           conv4_block3_concat[0][0]        \n",
      "                                                                 conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_0_bn (BatchNormali (None, 14, 14, 384)  1536        conv4_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_0_relu (Activation (None, 14, 14, 384)  0           conv4_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 128)  49152       conv4_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 128)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_concat (Concatenat (None, 14, 14, 416)  0           conv4_block4_concat[0][0]        \n",
      "                                                                 conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_0_bn (BatchNormali (None, 14, 14, 416)  1664        conv4_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_0_relu (Activation (None, 14, 14, 416)  0           conv4_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 128)  53248       conv4_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 128)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_concat (Concatenat (None, 14, 14, 448)  0           conv4_block5_concat[0][0]        \n",
      "                                                                 conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_0_bn (BatchNormali (None, 14, 14, 448)  1792        conv4_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_0_relu (Activation (None, 14, 14, 448)  0           conv4_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_conv (Conv2D)    (None, 14, 14, 128)  57344       conv4_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_relu (Activation (None, 14, 14, 128)  0           conv4_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_concat (Concatenat (None, 14, 14, 480)  0           conv4_block6_concat[0][0]        \n",
      "                                                                 conv4_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_0_bn (BatchNormali (None, 14, 14, 480)  1920        conv4_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_0_relu (Activation (None, 14, 14, 480)  0           conv4_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_conv (Conv2D)    (None, 14, 14, 128)  61440       conv4_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_relu (Activation (None, 14, 14, 128)  0           conv4_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_concat (Concatenat (None, 14, 14, 512)  0           conv4_block7_concat[0][0]        \n",
      "                                                                 conv4_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_0_bn (BatchNormali (None, 14, 14, 512)  2048        conv4_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_0_relu (Activation (None, 14, 14, 512)  0           conv4_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_conv (Conv2D)    (None, 14, 14, 128)  65536       conv4_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_relu (Activation (None, 14, 14, 128)  0           conv4_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_concat (Concatenat (None, 14, 14, 544)  0           conv4_block8_concat[0][0]        \n",
      "                                                                 conv4_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_0_bn (BatchNormal (None, 14, 14, 544)  2176        conv4_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_0_relu (Activatio (None, 14, 14, 544)  0           conv4_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_conv (Conv2D)   (None, 14, 14, 128)  69632       conv4_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_concat (Concatena (None, 14, 14, 576)  0           conv4_block9_concat[0][0]        \n",
      "                                                                 conv4_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_0_bn (BatchNormal (None, 14, 14, 576)  2304        conv4_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_0_relu (Activatio (None, 14, 14, 576)  0           conv4_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_conv (Conv2D)   (None, 14, 14, 128)  73728       conv4_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_concat (Concatena (None, 14, 14, 608)  0           conv4_block10_concat[0][0]       \n",
      "                                                                 conv4_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_0_bn (BatchNormal (None, 14, 14, 608)  2432        conv4_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_0_relu (Activatio (None, 14, 14, 608)  0           conv4_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_conv (Conv2D)   (None, 14, 14, 128)  77824       conv4_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_concat (Concatena (None, 14, 14, 640)  0           conv4_block11_concat[0][0]       \n",
      "                                                                 conv4_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_0_bn (BatchNormal (None, 14, 14, 640)  2560        conv4_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_0_relu (Activatio (None, 14, 14, 640)  0           conv4_block13_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_conv (Conv2D)   (None, 14, 14, 128)  81920       conv4_block13_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_concat (Concatena (None, 14, 14, 672)  0           conv4_block12_concat[0][0]       \n",
      "                                                                 conv4_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_0_bn (BatchNormal (None, 14, 14, 672)  2688        conv4_block13_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_0_relu (Activatio (None, 14, 14, 672)  0           conv4_block14_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_conv (Conv2D)   (None, 14, 14, 128)  86016       conv4_block14_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_concat (Concatena (None, 14, 14, 704)  0           conv4_block13_concat[0][0]       \n",
      "                                                                 conv4_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_0_bn (BatchNormal (None, 14, 14, 704)  2816        conv4_block14_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_0_relu (Activatio (None, 14, 14, 704)  0           conv4_block15_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_conv (Conv2D)   (None, 14, 14, 128)  90112       conv4_block15_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_concat (Concatena (None, 14, 14, 736)  0           conv4_block14_concat[0][0]       \n",
      "                                                                 conv4_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_0_bn (BatchNormal (None, 14, 14, 736)  2944        conv4_block15_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_0_relu (Activatio (None, 14, 14, 736)  0           conv4_block16_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_conv (Conv2D)   (None, 14, 14, 128)  94208       conv4_block16_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_concat (Concatena (None, 14, 14, 768)  0           conv4_block15_concat[0][0]       \n",
      "                                                                 conv4_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_0_bn (BatchNormal (None, 14, 14, 768)  3072        conv4_block16_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_0_relu (Activatio (None, 14, 14, 768)  0           conv4_block17_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_conv (Conv2D)   (None, 14, 14, 128)  98304       conv4_block17_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block17_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block17_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block17_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_concat (Concatena (None, 14, 14, 800)  0           conv4_block16_concat[0][0]       \n",
      "                                                                 conv4_block17_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_0_bn (BatchNormal (None, 14, 14, 800)  3200        conv4_block17_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_0_relu (Activatio (None, 14, 14, 800)  0           conv4_block18_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_conv (Conv2D)   (None, 14, 14, 128)  102400      conv4_block18_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block18_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block18_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block18_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_concat (Concatena (None, 14, 14, 832)  0           conv4_block17_concat[0][0]       \n",
      "                                                                 conv4_block18_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_0_bn (BatchNormal (None, 14, 14, 832)  3328        conv4_block18_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_0_relu (Activatio (None, 14, 14, 832)  0           conv4_block19_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_conv (Conv2D)   (None, 14, 14, 128)  106496      conv4_block19_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block19_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block19_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block19_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_concat (Concatena (None, 14, 14, 864)  0           conv4_block18_concat[0][0]       \n",
      "                                                                 conv4_block19_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_0_bn (BatchNormal (None, 14, 14, 864)  3456        conv4_block19_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_0_relu (Activatio (None, 14, 14, 864)  0           conv4_block20_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_conv (Conv2D)   (None, 14, 14, 128)  110592      conv4_block20_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block20_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block20_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block20_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_concat (Concatena (None, 14, 14, 896)  0           conv4_block19_concat[0][0]       \n",
      "                                                                 conv4_block20_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_0_bn (BatchNormal (None, 14, 14, 896)  3584        conv4_block20_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_0_relu (Activatio (None, 14, 14, 896)  0           conv4_block21_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_conv (Conv2D)   (None, 14, 14, 128)  114688      conv4_block21_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block21_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block21_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block21_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_concat (Concatena (None, 14, 14, 928)  0           conv4_block20_concat[0][0]       \n",
      "                                                                 conv4_block21_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_0_bn (BatchNormal (None, 14, 14, 928)  3712        conv4_block21_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_0_relu (Activatio (None, 14, 14, 928)  0           conv4_block22_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_conv (Conv2D)   (None, 14, 14, 128)  118784      conv4_block22_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block22_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block22_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block22_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_concat (Concatena (None, 14, 14, 960)  0           conv4_block21_concat[0][0]       \n",
      "                                                                 conv4_block22_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_0_bn (BatchNormal (None, 14, 14, 960)  3840        conv4_block22_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_0_relu (Activatio (None, 14, 14, 960)  0           conv4_block23_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_conv (Conv2D)   (None, 14, 14, 128)  122880      conv4_block23_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block23_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block23_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block23_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_concat (Concatena (None, 14, 14, 992)  0           conv4_block22_concat[0][0]       \n",
      "                                                                 conv4_block23_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_0_bn (BatchNormal (None, 14, 14, 992)  3968        conv4_block23_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_0_relu (Activatio (None, 14, 14, 992)  0           conv4_block24_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_conv (Conv2D)   (None, 14, 14, 128)  126976      conv4_block24_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block24_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block24_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block24_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_concat (Concatena (None, 14, 14, 1024) 0           conv4_block23_concat[0][0]       \n",
      "                                                                 conv4_block24_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool4_bn (BatchNormalization)   (None, 14, 14, 1024) 4096        conv4_block24_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool4_relu (Activation)         (None, 14, 14, 1024) 0           pool4_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool4_conv (Conv2D)             (None, 14, 14, 512)  524288      pool4_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool4_pool (AveragePooling2D)   (None, 7, 7, 512)    0           pool4_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 512)    2048        pool4_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_relu (Activation (None, 7, 7, 512)    0           conv5_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 128)    65536       conv5_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 128)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_concat (Concatenat (None, 7, 7, 544)    0           pool4_pool[0][0]                 \n",
      "                                                                 conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_0_bn (BatchNormali (None, 7, 7, 544)    2176        conv5_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_0_relu (Activation (None, 7, 7, 544)    0           conv5_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 128)    69632       conv5_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 128)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_concat (Concatenat (None, 7, 7, 576)    0           conv5_block1_concat[0][0]        \n",
      "                                                                 conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_0_bn (BatchNormali (None, 7, 7, 576)    2304        conv5_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_0_relu (Activation (None, 7, 7, 576)    0           conv5_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 128)    73728       conv5_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 128)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_concat (Concatenat (None, 7, 7, 608)    0           conv5_block2_concat[0][0]        \n",
      "                                                                 conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_0_bn (BatchNormali (None, 7, 7, 608)    2432        conv5_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_0_relu (Activation (None, 7, 7, 608)    0           conv5_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_conv (Conv2D)    (None, 7, 7, 128)    77824       conv5_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_relu (Activation (None, 7, 7, 128)    0           conv5_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_concat (Concatenat (None, 7, 7, 640)    0           conv5_block3_concat[0][0]        \n",
      "                                                                 conv5_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_0_bn (BatchNormali (None, 7, 7, 640)    2560        conv5_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_0_relu (Activation (None, 7, 7, 640)    0           conv5_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_conv (Conv2D)    (None, 7, 7, 128)    81920       conv5_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_relu (Activation (None, 7, 7, 128)    0           conv5_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_concat (Concatenat (None, 7, 7, 672)    0           conv5_block4_concat[0][0]        \n",
      "                                                                 conv5_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_0_bn (BatchNormali (None, 7, 7, 672)    2688        conv5_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_0_relu (Activation (None, 7, 7, 672)    0           conv5_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_conv (Conv2D)    (None, 7, 7, 128)    86016       conv5_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_relu (Activation (None, 7, 7, 128)    0           conv5_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_concat (Concatenat (None, 7, 7, 704)    0           conv5_block5_concat[0][0]        \n",
      "                                                                 conv5_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_0_bn (BatchNormali (None, 7, 7, 704)    2816        conv5_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_0_relu (Activation (None, 7, 7, 704)    0           conv5_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_conv (Conv2D)    (None, 7, 7, 128)    90112       conv5_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_relu (Activation (None, 7, 7, 128)    0           conv5_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_concat (Concatenat (None, 7, 7, 736)    0           conv5_block6_concat[0][0]        \n",
      "                                                                 conv5_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_0_bn (BatchNormali (None, 7, 7, 736)    2944        conv5_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_0_relu (Activation (None, 7, 7, 736)    0           conv5_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_conv (Conv2D)    (None, 7, 7, 128)    94208       conv5_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_relu (Activation (None, 7, 7, 128)    0           conv5_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_concat (Concatenat (None, 7, 7, 768)    0           conv5_block7_concat[0][0]        \n",
      "                                                                 conv5_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_0_bn (BatchNormali (None, 7, 7, 768)    3072        conv5_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_0_relu (Activation (None, 7, 7, 768)    0           conv5_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_conv (Conv2D)    (None, 7, 7, 128)    98304       conv5_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_relu (Activation (None, 7, 7, 128)    0           conv5_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_concat (Concatenat (None, 7, 7, 800)    0           conv5_block8_concat[0][0]        \n",
      "                                                                 conv5_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_0_bn (BatchNormal (None, 7, 7, 800)    3200        conv5_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_0_relu (Activatio (None, 7, 7, 800)    0           conv5_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_conv (Conv2D)   (None, 7, 7, 128)    102400      conv5_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_concat (Concatena (None, 7, 7, 832)    0           conv5_block9_concat[0][0]        \n",
      "                                                                 conv5_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_0_bn (BatchNormal (None, 7, 7, 832)    3328        conv5_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_0_relu (Activatio (None, 7, 7, 832)    0           conv5_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_conv (Conv2D)   (None, 7, 7, 128)    106496      conv5_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_concat (Concatena (None, 7, 7, 864)    0           conv5_block10_concat[0][0]       \n",
      "                                                                 conv5_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_0_bn (BatchNormal (None, 7, 7, 864)    3456        conv5_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_0_relu (Activatio (None, 7, 7, 864)    0           conv5_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_conv (Conv2D)   (None, 7, 7, 128)    110592      conv5_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_concat (Concatena (None, 7, 7, 896)    0           conv5_block11_concat[0][0]       \n",
      "                                                                 conv5_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_0_bn (BatchNormal (None, 7, 7, 896)    3584        conv5_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_0_relu (Activatio (None, 7, 7, 896)    0           conv5_block13_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_conv (Conv2D)   (None, 7, 7, 128)    114688      conv5_block13_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_concat (Concatena (None, 7, 7, 928)    0           conv5_block12_concat[0][0]       \n",
      "                                                                 conv5_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_0_bn (BatchNormal (None, 7, 7, 928)    3712        conv5_block13_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_0_relu (Activatio (None, 7, 7, 928)    0           conv5_block14_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_conv (Conv2D)   (None, 7, 7, 128)    118784      conv5_block14_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_concat (Concatena (None, 7, 7, 960)    0           conv5_block13_concat[0][0]       \n",
      "                                                                 conv5_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_0_bn (BatchNormal (None, 7, 7, 960)    3840        conv5_block14_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_0_relu (Activatio (None, 7, 7, 960)    0           conv5_block15_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_conv (Conv2D)   (None, 7, 7, 128)    122880      conv5_block15_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_concat (Concatena (None, 7, 7, 992)    0           conv5_block14_concat[0][0]       \n",
      "                                                                 conv5_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_0_bn (BatchNormal (None, 7, 7, 992)    3968        conv5_block15_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_0_relu (Activatio (None, 7, 7, 992)    0           conv5_block16_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_conv (Conv2D)   (None, 7, 7, 128)    126976      conv5_block16_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_concat (Concatena (None, 7, 7, 1024)   0           conv5_block15_concat[0][0]       \n",
      "                                                                 conv5_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bn (BatchNormalization)         (None, 7, 7, 1024)   4096        conv5_block16_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "relu (Activation)               (None, 7, 7, 1024)   0           bn[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 1024)         0           relu[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            1025        global_average_pooling2d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 7,038,529\n",
      "Trainable params: 6,954,881\n",
      "Non-trainable params: 83,648\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "#keras.applications.densenet.DenseNet121(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n",
    "from tensorflow.keras.applications.densenet import DenseNet121\n",
    "#from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.densenet import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Activation\n",
    "\n",
    "def Densenet121(show_layers,weights,input_shape):\n",
    "        base_model = DenseNet121(include_top=False, weights=weights, input_shape=input_shape)\n",
    "        x = GlobalAveragePooling2D()(base_model.output)\n",
    "        pred = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "        model = Model(inputs=base_model.input, outputs=pred)\n",
    "        model.compile(optimizer=SGD(lr=1e-3, decay=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "        if show_layers:\n",
    "            for i, layer in enumerate(model.layers):\n",
    "                print(i, layer.name, layer.trainable)\n",
    "        return model\n",
    "    \n",
    "model = Densenet121(False,weights=None,input_shape=(224, 224, 3))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## casia\n",
    "- 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>  1\n",
      "============================================Densenet121=============================================\n",
      "Found 21322 images belonging to 2 classes.\n",
      "Found 6579 images belonging to 2 classes.\n",
      "train shape : (21322,)\n",
      "Epoch 1/100\n",
      "2665/2666 [============================>.] - ETA: 0s - loss: 0.2216 - accuracy: 0.9166\n",
      "Epoch 00001: val_loss improved from inf to 0.05415, saving model to .\\result_CASIA\\1-01-0.2216.hdf5\n",
      "2666/2666 [==============================] - 1625s 610ms/step - loss: 0.2215 - accuracy: 0.9166 - val_loss: 0.0542 - val_accuracy: 0.9995\n",
      "Epoch 2/100\n",
      "2665/2666 [============================>.] - ETA: 0s - loss: 0.0847 - accuracy: 0.9769\n",
      "Epoch 00002: val_loss improved from 0.05415 to 0.01869, saving model to .\\result_CASIA\\1-02-0.0840.hdf5\n",
      "2666/2666 [==============================] - 1608s 603ms/step - loss: 0.0848 - accuracy: 0.9769 - val_loss: 0.0187 - val_accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "2665/2666 [============================>.] - ETA: 0s - loss: 0.0512 - accuracy: 0.9863\n",
      "Epoch 00003: val_loss improved from 0.01869 to 0.01174, saving model to .\\result_CASIA\\1-03-0.0510.hdf5\n",
      "2666/2666 [==============================] - 1596s 599ms/step - loss: 0.0512 - accuracy: 0.9863 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "2665/2666 [============================>.] - ETA: 0s - loss: 0.0330 - accuracy: 0.9929\n",
      "Epoch 00004: val_loss did not improve from 0.01174\n",
      "2666/2666 [==============================] - 1599s 600ms/step - loss: 0.0330 - accuracy: 0.9929 - val_loss: 0.0123 - val_accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "2665/2666 [============================>.] - ETA: 0s - loss: 0.0230 - accuracy: 0.9952\n",
      "Epoch 00005: val_loss did not improve from 0.01174\n",
      "2666/2666 [==============================] - 1595s 598ms/step - loss: 0.0230 - accuracy: 0.9952 - val_loss: 0.0899 - val_accuracy: 0.9690\n",
      "Epoch 6/100\n",
      "2665/2666 [============================>.] - ETA: 0s - loss: 0.0208 - accuracy: 0.9951\n",
      "Epoch 00006: val_loss improved from 0.01174 to 0.00840, saving model to .\\result_CASIA\\1-06-0.0208.hdf5\n",
      "2666/2666 [==============================] - 1601s 601ms/step - loss: 0.0208 - accuracy: 0.9951 - val_loss: 0.0084 - val_accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "2665/2666 [============================>.] - ETA: 0s - loss: 0.0171 - accuracy: 0.9964\n",
      "Epoch 00007: val_loss improved from 0.00840 to 0.00216, saving model to .\\result_CASIA\\1-07-0.0171.hdf5\n",
      "2666/2666 [==============================] - 1606s 602ms/step - loss: 0.0171 - accuracy: 0.9964 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "2665/2666 [============================>.] - ETA: 0s - loss: 0.0147 - accuracy: 0.9970\n",
      "Epoch 00008: val_loss did not improve from 0.00216\n",
      "2666/2666 [==============================] - 1599s 600ms/step - loss: 0.0147 - accuracy: 0.9970 - val_loss: 0.0196 - val_accuracy: 0.9973\n",
      "Epoch 9/100\n",
      "2665/2666 [============================>.] - ETA: 0s - loss: 0.0157 - accuracy: 0.9967\n",
      "Epoch 00009: val_loss did not improve from 0.00216\n",
      "2666/2666 [==============================] - 1598s 599ms/step - loss: 0.0157 - accuracy: 0.9967 - val_loss: 0.0053 - val_accuracy: 0.9998\n",
      "Epoch 10/100\n",
      "2665/2666 [============================>.] - ETA: 0s - loss: 0.0118 - accuracy: 0.9978\n",
      "Epoch 00010: val_loss did not improve from 0.00216\n",
      "2666/2666 [==============================] - 1605s 602ms/step - loss: 0.0118 - accuracy: 0.9978 - val_loss: 0.6597 - val_accuracy: 0.8027\n",
      "Epoch 11/100\n",
      "2665/2666 [============================>.] - ETA: 0s - loss: 0.0082 - accuracy: 0.9990\n",
      "Epoch 00011: val_loss did not improve from 0.00216\n",
      "2666/2666 [==============================] - 1593s 597ms/step - loss: 0.0082 - accuracy: 0.9990 - val_loss: 0.0121 - val_accuracy: 0.9997\n",
      "Epoch 12/100\n",
      "2665/2666 [============================>.] - ETA: 0s - loss: 0.0090 - accuracy: 0.9983\n",
      "Epoch 00012: val_loss did not improve from 0.00216\n",
      "2666/2666 [==============================] - 1595s 598ms/step - loss: 0.0090 - accuracy: 0.9983 - val_loss: 0.0147 - val_accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "2665/2666 [============================>.] - ETA: 0s - loss: 0.0095 - accuracy: 0.9990\n",
      "Epoch 00013: val_loss did not improve from 0.00216\n",
      "2666/2666 [==============================] - 1596s 598ms/step - loss: 0.0095 - accuracy: 0.9990 - val_loss: 0.0090 - val_accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "2665/2666 [============================>.] - ETA: 0s - loss: 0.0085 - accuracy: 0.9986\n",
      "Epoch 00014: val_loss did not improve from 0.00216\n",
      "2666/2666 [==============================] - 1600s 600ms/step - loss: 0.0085 - accuracy: 0.9986 - val_loss: 0.0140 - val_accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "2665/2666 [============================>.] - ETA: 0s - loss: 0.0079 - accuracy: 0.9985\n",
      "Epoch 00015: val_loss improved from 0.00216 to 0.00152, saving model to .\\result_CASIA\\1-15-0.0079.hdf5\n",
      "2666/2666 [==============================] - 1598s 599ms/step - loss: 0.0079 - accuracy: 0.9985 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "2665/2666 [============================>.] - ETA: 0s - loss: 0.0063 - accuracy: 0.9991\n",
      "Epoch 00016: val_loss did not improve from 0.00152\n",
      "2666/2666 [==============================] - 1598s 599ms/step - loss: 0.0063 - accuracy: 0.9991 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "2665/2666 [============================>.] - ETA: 0s - loss: 0.0087 - accuracy: 0.9978\n",
      "Epoch 00017: val_loss did not improve from 0.00152\n",
      "2666/2666 [==============================] - 1596s 599ms/step - loss: 0.0087 - accuracy: 0.9978 - val_loss: 0.0082 - val_accuracy: 0.9980\n",
      "Epoch 18/100\n",
      "2665/2666 [============================>.] - ETA: 0s - loss: 0.0047 - accuracy: 0.9996\n",
      "Epoch 00018: val_loss did not improve from 0.00152\n",
      "2666/2666 [==============================] - 1606s 603ms/step - loss: 0.0047 - accuracy: 0.9996 - val_loss: 0.0085 - val_accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "2665/2666 [============================>.] - ETA: 0s - loss: 0.0051 - accuracy: 0.9991\n",
      "Epoch 00019: val_loss did not improve from 0.00152\n",
      "2666/2666 [==============================] - 1594s 598ms/step - loss: 0.0051 - accuracy: 0.9991 - val_loss: 0.0097 - val_accuracy: 0.9998\n",
      "Epoch 20/100\n",
      "2665/2666 [============================>.] - ETA: 0s - loss: 0.0067 - accuracy: 0.9990\n",
      "Epoch 00020: val_loss did not improve from 0.00152\n",
      "2666/2666 [==============================] - 1599s 600ms/step - loss: 0.0067 - accuracy: 0.9990 - val_loss: 0.1778 - val_accuracy: 0.9153\n",
      "Epoch 21/100\n",
      "2665/2666 [============================>.] - ETA: 0s - loss: 0.0053 - accuracy: 0.9994\n",
      "Epoch 00021: val_loss did not improve from 0.00152\n",
      "2666/2666 [==============================] - 1599s 600ms/step - loss: 0.0053 - accuracy: 0.9994 - val_loss: 0.0093 - val_accuracy: 0.9992\n",
      "Epoch 22/100\n",
      "2665/2666 [============================>.] - ETA: 0s - loss: 0.0030 - accuracy: 0.9998\n",
      "Epoch 00022: val_loss did not improve from 0.00152\n",
      "2666/2666 [==============================] - 1602s 601ms/step - loss: 0.0030 - accuracy: 0.9998 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "2665/2666 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9990\n",
      "Epoch 00023: val_loss did not improve from 0.00152\n",
      "2666/2666 [==============================] - 1595s 598ms/step - loss: 0.0048 - accuracy: 0.9990 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "2665/2666 [============================>.] - ETA: 0s - loss: 0.0064 - accuracy: 0.9994\n",
      "Epoch 00024: val_loss did not improve from 0.00152\n",
      "2666/2666 [==============================] - 1597s 599ms/step - loss: 0.0064 - accuracy: 0.9994 - val_loss: 0.0049 - val_accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "2665/2666 [============================>.] - ETA: 0s - loss: 0.0067 - accuracy: 0.9985\n",
      "Epoch 00025: val_loss did not improve from 0.00152\n",
      "2666/2666 [==============================] - 1599s 600ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0030 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 1.0000\tloss: 0.0015\n",
      "=============================================1 finished=============================================\n"
     ]
    }
   ],
   "source": [
    "#### import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "%matplotlib inline\n",
    "SEED = 777\n",
    "\n",
    "def main():\n",
    "    inputSize = 224\n",
    "    batchSize = 8\n",
    "    nEpoch = 100\n",
    "    \n",
    "    # 디렉토리 정보 설정\n",
    "    # dataDir = 'D:\\\\Face_Database\\\\B-Database'\n",
    "    dataDir = 'E:\\\\Face_Database\\\\public-Database\\\\1_crop_result\\\\CASIA-FASD\\\\train'   \n",
    "    # 학습 및 테스트 할 데이터베이스 디렉토리명\n",
    "    trainDB = '1'  # D:\\Face_Database\\B-Database\\protocol_4\\train\n",
    "    validDB = '1' \n",
    "    \n",
    "    saveDir = '.\\\\result_CASIA'\n",
    "    if not os.path.exists(saveDir):    \n",
    "        os.makedirs(saveDir)\n",
    "\n",
    "    model_path = os.path.join(saveDir, trainDB + '-{epoch:02d}-{loss:.4f}.hdf5')\n",
    "    # 데이터베이스별 학습 수행\n",
    "    print('>> ', trainDB)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    print('Densenet121'.center(100, '='))\n",
    "   \n",
    "    '''\n",
    "    training\n",
    "    '''\n",
    "    K.clear_session()\n",
    "\n",
    "    # 네트워크 정의\n",
    "    model = Densenet121(False,weights=None,input_shape=(224, 224, 3))\n",
    "\n",
    "    # 데이터 generator 생성\n",
    "    train_datagen = ImageDataGenerator(rescale=1.0/255.0,\n",
    "                                       horizontal_flip=True,)#이미지augmentation > 이미지를 회전,확대,축소 등 여러변형해보는것.\n",
    "    train_generator = train_datagen.flow_from_directory(os.path.join(*[dataDir,trainDB,'train']),\n",
    "                                                        target_size=(inputSize, inputSize),\n",
    "                                                        batch_size=batchSize,\n",
    "                                                        class_mode='binary',\n",
    "                                                        interpolation='bilinear',\n",
    "                                                        shuffle=True,)\n",
    "\n",
    "    valid_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    valid_generator = valid_datagen.flow_from_directory(os.path.join(*[dataDir, validDB,'val']),\n",
    "                                                        target_size=(inputSize, inputSize),\n",
    "                                                        batch_size=batchSize,\n",
    "                                                        class_mode='binary',\n",
    "                                                        interpolation='bilinear',\n",
    "                                                        shuffle=False,)\n",
    "\n",
    "    print('train shape :',train_generator.classes.shape)\n",
    "    # unbalanced class를 해결하기 위한 class_weight 설정 #np.unique는 중복원소제거하는거\n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.unique(train_generator.classes), train_generator.classes) \n",
    "\n",
    "    # callback 함수 정의 dropout함수\n",
    "    early_stop = EarlyStopping(patience=10, monitor='val_loss')\n",
    "    cb_checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "    # 모델 학습\n",
    "    hist = model.fit_generator(train_generator, \n",
    "                               epochs=nEpoch,\n",
    "                               steps_per_epoch=len(train_generator),\n",
    "                               class_weight=class_weights,\n",
    "                               validation_data=valid_generator,\n",
    "                               validation_steps=len(valid_generator),\n",
    "                               verbose=1,\n",
    "                               callbacks=[early_stop, cb_checkpoint])\n",
    "\n",
    "    \n",
    "    # 학습 결과 그래프 그리기\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, loss_ax = plt.subplots()\n",
    "    acc_ax = loss_ax.twinx()\n",
    "\n",
    "    loss_ax.plot(hist.history['loss'], 'b', label='train loss')\n",
    "    loss_ax.plot(hist.history['val_loss'], '--r', label='val loss')\n",
    "    loss_ax.set_xlabel('epoch')\n",
    "    loss_ax.set_ylabel('loss')\n",
    "    loss_ax.legend(loc='upper left')\n",
    "\n",
    "    plt.show()\n",
    "    print('acc: {:.4f}\\tloss: {:.4f}'.format(hist.history['val_accuracy'][-11], hist.history['val_loss'][-11]))\n",
    "\n",
    "    print(('{} finished'.format(trainDB)).center(100,'='))\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>  2\n",
      "============================================Densenet121=============================================\n",
      "Found 20692 images belonging to 2 classes.\n",
      "Found 7208 images belonging to 2 classes.\n",
      "train shape : (20692,)\n",
      "Epoch 1/100\n",
      "2586/2587 [============================>.] - ETA: 0s - loss: 0.1883 - accuracy: 0.9323\n",
      "Epoch 00001: val_loss improved from inf to 0.56943, saving model to .\\result_CASIA\\2-01-0.1882.hdf5\n",
      "2587/2587 [==============================] - 1655s 640ms/step - loss: 0.1882 - accuracy: 0.9323 - val_loss: 0.5694 - val_accuracy: 0.7904\n",
      "Epoch 2/100\n",
      "2586/2587 [============================>.] - ETA: 0s - loss: 0.0688 - accuracy: 0.9796\n",
      "Epoch 00002: val_loss improved from 0.56943 to 0.37349, saving model to .\\result_CASIA\\2-02-0.0688.hdf5\n",
      "2587/2587 [==============================] - 1636s 632ms/step - loss: 0.0688 - accuracy: 0.9797 - val_loss: 0.3735 - val_accuracy: 0.8764\n",
      "Epoch 3/100\n",
      "2586/2587 [============================>.] - ETA: 0s - loss: 0.0432 - accuracy: 0.9883\n",
      "Epoch 00003: val_loss improved from 0.37349 to 0.17458, saving model to .\\result_CASIA\\2-03-0.0432.hdf5\n",
      "2587/2587 [==============================] - 1634s 632ms/step - loss: 0.0432 - accuracy: 0.9883 - val_loss: 0.1746 - val_accuracy: 0.9384\n",
      "Epoch 4/100\n",
      "2586/2587 [============================>.] - ETA: 0s - loss: 0.0315 - accuracy: 0.9919\n",
      "Epoch 00004: val_loss improved from 0.17458 to 0.07578, saving model to .\\result_CASIA\\2-04-0.0315.hdf5\n",
      "2587/2587 [==============================] - 1659s 641ms/step - loss: 0.0315 - accuracy: 0.9919 - val_loss: 0.0758 - val_accuracy: 0.9788\n",
      "Epoch 5/100\n",
      "2586/2587 [============================>.] - ETA: 0s - loss: 0.0220 - accuracy: 0.9948\n",
      "Epoch 00005: val_loss did not improve from 0.07578\n",
      "2587/2587 [==============================] - 1719s 665ms/step - loss: 0.0220 - accuracy: 0.9948 - val_loss: 0.1080 - val_accuracy: 0.9752\n",
      "Epoch 6/100\n",
      "2586/2587 [============================>.] - ETA: 0s - loss: 0.0205 - accuracy: 0.9954\n",
      "Epoch 00006: val_loss improved from 0.07578 to 0.05254, saving model to .\\result_CASIA\\2-06-0.0205.hdf5\n",
      "2587/2587 [==============================] - 1676s 648ms/step - loss: 0.0205 - accuracy: 0.9954 - val_loss: 0.0525 - val_accuracy: 0.9731\n",
      "Epoch 7/100\n",
      "2586/2587 [============================>.] - ETA: 0s - loss: 0.0142 - accuracy: 0.9972\n",
      "Epoch 00007: val_loss did not improve from 0.05254\n",
      "2587/2587 [==============================] - 1674s 647ms/step - loss: 0.0142 - accuracy: 0.9972 - val_loss: 0.1659 - val_accuracy: 0.9691\n",
      "Epoch 8/100\n",
      "2586/2587 [============================>.] - ETA: 0s - loss: 0.0123 - accuracy: 0.9977\n",
      "Epoch 00008: val_loss did not improve from 0.05254\n",
      "2587/2587 [==============================] - 1674s 647ms/step - loss: 0.0123 - accuracy: 0.9977 - val_loss: 0.1486 - val_accuracy: 0.9526\n",
      "Epoch 9/100\n",
      "2586/2587 [============================>.] - ETA: 0s - loss: 0.0108 - accuracy: 0.9982\n",
      "Epoch 00009: val_loss improved from 0.05254 to 0.05231, saving model to .\\result_CASIA\\2-09-0.0108.hdf5\n",
      "2587/2587 [==============================] - 1667s 644ms/step - loss: 0.0108 - accuracy: 0.9982 - val_loss: 0.0523 - val_accuracy: 0.9817\n",
      "Epoch 10/100\n",
      "2586/2587 [============================>.] - ETA: 0s - loss: 0.0105 - accuracy: 0.9984\n",
      "Epoch 00010: val_loss did not improve from 0.05231\n",
      "2587/2587 [==============================] - 1671s 646ms/step - loss: 0.0105 - accuracy: 0.9984 - val_loss: 0.0750 - val_accuracy: 0.9772\n",
      "Epoch 11/100\n",
      "2586/2587 [============================>.] - ETA: 0s - loss: 0.0105 - accuracy: 0.9979\n",
      "Epoch 00011: val_loss did not improve from 0.05231\n",
      "2587/2587 [==============================] - 1667s 644ms/step - loss: 0.0105 - accuracy: 0.9979 - val_loss: 0.0598 - val_accuracy: 0.9797\n",
      "Epoch 12/100\n",
      "2586/2587 [============================>.] - ETA: 0s - loss: 0.0090 - accuracy: 0.9982\n",
      "Epoch 00012: val_loss did not improve from 0.05231\n",
      "2587/2587 [==============================] - 1659s 641ms/step - loss: 0.0090 - accuracy: 0.9982 - val_loss: 0.0730 - val_accuracy: 0.9800\n",
      "Epoch 13/100\n",
      "2586/2587 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9988\n",
      "Epoch 00013: val_loss did not improve from 0.05231\n",
      "2587/2587 [==============================] - 1659s 641ms/step - loss: 0.0073 - accuracy: 0.9988 - val_loss: 0.1226 - val_accuracy: 0.9630\n",
      "Epoch 14/100\n",
      "2586/2587 [============================>.] - ETA: 0s - loss: 0.0077 - accuracy: 0.9986\n",
      "Epoch 00014: val_loss improved from 0.05231 to 0.03007, saving model to .\\result_CASIA\\2-14-0.0077.hdf5\n",
      "2587/2587 [==============================] - 1665s 644ms/step - loss: 0.0077 - accuracy: 0.9986 - val_loss: 0.0301 - val_accuracy: 0.9836\n",
      "Epoch 15/100\n",
      "2586/2587 [============================>.] - ETA: 0s - loss: 0.0060 - accuracy: 0.9992\n",
      "Epoch 00015: val_loss did not improve from 0.03007\n",
      "2587/2587 [==============================] - 1670s 646ms/step - loss: 0.0060 - accuracy: 0.9992 - val_loss: 0.0662 - val_accuracy: 0.9788\n",
      "Epoch 16/100\n",
      "2586/2587 [============================>.] - ETA: 0s - loss: 0.0079 - accuracy: 0.9984\n",
      "Epoch 00016: val_loss did not improve from 0.03007\n",
      "2587/2587 [==============================] - 1676s 648ms/step - loss: 0.0079 - accuracy: 0.9984 - val_loss: 0.0464 - val_accuracy: 0.9817\n",
      "Epoch 17/100\n",
      "2586/2587 [============================>.] - ETA: 0s - loss: 0.0061 - accuracy: 0.9987\n",
      "Epoch 00017: val_loss did not improve from 0.03007\n",
      "2587/2587 [==============================] - 1664s 643ms/step - loss: 0.0061 - accuracy: 0.9987 - val_loss: 0.0743 - val_accuracy: 0.9777\n",
      "Epoch 18/100\n",
      "2586/2587 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9995\n",
      "Epoch 00018: val_loss did not improve from 0.03007\n",
      "2587/2587 [==============================] - 1670s 645ms/step - loss: 0.0042 - accuracy: 0.9995 - val_loss: 0.0614 - val_accuracy: 0.9817\n",
      "Epoch 19/100\n",
      "2586/2587 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9996\n",
      "Epoch 00019: val_loss did not improve from 0.03007\n",
      "2587/2587 [==============================] - 1666s 644ms/step - loss: 0.0037 - accuracy: 0.9996 - val_loss: 0.0450 - val_accuracy: 0.9817\n",
      "Epoch 20/100\n",
      "2586/2587 [============================>.] - ETA: 0s - loss: 0.0056 - accuracy: 0.9990\n",
      "Epoch 00020: val_loss improved from 0.03007 to 0.03006, saving model to .\\result_CASIA\\2-20-0.0056.hdf5\n",
      "2587/2587 [==============================] - 1653s 639ms/step - loss: 0.0056 - accuracy: 0.9990 - val_loss: 0.0301 - val_accuracy: 0.9879\n",
      "Epoch 21/100\n",
      "2586/2587 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 0.9998\n",
      "Epoch 00021: val_loss improved from 0.03006 to 0.01583, saving model to .\\result_CASIA\\2-21-0.0032.hdf5\n",
      "2587/2587 [==============================] - 1660s 642ms/step - loss: 0.0032 - accuracy: 0.9998 - val_loss: 0.0158 - val_accuracy: 0.9992\n",
      "Epoch 22/100\n",
      "2586/2587 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9998\n",
      "Epoch 00022: val_loss did not improve from 0.01583\n",
      "2587/2587 [==============================] - 1666s 644ms/step - loss: 0.0033 - accuracy: 0.9998 - val_loss: 0.0408 - val_accuracy: 0.9815\n",
      "Epoch 23/100\n",
      "2586/2587 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9995\n",
      "Epoch 00023: val_loss did not improve from 0.01583\n",
      "2587/2587 [==============================] - 1668s 645ms/step - loss: 0.0037 - accuracy: 0.9995 - val_loss: 0.0592 - val_accuracy: 0.9815\n",
      "Epoch 24/100\n",
      "2586/2587 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9991\n",
      "Epoch 00024: val_loss did not improve from 0.01583\n",
      "2587/2587 [==============================] - 1671s 646ms/step - loss: 0.0043 - accuracy: 0.9991 - val_loss: 0.0399 - val_accuracy: 0.9817\n",
      "Epoch 25/100\n",
      "2586/2587 [============================>.] - ETA: 0s - loss: 0.0058 - accuracy: 0.9987\n",
      "Epoch 00025: val_loss did not improve from 0.01583\n",
      "2587/2587 [==============================] - 1664s 643ms/step - loss: 0.0058 - accuracy: 0.9987 - val_loss: 0.0882 - val_accuracy: 0.9815\n",
      "Epoch 26/100\n",
      "2586/2587 [============================>.] - ETA: 0s - loss: 0.0048 - accuracy: 0.9992\n",
      "Epoch 00026: val_loss did not improve from 0.01583\n",
      "2587/2587 [==============================] - 1664s 643ms/step - loss: 0.0048 - accuracy: 0.9992 - val_loss: 0.0297 - val_accuracy: 0.9847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100\n",
      "2586/2587 [============================>.] - ETA: 0s - loss: 0.0034 - accuracy: 0.9994\n",
      "Epoch 00027: val_loss did not improve from 0.01583\n",
      "2587/2587 [==============================] - 1669s 645ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.0365 - val_accuracy: 0.9815\n",
      "Epoch 28/100\n",
      "2586/2587 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9994\n",
      "Epoch 00028: val_loss did not improve from 0.01583\n",
      "2587/2587 [==============================] - 1663s 643ms/step - loss: 0.0037 - accuracy: 0.9994 - val_loss: 0.0391 - val_accuracy: 0.9815\n",
      "Epoch 29/100\n",
      "2586/2587 [============================>.] - ETA: 0s - loss: 0.0028 - accuracy: 0.9998\n",
      "Epoch 00029: val_loss did not improve from 0.01583\n",
      "2587/2587 [==============================] - 1666s 644ms/step - loss: 0.0028 - accuracy: 0.9998 - val_loss: 0.0483 - val_accuracy: 0.9817\n",
      "Epoch 30/100\n",
      "2586/2587 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 0.9995\n",
      "Epoch 00030: val_loss did not improve from 0.01583\n",
      "2587/2587 [==============================] - 1669s 645ms/step - loss: 0.0032 - accuracy: 0.9995 - val_loss: 0.0383 - val_accuracy: 0.9817\n",
      "Epoch 31/100\n",
      "2586/2587 [============================>.] - ETA: 0s - loss: 0.0031 - accuracy: 0.9996\n",
      "Epoch 00031: val_loss did not improve from 0.01583\n",
      "2587/2587 [==============================] - 1666s 644ms/step - loss: 0.0031 - accuracy: 0.9996 - val_loss: 0.0357 - val_accuracy: 0.9817\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9992\tloss: 0.0158\n",
      "=============================================2 finished=============================================\n"
     ]
    }
   ],
   "source": [
    "#### import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "%matplotlib inline\n",
    "SEED = 777\n",
    "\n",
    "def main():\n",
    "    inputSize = 224\n",
    "    batchSize = 8\n",
    "    nEpoch = 100\n",
    "    \n",
    "    # 디렉토리 정보 설정\n",
    "    # dataDir = 'D:\\\\Face_Database\\\\B-Database'\n",
    "    dataDir = 'E:\\\\Face_Database\\\\public-Database\\\\1_crop_result\\\\CASIA-FASD\\\\train'   \n",
    "    # 학습 및 테스트 할 데이터베이스 디렉토리명\n",
    "    trainDB = '2'  # D:\\Face_Database\\B-Database\\protocol_4\\train\n",
    "    validDB = '2' \n",
    "    \n",
    "    saveDir = '.\\\\result_CASIA'\n",
    "    if not os.path.exists(saveDir):    \n",
    "        os.makedirs(saveDir)\n",
    "\n",
    "    model_path = os.path.join(saveDir, trainDB + '-{epoch:02d}-{loss:.4f}.hdf5')\n",
    "    # 데이터베이스별 학습 수행\n",
    "    print('>> ', trainDB)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    print('Densenet121'.center(100, '='))\n",
    "   \n",
    "    '''\n",
    "    training\n",
    "    '''\n",
    "    K.clear_session()\n",
    "\n",
    "    # 네트워크 정의\n",
    "    model = Densenet121(False,weights=None,input_shape=(224, 224, 3))\n",
    "\n",
    "    # 데이터 generator 생성\n",
    "    train_datagen = ImageDataGenerator(rescale=1.0/255.0,\n",
    "                                       horizontal_flip=True,)#이미지augmentation > 이미지를 회전,확대,축소 등 여러변형해보는것.\n",
    "    train_generator = train_datagen.flow_from_directory(os.path.join(*[dataDir,trainDB,'train']),\n",
    "                                                        target_size=(inputSize, inputSize),\n",
    "                                                        batch_size=batchSize,\n",
    "                                                        class_mode='binary',\n",
    "                                                        interpolation='bilinear',\n",
    "                                                        shuffle=True,)\n",
    "\n",
    "    valid_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    valid_generator = valid_datagen.flow_from_directory(os.path.join(*[dataDir, validDB,'val']),\n",
    "                                                        target_size=(inputSize, inputSize),\n",
    "                                                        batch_size=batchSize,\n",
    "                                                        class_mode='binary',\n",
    "                                                        interpolation='bilinear',\n",
    "                                                        shuffle=False,)\n",
    "\n",
    "    print('train shape :',train_generator.classes.shape)\n",
    "    # unbalanced class를 해결하기 위한 class_weight 설정 #np.unique는 중복원소제거하는거\n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.unique(train_generator.classes), train_generator.classes) \n",
    "\n",
    "    # callback 함수 정의 dropout함수\n",
    "    early_stop = EarlyStopping(patience=10, monitor='val_loss')\n",
    "    cb_checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "    # 모델 학습\n",
    "    hist = model.fit_generator(train_generator, \n",
    "                               epochs=nEpoch,\n",
    "                               steps_per_epoch=len(train_generator),\n",
    "                               class_weight=class_weights,\n",
    "                               validation_data=valid_generator,\n",
    "                               validation_steps=len(valid_generator),\n",
    "                               verbose=1,\n",
    "                               callbacks=[early_stop, cb_checkpoint])\n",
    "\n",
    "    \n",
    "    # 학습 결과 그래프 그리기\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, loss_ax = plt.subplots()\n",
    "    acc_ax = loss_ax.twinx()\n",
    "\n",
    "    loss_ax.plot(hist.history['loss'], 'b', label='train loss')\n",
    "    loss_ax.plot(hist.history['val_loss'], '--r', label='val loss')\n",
    "    loss_ax.set_xlabel('epoch')\n",
    "    loss_ax.set_ylabel('loss')\n",
    "    loss_ax.legend(loc='upper left')\n",
    "\n",
    "    plt.show()\n",
    "    print('acc: {:.4f}\\tloss: {:.4f}'.format(hist.history['val_accuracy'][-11], hist.history['val_loss'][-11]))\n",
    "\n",
    "    print(('{} finished'.format(trainDB)).center(100,'='))\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>  3\n",
      "============================================Densenet121=============================================\n",
      "Found 20952 images belonging to 2 classes.\n",
      "Found 6950 images belonging to 2 classes.\n",
      "train shape : (20952,)\n",
      "Epoch 1/100\n",
      "2618/2619 [============================>.] - ETA: 0s - loss: 0.2147 - accuracy: 0.9214\n",
      "Epoch 00001: val_loss improved from inf to 0.12198, saving model to .\\result_CASIA\\3-01-0.2146.hdf5\n",
      "2619/2619 [==============================] - 1715s 655ms/step - loss: 0.2146 - accuracy: 0.9214 - val_loss: 0.1220 - val_accuracy: 0.9400\n",
      "Epoch 2/100\n",
      "2618/2619 [============================>.] - ETA: 0s - loss: 0.0744 - accuracy: 0.9792\n",
      "Epoch 00002: val_loss improved from 0.12198 to 0.06054, saving model to .\\result_CASIA\\3-02-0.0744.hdf5\n",
      "2619/2619 [==============================] - 1702s 650ms/step - loss: 0.0744 - accuracy: 0.9792 - val_loss: 0.0605 - val_accuracy: 0.9919\n",
      "Epoch 3/100\n",
      "2618/2619 [============================>.] - ETA: 0s - loss: 0.0449 - accuracy: 0.9879\n",
      "Epoch 00003: val_loss did not improve from 0.06054\n",
      "2619/2619 [==============================] - 1688s 644ms/step - loss: 0.0450 - accuracy: 0.9879 - val_loss: 0.1497 - val_accuracy: 0.9371\n",
      "Epoch 4/100\n",
      "2618/2619 [============================>.] - ETA: 0s - loss: 0.0284 - accuracy: 0.9936\n",
      "Epoch 00004: val_loss did not improve from 0.06054\n",
      "2619/2619 [==============================] - 1692s 646ms/step - loss: 0.0284 - accuracy: 0.9937 - val_loss: 0.1822 - val_accuracy: 0.9219\n",
      "Epoch 5/100\n",
      "2618/2619 [============================>.] - ETA: 0s - loss: 0.0229 - accuracy: 0.9951\n",
      "Epoch 00005: val_loss did not improve from 0.06054\n",
      "2619/2619 [==============================] - 1686s 644ms/step - loss: 0.0229 - accuracy: 0.9951 - val_loss: 0.3262 - val_accuracy: 0.8229\n",
      "Epoch 6/100\n",
      "2618/2619 [============================>.] - ETA: 0s - loss: 0.0168 - accuracy: 0.9968\n",
      "Epoch 00006: val_loss improved from 0.06054 to 0.01135, saving model to .\\result_CASIA\\3-06-0.0168.hdf5\n",
      "2619/2619 [==============================] - 1691s 646ms/step - loss: 0.0168 - accuracy: 0.9968 - val_loss: 0.0113 - val_accuracy: 0.9991\n",
      "Epoch 7/100\n",
      "2618/2619 [============================>.] - ETA: 0s - loss: 0.0140 - accuracy: 0.9979\n",
      "Epoch 00007: val_loss did not improve from 0.01135\n",
      "2619/2619 [==============================] - 1678s 641ms/step - loss: 0.0140 - accuracy: 0.9979 - val_loss: 0.0259 - val_accuracy: 0.9912\n",
      "Epoch 8/100\n",
      "2618/2619 [============================>.] - ETA: 0s - loss: 0.0138 - accuracy: 0.9973\n",
      "Epoch 00008: val_loss did not improve from 0.01135\n",
      "2619/2619 [==============================] - 1673s 639ms/step - loss: 0.0138 - accuracy: 0.9973 - val_loss: 0.0492 - val_accuracy: 0.9878\n",
      "Epoch 9/100\n",
      "2618/2619 [============================>.] - ETA: 0s - loss: 0.0129 - accuracy: 0.9975\n",
      "Epoch 00009: val_loss did not improve from 0.01135\n",
      "2619/2619 [==============================] - 1686s 644ms/step - loss: 0.0129 - accuracy: 0.9975 - val_loss: 0.1469 - val_accuracy: 0.9230\n",
      "Epoch 10/100\n",
      "2618/2619 [============================>.] - ETA: 0s - loss: 0.0091 - accuracy: 0.9983\n",
      "Epoch 00010: val_loss did not improve from 0.01135\n",
      "2619/2619 [==============================] - 1696s 648ms/step - loss: 0.0091 - accuracy: 0.9983 - val_loss: 0.0218 - val_accuracy: 0.9954\n",
      "Epoch 11/100\n",
      "2618/2619 [============================>.] - ETA: 0s - loss: 0.0093 - accuracy: 0.9987\n",
      "Epoch 00011: val_loss did not improve from 0.01135\n",
      "2619/2619 [==============================] - 1686s 644ms/step - loss: 0.0093 - accuracy: 0.9987 - val_loss: 0.0192 - val_accuracy: 0.9973\n",
      "Epoch 12/100\n",
      "2618/2619 [============================>.] - ETA: 0s - loss: 0.0080 - accuracy: 0.9985\n",
      "Epoch 00012: val_loss did not improve from 0.01135\n",
      "2619/2619 [==============================] - 1686s 644ms/step - loss: 0.0080 - accuracy: 0.9985 - val_loss: 0.0144 - val_accuracy: 0.9988\n",
      "Epoch 13/100\n",
      "2618/2619 [============================>.] - ETA: 0s - loss: 0.0078 - accuracy: 0.9984\n",
      "Epoch 00013: val_loss did not improve from 0.01135\n",
      "2619/2619 [==============================] - 1680s 641ms/step - loss: 0.0078 - accuracy: 0.9984 - val_loss: 0.0545 - val_accuracy: 0.9706\n",
      "Epoch 14/100\n",
      "2618/2619 [============================>.] - ETA: 0s - loss: 0.0071 - accuracy: 0.9985\n",
      "Epoch 00014: val_loss did not improve from 0.01135\n",
      "2619/2619 [==============================] - 1686s 644ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0193 - val_accuracy: 0.9921\n",
      "Epoch 15/100\n",
      "2618/2619 [============================>.] - ETA: 0s - loss: 0.0053 - accuracy: 0.9989\n",
      "Epoch 00015: val_loss did not improve from 0.01135\n",
      "2619/2619 [==============================] - 1688s 645ms/step - loss: 0.0053 - accuracy: 0.9989 - val_loss: 0.0443 - val_accuracy: 0.9725\n",
      "Epoch 16/100\n",
      "2618/2619 [============================>.] - ETA: 0s - loss: 0.0058 - accuracy: 0.9990\n",
      "Epoch 00016: val_loss improved from 0.01135 to 0.01070, saving model to .\\result_CASIA\\3-16-0.0058.hdf5\n",
      "2619/2619 [==============================] - 1679s 641ms/step - loss: 0.0058 - accuracy: 0.9990 - val_loss: 0.0107 - val_accuracy: 0.9991\n",
      "Epoch 17/100\n",
      "2618/2619 [============================>.] - ETA: 0s - loss: 0.0052 - accuracy: 0.9991\n",
      "Epoch 00017: val_loss did not improve from 0.01070\n",
      "2619/2619 [==============================] - 1687s 644ms/step - loss: 0.0052 - accuracy: 0.9991 - val_loss: 0.0219 - val_accuracy: 0.9925\n",
      "Epoch 18/100\n",
      "2618/2619 [============================>.] - ETA: 0s - loss: 0.0051 - accuracy: 0.9992\n",
      "Epoch 00018: val_loss did not improve from 0.01070\n",
      "2619/2619 [==============================] - 1682s 642ms/step - loss: 0.0051 - accuracy: 0.9992 - val_loss: 0.0154 - val_accuracy: 0.9988\n",
      "Epoch 19/100\n",
      "2618/2619 [============================>.] - ETA: 0s - loss: 0.0064 - accuracy: 0.9987\n",
      "Epoch 00019: val_loss did not improve from 0.01070\n",
      "2619/2619 [==============================] - 1673s 639ms/step - loss: 0.0064 - accuracy: 0.9987 - val_loss: 0.0500 - val_accuracy: 0.9778\n",
      "Epoch 20/100\n",
      "2618/2619 [============================>.] - ETA: 0s - loss: 0.0054 - accuracy: 0.9989\n",
      "Epoch 00020: val_loss improved from 0.01070 to 0.00566, saving model to .\\result_CASIA\\3-20-0.0056.hdf5\n",
      "2619/2619 [==============================] - 1677s 641ms/step - loss: 0.0056 - accuracy: 0.9987 - val_loss: 0.0057 - val_accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "2618/2619 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9997\n",
      "Epoch 00021: val_loss did not improve from 0.00566\n",
      "2619/2619 [==============================] - 1684s 643ms/step - loss: 0.0038 - accuracy: 0.9997 - val_loss: 0.0134 - val_accuracy: 0.9993\n",
      "Epoch 22/100\n",
      "2618/2619 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9995\n",
      "Epoch 00022: val_loss did not improve from 0.00566\n",
      "2619/2619 [==============================] - 1686s 644ms/step - loss: 0.0039 - accuracy: 0.9995 - val_loss: 0.0172 - val_accuracy: 0.9984\n",
      "Epoch 23/100\n",
      "2618/2619 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9992\n",
      "Epoch 00023: val_loss did not improve from 0.00566\n",
      "2619/2619 [==============================] - 1685s 643ms/step - loss: 0.0041 - accuracy: 0.9992 - val_loss: 0.0169 - val_accuracy: 0.9929\n",
      "Epoch 24/100\n",
      "2618/2619 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9993\n",
      "Epoch 00024: val_loss did not improve from 0.00566\n",
      "2619/2619 [==============================] - 1679s 641ms/step - loss: 0.0038 - accuracy: 0.9993 - val_loss: 0.0159 - val_accuracy: 0.9986\n",
      "Epoch 25/100\n",
      "2618/2619 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 0.9996\n",
      "Epoch 00025: val_loss did not improve from 0.00566\n",
      "2619/2619 [==============================] - 1683s 642ms/step - loss: 0.0033 - accuracy: 0.9996 - val_loss: 0.0147 - val_accuracy: 0.9991\n",
      "Epoch 26/100\n",
      "2618/2619 [============================>.] - ETA: 0s - loss: 0.0049 - accuracy: 0.9990\n",
      "Epoch 00026: val_loss did not improve from 0.00566\n",
      "2619/2619 [==============================] - 1685s 643ms/step - loss: 0.0049 - accuracy: 0.9990 - val_loss: 0.0162 - val_accuracy: 0.9991\n",
      "Epoch 27/100\n",
      "2618/2619 [============================>.] - ETA: 0s - loss: 0.0035 - accuracy: 0.9995\n",
      "Epoch 00027: val_loss did not improve from 0.00566\n",
      "2619/2619 [==============================] - 1682s 642ms/step - loss: 0.0035 - accuracy: 0.9995 - val_loss: 0.0154 - val_accuracy: 0.9988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100\n",
      "2618/2619 [============================>.] - ETA: 0s - loss: 0.0038 - accuracy: 0.9991\n",
      "Epoch 00028: val_loss did not improve from 0.00566\n",
      "2619/2619 [==============================] - 1686s 644ms/step - loss: 0.0038 - accuracy: 0.9991 - val_loss: 0.0224 - val_accuracy: 0.9957\n",
      "Epoch 29/100\n",
      "2618/2619 [============================>.] - ETA: 0s - loss: 0.0047 - accuracy: 0.9991\n",
      "Epoch 00029: val_loss did not improve from 0.00566\n",
      "2619/2619 [==============================] - 1703s 650ms/step - loss: 0.0047 - accuracy: 0.9991 - val_loss: 0.0204 - val_accuracy: 0.9974\n",
      "Epoch 30/100\n",
      "2618/2619 [============================>.] - ETA: 0s - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 00030: val_loss did not improve from 0.00566\n",
      "2619/2619 [==============================] - 1704s 651ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0187 - val_accuracy: 0.9981\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 1.0000\tloss: 0.0057\n",
      "=============================================3 finished=============================================\n"
     ]
    }
   ],
   "source": [
    "#### import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "%matplotlib inline\n",
    "SEED = 777\n",
    "\n",
    "def main():\n",
    "    inputSize = 224\n",
    "    batchSize = 8\n",
    "    nEpoch = 100\n",
    "    \n",
    "    # 디렉토리 정보 설정\n",
    "    # dataDir = 'D:\\\\Face_Database\\\\B-Database'\n",
    "    dataDir = 'E:\\\\Face_Database\\\\public-Database\\\\1_crop_result\\\\CASIA-FASD\\\\train'   \n",
    "    # 학습 및 테스트 할 데이터베이스 디렉토리명\n",
    "    trainDB = '3'  # D:\\Face_Database\\B-Database\\protocol_4\\train\n",
    "    validDB = '3' \n",
    "    \n",
    "    saveDir = '.\\\\result_CASIA'\n",
    "    if not os.path.exists(saveDir):    \n",
    "        os.makedirs(saveDir)\n",
    "\n",
    "    model_path = os.path.join(saveDir, trainDB + '-{epoch:02d}-{loss:.4f}.hdf5')\n",
    "    # 데이터베이스별 학습 수행\n",
    "    print('>> ', trainDB)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    print('Densenet121'.center(100, '='))\n",
    "   \n",
    "    '''\n",
    "    training\n",
    "    '''\n",
    "    K.clear_session()\n",
    "\n",
    "    # 네트워크 정의\n",
    "    model = Densenet121(False,weights=None,input_shape=(224, 224, 3))\n",
    "\n",
    "    # 데이터 generator 생성\n",
    "    train_datagen = ImageDataGenerator(rescale=1.0/255.0,\n",
    "                                       horizontal_flip=True,)#이미지augmentation > 이미지를 회전,확대,축소 등 여러변형해보는것.\n",
    "    train_generator = train_datagen.flow_from_directory(os.path.join(*[dataDir,trainDB,'train']),\n",
    "                                                        target_size=(inputSize, inputSize),\n",
    "                                                        batch_size=batchSize,\n",
    "                                                        class_mode='binary',\n",
    "                                                        interpolation='bilinear',\n",
    "                                                        shuffle=True,)\n",
    "\n",
    "    valid_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    valid_generator = valid_datagen.flow_from_directory(os.path.join(*[dataDir, validDB,'val']),\n",
    "                                                        target_size=(inputSize, inputSize),\n",
    "                                                        batch_size=batchSize,\n",
    "                                                        class_mode='binary',\n",
    "                                                        interpolation='bilinear',\n",
    "                                                        shuffle=False,)\n",
    "\n",
    "    print('train shape :',train_generator.classes.shape)\n",
    "    # unbalanced class를 해결하기 위한 class_weight 설정 #np.unique는 중복원소제거하는거\n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.unique(train_generator.classes), train_generator.classes) \n",
    "\n",
    "    # callback 함수 정의 dropout함수\n",
    "    early_stop = EarlyStopping(patience=10, monitor='val_loss')\n",
    "    cb_checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "    # 모델 학습\n",
    "    hist = model.fit_generator(train_generator, \n",
    "                               epochs=nEpoch,\n",
    "                               steps_per_epoch=len(train_generator),\n",
    "                               class_weight=class_weights,\n",
    "                               validation_data=valid_generator,\n",
    "                               validation_steps=len(valid_generator),\n",
    "                               verbose=1,\n",
    "                               callbacks=[early_stop, cb_checkpoint])\n",
    "\n",
    "    \n",
    "    # 학습 결과 그래프 그리기\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, loss_ax = plt.subplots()\n",
    "    acc_ax = loss_ax.twinx()\n",
    "\n",
    "    loss_ax.plot(hist.history['loss'], 'b', label='train loss')\n",
    "    loss_ax.plot(hist.history['val_loss'], '--r', label='val loss')\n",
    "    loss_ax.set_xlabel('epoch')\n",
    "    loss_ax.set_ylabel('loss')\n",
    "    loss_ax.legend(loc='upper left')\n",
    "\n",
    "    plt.show()\n",
    "    print('acc: {:.4f}\\tloss: {:.4f}'.format(hist.history['val_accuracy'][-11], hist.history['val_loss'][-11]))\n",
    "\n",
    "    print(('{} finished'.format(trainDB)).center(100,'='))\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>  4\n",
      "============================================Densenet121=============================================\n",
      "Found 20745 images belonging to 2 classes.\n",
      "Found 7163 images belonging to 2 classes.\n",
      "train shape : (20745,)\n",
      "Epoch 1/100\n",
      "2593/2594 [============================>.] - ETA: 0s - loss: 0.1795 - accuracy: 0.9370\n",
      "Epoch 00001: val_loss improved from inf to 0.75001, saving model to .\\result_CASIA\\4-01-0.1795.hdf5\n",
      "2594/2594 [==============================] - 1688s 651ms/step - loss: 0.1794 - accuracy: 0.9370 - val_loss: 0.7500 - val_accuracy: 0.8610\n",
      "Epoch 2/100\n",
      "2593/2594 [============================>.] - ETA: 0s - loss: 0.0541 - accuracy: 0.9879\n",
      "Epoch 00002: val_loss did not improve from 0.75001\n",
      "2594/2594 [==============================] - 1665s 642ms/step - loss: 0.0541 - accuracy: 0.9879 - val_loss: 3.1514 - val_accuracy: 0.4480\n",
      "Epoch 3/100\n",
      "2593/2594 [============================>.] - ETA: 0s - loss: 0.0318 - accuracy: 0.9932\n",
      "Epoch 00003: val_loss improved from 0.75001 to 0.65423, saving model to .\\result_CASIA\\4-03-0.0319.hdf5\n",
      "2594/2594 [==============================] - 1674s 645ms/step - loss: 0.0319 - accuracy: 0.9933 - val_loss: 0.6542 - val_accuracy: 0.8575\n",
      "Epoch 4/100\n",
      "2593/2594 [============================>.] - ETA: 0s - loss: 0.0202 - accuracy: 0.9972\n",
      "Epoch 00004: val_loss improved from 0.65423 to 0.29002, saving model to .\\result_CASIA\\4-04-0.0202.hdf5\n",
      "2594/2594 [==============================] - 1658s 639ms/step - loss: 0.0202 - accuracy: 0.9972 - val_loss: 0.2900 - val_accuracy: 0.9136\n",
      "Epoch 5/100\n",
      "2593/2594 [============================>.] - ETA: 0s - loss: 0.0189 - accuracy: 0.9959\n",
      "Epoch 00005: val_loss did not improve from 0.29002\n",
      "2594/2594 [==============================] - 1667s 643ms/step - loss: 0.0189 - accuracy: 0.9959 - val_loss: 0.8800 - val_accuracy: 0.8478\n",
      "Epoch 6/100\n",
      "2593/2594 [============================>.] - ETA: 0s - loss: 0.0147 - accuracy: 0.9982\n",
      "Epoch 00006: val_loss did not improve from 0.29002\n",
      "2594/2594 [==============================] - 1665s 642ms/step - loss: 0.0147 - accuracy: 0.9982 - val_loss: 0.3447 - val_accuracy: 0.8797\n",
      "Epoch 7/100\n",
      "2593/2594 [============================>.] - ETA: 0s - loss: 0.0110 - accuracy: 0.9983\n",
      "Epoch 00007: val_loss did not improve from 0.29002\n",
      "2594/2594 [==============================] - 1655s 638ms/step - loss: 0.0110 - accuracy: 0.9983 - val_loss: 0.6932 - val_accuracy: 0.8312\n",
      "Epoch 8/100\n",
      "2593/2594 [============================>.] - ETA: 0s - loss: 0.0123 - accuracy: 0.9976\n",
      "Epoch 00008: val_loss did not improve from 0.29002\n",
      "2594/2594 [==============================] - 1663s 641ms/step - loss: 0.0123 - accuracy: 0.9976 - val_loss: 0.4099 - val_accuracy: 0.8572\n",
      "Epoch 9/100\n",
      "2593/2594 [============================>.] - ETA: 0s - loss: 0.0074 - accuracy: 0.9989\n",
      "Epoch 00009: val_loss improved from 0.29002 to 0.28510, saving model to .\\result_CASIA\\4-09-0.0075.hdf5\n",
      "2594/2594 [==============================] - 1664s 642ms/step - loss: 0.0075 - accuracy: 0.9989 - val_loss: 0.2851 - val_accuracy: 0.9034\n",
      "Epoch 10/100\n",
      "2593/2594 [============================>.] - ETA: 0s - loss: 0.0095 - accuracy: 0.9989\n",
      "Epoch 00010: val_loss improved from 0.28510 to 0.24468, saving model to .\\result_CASIA\\4-10-0.0076.hdf5\n",
      "2594/2594 [==============================] - 1660s 640ms/step - loss: 0.0095 - accuracy: 0.9989 - val_loss: 0.2447 - val_accuracy: 0.9010\n",
      "Epoch 11/100\n",
      "2593/2594 [============================>.] - ETA: 0s - loss: 0.0072 - accuracy: 0.9989\n",
      "Epoch 00011: val_loss did not improve from 0.24468\n",
      "2594/2594 [==============================] - 1665s 642ms/step - loss: 0.0072 - accuracy: 0.9989 - val_loss: 0.7982 - val_accuracy: 0.7110\n",
      "Epoch 12/100\n",
      "2593/2594 [============================>.] - ETA: 0s - loss: 0.0052 - accuracy: 0.9995\n",
      "Epoch 00012: val_loss did not improve from 0.24468\n",
      "2594/2594 [==============================] - 1657s 639ms/step - loss: 0.0052 - accuracy: 0.9995 - val_loss: 0.5272 - val_accuracy: 0.8255\n",
      "Epoch 13/100\n",
      "2593/2594 [============================>.] - ETA: 0s - loss: 0.0054 - accuracy: 0.9994\n",
      "Epoch 00013: val_loss improved from 0.24468 to 0.12999, saving model to .\\result_CASIA\\4-13-0.0054.hdf5\n",
      "2594/2594 [==============================] - 1664s 642ms/step - loss: 0.0054 - accuracy: 0.9994 - val_loss: 0.1300 - val_accuracy: 0.9386\n",
      "Epoch 14/100\n",
      "2593/2594 [============================>.] - ETA: 0s - loss: 0.0059 - accuracy: 0.9991\n",
      "Epoch 00014: val_loss did not improve from 0.12999\n",
      "2594/2594 [==============================] - 1674s 645ms/step - loss: 0.0059 - accuracy: 0.9991 - val_loss: 0.4931 - val_accuracy: 0.8598\n",
      "Epoch 15/100\n",
      "2593/2594 [============================>.] - ETA: 0s - loss: 0.0042 - accuracy: 0.9998\n",
      "Epoch 00015: val_loss did not improve from 0.12999\n",
      "2594/2594 [==============================] - 1653s 637ms/step - loss: 0.0042 - accuracy: 0.9998 - val_loss: 0.4940 - val_accuracy: 0.8321\n",
      "Epoch 16/100\n",
      "2593/2594 [============================>.] - ETA: 0s - loss: 0.0057 - accuracy: 0.9991\n",
      "Epoch 00016: val_loss did not improve from 0.12999\n",
      "2594/2594 [==============================] - 1658s 639ms/step - loss: 0.0057 - accuracy: 0.9991 - val_loss: 0.5175 - val_accuracy: 0.8565\n",
      "Epoch 17/100\n",
      "2593/2594 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9995\n",
      "Epoch 00017: val_loss did not improve from 0.12999\n",
      "2594/2594 [==============================] - 1662s 641ms/step - loss: 0.0043 - accuracy: 0.9995 - val_loss: 0.3438 - val_accuracy: 0.8758\n",
      "Epoch 18/100\n",
      "2593/2594 [============================>.] - ETA: 0s - loss: 0.0044 - accuracy: 0.9994\n",
      "Epoch 00018: val_loss did not improve from 0.12999\n",
      "2594/2594 [==============================] - 1654s 638ms/step - loss: 0.0044 - accuracy: 0.9994 - val_loss: 0.5535 - val_accuracy: 0.8018\n",
      "Epoch 19/100\n",
      "2593/2594 [============================>.] - ETA: 0s - loss: 0.0032 - accuracy: 0.9997\n",
      "Epoch 00019: val_loss did not improve from 0.12999\n",
      "2594/2594 [==============================] - 1658s 639ms/step - loss: 0.0032 - accuracy: 0.9997 - val_loss: 1.2667 - val_accuracy: 0.6806\n",
      "Epoch 20/100\n",
      "2593/2594 [============================>.] - ETA: 0s - loss: 0.0039 - accuracy: 0.9994\n",
      "Epoch 00020: val_loss did not improve from 0.12999\n",
      "2594/2594 [==============================] - 1667s 643ms/step - loss: 0.0039 - accuracy: 0.9994 - val_loss: 0.6120 - val_accuracy: 0.7955\n",
      "Epoch 21/100\n",
      "2593/2594 [============================>.] - ETA: 0s - loss: 0.0043 - accuracy: 0.9994\n",
      "Epoch 00021: val_loss did not improve from 0.12999\n",
      "2594/2594 [==============================] - 1662s 641ms/step - loss: 0.0043 - accuracy: 0.9994 - val_loss: 0.4427 - val_accuracy: 0.8562\n",
      "Epoch 22/100\n",
      "2593/2594 [============================>.] - ETA: 0s - loss: 0.0068 - accuracy: 0.9992\n",
      "Epoch 00022: val_loss did not improve from 0.12999\n",
      "2594/2594 [==============================] - 1663s 641ms/step - loss: 0.0068 - accuracy: 0.9992 - val_loss: 0.8962 - val_accuracy: 0.7268\n",
      "Epoch 23/100\n",
      "2593/2594 [============================>.] - ETA: 0s - loss: 0.0029 - accuracy: 0.9998\n",
      "Epoch 00023: val_loss did not improve from 0.12999\n",
      "2594/2594 [==============================] - 1654s 638ms/step - loss: 0.0029 - accuracy: 0.9998 - val_loss: 0.6577 - val_accuracy: 0.7828\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9386\tloss: 0.1300\n",
      "=============================================4 finished=============================================\n"
     ]
    }
   ],
   "source": [
    "#### import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "%matplotlib inline\n",
    "SEED = 777\n",
    "\n",
    "def main():\n",
    "    inputSize = 224\n",
    "    batchSize = 8\n",
    "    nEpoch = 100\n",
    "    \n",
    "    # 디렉토리 정보 설정\n",
    "    # dataDir = 'D:\\\\Face_Database\\\\B-Database'\n",
    "    dataDir = 'E:\\\\Face_Database\\\\public-Database\\\\1_crop_result\\\\CASIA-FASD\\\\train'   \n",
    "    # 학습 및 테스트 할 데이터베이스 디렉토리명\n",
    "    trainDB = '4'  # D:\\Face_Database\\B-Database\\protocol_4\\train\n",
    "    validDB = '4' \n",
    "    \n",
    "    saveDir = '.\\\\result_CASIA'\n",
    "    if not os.path.exists(saveDir):    \n",
    "        os.makedirs(saveDir)\n",
    "\n",
    "    model_path = os.path.join(saveDir, trainDB + '-{epoch:02d}-{loss:.4f}.hdf5')\n",
    "    # 데이터베이스별 학습 수행\n",
    "    print('>> ', trainDB)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    print('Densenet121'.center(100, '='))\n",
    "   \n",
    "    '''\n",
    "    training\n",
    "    '''\n",
    "    K.clear_session()\n",
    "\n",
    "    # 네트워크 정의\n",
    "    model = Densenet121(False,weights=None,input_shape=(224, 224, 3))\n",
    "\n",
    "    # 데이터 generator 생성\n",
    "    train_datagen = ImageDataGenerator(rescale=1.0/255.0,\n",
    "                                       horizontal_flip=True,)#이미지augmentation > 이미지를 회전,확대,축소 등 여러변형해보는것.\n",
    "    train_generator = train_datagen.flow_from_directory(os.path.join(*[dataDir,trainDB,'train']),\n",
    "                                                        target_size=(inputSize, inputSize),\n",
    "                                                        batch_size=batchSize,\n",
    "                                                        class_mode='binary',\n",
    "                                                        interpolation='bilinear',\n",
    "                                                        shuffle=True,)\n",
    "\n",
    "    valid_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    valid_generator = valid_datagen.flow_from_directory(os.path.join(*[dataDir, validDB,'val']),\n",
    "                                                        target_size=(inputSize, inputSize),\n",
    "                                                        batch_size=batchSize,\n",
    "                                                        class_mode='binary',\n",
    "                                                        interpolation='bilinear',\n",
    "                                                        shuffle=False,)\n",
    "\n",
    "    print('train shape :',train_generator.classes.shape)\n",
    "    # unbalanced class를 해결하기 위한 class_weight 설정 #np.unique는 중복원소제거하는거\n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.unique(train_generator.classes), train_generator.classes) \n",
    "\n",
    "    # callback 함수 정의 dropout함수\n",
    "    early_stop = EarlyStopping(patience=10, monitor='val_loss')\n",
    "    cb_checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "    # 모델 학습\n",
    "    hist = model.fit_generator(train_generator, \n",
    "                               epochs=nEpoch,\n",
    "                               steps_per_epoch=len(train_generator),\n",
    "                               class_weight=class_weights,\n",
    "                               validation_data=valid_generator,\n",
    "                               validation_steps=len(valid_generator),\n",
    "                               verbose=1,\n",
    "                               callbacks=[early_stop, cb_checkpoint])\n",
    "\n",
    "    \n",
    "    # 학습 결과 그래프 그리기\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, loss_ax = plt.subplots()\n",
    "    acc_ax = loss_ax.twinx()\n",
    "\n",
    "    loss_ax.plot(hist.history['loss'], 'b', label='train loss')\n",
    "    loss_ax.plot(hist.history['val_loss'], '--r', label='val loss')\n",
    "    loss_ax.set_xlabel('epoch')\n",
    "    loss_ax.set_ylabel('loss')\n",
    "    loss_ax.legend(loc='upper left')\n",
    "\n",
    "    plt.show()\n",
    "    print('acc: {:.4f}\\tloss: {:.4f}'.format(hist.history['val_accuracy'][-11], hist.history['val_loss'][-11]))\n",
    "\n",
    "    print(('{} finished'.format(trainDB)).center(100,'='))\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>  Result\n",
      "============================================Densenet121=============================================\n",
      "Found 27901 images belonging to 2 classes.\n",
      "train shape : (27901,)\n",
      "Epoch 1/17\n",
      "3487/3488 [============================>.] - ETA: 0s - loss: 0.1839 - accuracy: 0.9338\n",
      "Epoch 00001: loss improved from inf to 0.18385, saving model to .\\result_CASIA\\Result-01-0.1839.hdf5\n",
      "3488/3488 [==============================] - 2010s 576ms/step - loss: 0.1839 - accuracy: 0.9338\n",
      "Epoch 2/17\n",
      "3487/3488 [============================>.] - ETA: 0s - loss: 0.0596 - accuracy: 0.9829\n",
      "Epoch 00002: loss improved from 0.18385 to 0.05958, saving model to .\\result_CASIA\\Result-02-0.0596.hdf5\n",
      "3488/3488 [==============================] - 1980s 568ms/step - loss: 0.0596 - accuracy: 0.9829\n",
      "Epoch 3/17\n",
      "3487/3488 [============================>.] - ETA: 0s - loss: 0.0355 - accuracy: 0.9911\n",
      "Epoch 00003: loss improved from 0.05958 to 0.03548, saving model to .\\result_CASIA\\Result-03-0.0355.hdf5\n",
      "3488/3488 [==============================] - 1980s 568ms/step - loss: 0.0355 - accuracy: 0.9911\n",
      "Epoch 4/17\n",
      "3487/3488 [============================>.] - ETA: 0s - loss: 0.0236 - accuracy: 0.9953\n",
      "Epoch 00004: loss improved from 0.03548 to 0.02359, saving model to .\\result_CASIA\\Result-04-0.0236.hdf5\n",
      "3488/3488 [==============================] - 1982s 568ms/step - loss: 0.0236 - accuracy: 0.9953\n",
      "Epoch 5/17\n",
      "3487/3488 [============================>.] - ETA: 0s - loss: 0.0201 - accuracy: 0.9947\n",
      "Epoch 00005: loss improved from 0.02359 to 0.02013, saving model to .\\result_CASIA\\Result-05-0.0201.hdf5\n",
      "3488/3488 [==============================] - 1974s 566ms/step - loss: 0.0201 - accuracy: 0.9947\n",
      "Epoch 6/17\n",
      "3487/3488 [============================>.] - ETA: 0s - loss: 0.0151 - accuracy: 0.9966\n",
      "Epoch 00006: loss improved from 0.02013 to 0.01512, saving model to .\\result_CASIA\\Result-06-0.0151.hdf5\n",
      "3488/3488 [==============================] - 1978s 567ms/step - loss: 0.0151 - accuracy: 0.9966\n",
      "Epoch 7/17\n",
      "3487/3488 [============================>.] - ETA: 0s - loss: 0.0143 - accuracy: 0.9970\n",
      "Epoch 00007: loss improved from 0.01512 to 0.01432, saving model to .\\result_CASIA\\Result-07-0.0143.hdf5\n",
      "3488/3488 [==============================] - 1971s 565ms/step - loss: 0.0143 - accuracy: 0.9970\n",
      "Epoch 8/17\n",
      "3487/3488 [============================>.] - ETA: 0s - loss: 0.0113 - accuracy: 0.9978\n",
      "Epoch 00008: loss improved from 0.01432 to 0.01127, saving model to .\\result_CASIA\\Result-08-0.0113.hdf5\n",
      "3488/3488 [==============================] - 1970s 565ms/step - loss: 0.0113 - accuracy: 0.9978\n",
      "Epoch 9/17\n",
      "3487/3488 [============================>.] - ETA: 0s - loss: 0.0112 - accuracy: 0.9976\n",
      "Epoch 00009: loss improved from 0.01127 to 0.01118, saving model to .\\result_CASIA\\Result-09-0.0112.hdf5\n",
      "3488/3488 [==============================] - 1970s 565ms/step - loss: 0.0112 - accuracy: 0.9976\n",
      "Epoch 10/17\n",
      "3487/3488 [============================>.] - ETA: 0s - loss: 0.0095 - accuracy: 0.9978\n",
      "Epoch 00010: loss improved from 0.01118 to 0.00948, saving model to .\\result_CASIA\\Result-10-0.0095.hdf5\n",
      "3488/3488 [==============================] - 1971s 565ms/step - loss: 0.0095 - accuracy: 0.9978\n",
      "Epoch 11/17\n",
      "3487/3488 [============================>.] - ETA: 0s - loss: 0.0095 - accuracy: 0.9978\n",
      "Epoch 00011: loss did not improve from 0.00948\n",
      "3488/3488 [==============================] - 1971s 565ms/step - loss: 0.0095 - accuracy: 0.9978\n",
      "Epoch 12/17\n",
      "3487/3488 [============================>.] - ETA: 0s - loss: 0.0059 - accuracy: 0.9993\n",
      "Epoch 00012: loss improved from 0.00948 to 0.00590, saving model to .\\result_CASIA\\Result-12-0.0059.hdf5\n",
      "3488/3488 [==============================] - 1973s 566ms/step - loss: 0.0059 - accuracy: 0.9993\n",
      "Epoch 13/17\n",
      "3487/3488 [============================>.] - ETA: 0s - loss: 0.0076 - accuracy: 0.9989\n",
      "Epoch 00013: loss did not improve from 0.00590\n",
      "3488/3488 [==============================] - 1961s 562ms/step - loss: 0.0076 - accuracy: 0.9989\n",
      "Epoch 14/17\n",
      "3487/3488 [============================>.] - ETA: 0s - loss: 0.0044 - accuracy: 0.9997\n",
      "Epoch 00014: loss improved from 0.00590 to 0.00438, saving model to .\\result_CASIA\\Result-14-0.0044.hdf5\n",
      "3488/3488 [==============================] - 1970s 565ms/step - loss: 0.0044 - accuracy: 0.9997\n",
      "Epoch 15/17\n",
      "3487/3488 [============================>.] - ETA: 0s - loss: 0.0053 - accuracy: 0.9993\n",
      "Epoch 00015: loss did not improve from 0.00438\n",
      "3488/3488 [==============================] - 1970s 565ms/step - loss: 0.0053 - accuracy: 0.9993\n",
      "Epoch 16/17\n",
      "3487/3488 [============================>.] - ETA: 0s - loss: 0.0045 - accuracy: 0.9994\n",
      "Epoch 00016: loss did not improve from 0.00438\n",
      "3488/3488 [==============================] - 1970s 565ms/step - loss: 0.0045 - accuracy: 0.9994\n",
      "Epoch 17/17\n",
      "3487/3488 [============================>.] - ETA: 0s - loss: 0.0063 - accuracy: 0.9987\n",
      "Epoch 00017: loss did not improve from 0.00438\n",
      "3488/3488 [==============================] - 1972s 565ms/step - loss: 0.0063 - accuracy: 0.9987\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9970\tloss: 0.0143\n",
      "==========================================Result finished===========================================\n"
     ]
    }
   ],
   "source": [
    "#### import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "%matplotlib inline\n",
    "SEED = 777\n",
    "\n",
    "def main():\n",
    "    inputSize = 224\n",
    "    batchSize = 8\n",
    "    nEpoch = 17\n",
    "    \n",
    "    # 디렉토리 정보 설정\n",
    "    # dataDir = 'D:\\\\Face_Database\\\\B-Database'\n",
    "    dataDir = 'E:\\\\Face_Database\\\\public-Database\\\\1_crop_result\\\\CASIA-FASD\\\\train'   \n",
    "    # 학습 및 테스트 할 데이터베이스 디렉토리명\n",
    "    trainDB = 'Result'  # D:\\Face_Database\\B-Database\\protocol_4\\train\n",
    " \n",
    "    \n",
    "    saveDir = '.\\\\result_CASIA'\n",
    "    if not os.path.exists(saveDir):    \n",
    "        os.makedirs(saveDir)\n",
    "\n",
    "    model_path = os.path.join(saveDir, trainDB + '-{epoch:02d}-{loss:.4f}.hdf5')\n",
    "    # 데이터베이스별 학습 수행\n",
    "    print('>> ', trainDB)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    print('Densenet121'.center(100, '='))\n",
    "   \n",
    "    '''\n",
    "    training\n",
    "    '''\n",
    "    K.clear_session()\n",
    "\n",
    "    # 네트워크 정의\n",
    "    model = Densenet121(False,weights=None,input_shape=(224, 224, 3))\n",
    "\n",
    "    # 데이터 generator 생성\n",
    "    train_datagen = ImageDataGenerator(rescale=1.0/255.0,\n",
    "                                       horizontal_flip=True,)#이미지augmentation > 이미지를 회전,확대,축소 등 여러변형해보는것.\n",
    "    train_generator = train_datagen.flow_from_directory(os.path.join(*[dataDir,trainDB,'train']),\n",
    "                                                        target_size=(inputSize, inputSize),\n",
    "                                                        batch_size=batchSize,\n",
    "                                                        class_mode='binary',\n",
    "                                                        interpolation='bilinear',\n",
    "                                                        shuffle=True,)\n",
    "\n",
    "  \n",
    "    print('train shape :',train_generator.classes.shape)\n",
    "    # unbalanced class를 해결하기 위한 class_weight 설정 #np.unique는 중복원소제거하는거\n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.unique(train_generator.classes), train_generator.classes) \n",
    "\n",
    "    # callback 함수 정의 dropout함수\n",
    "    early_stop = EarlyStopping(patience=10, monitor='loss')\n",
    "    cb_checkpoint = ModelCheckpoint(filepath=model_path, monitor='loss', verbose=1, save_best_only=True)\n",
    "\n",
    "    # 모델 학습\n",
    "    hist = model.fit_generator(train_generator, \n",
    "                               epochs=nEpoch,\n",
    "                               steps_per_epoch=len(train_generator),\n",
    "                               class_weight=class_weights,\n",
    "                               verbose=1,\n",
    "                               callbacks=[cb_checkpoint]\n",
    "                               )\n",
    "\n",
    "    \n",
    "    # 학습 결과 그래프 그리기\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, loss_ax = plt.subplots()\n",
    "    acc_ax = loss_ax.twinx()\n",
    "\n",
    "    loss_ax.plot(hist.history['loss'], 'b', label='train loss')\n",
    " #   loss_ax.plot(hist.history['val_loss'], '--r', label='val loss')\n",
    "    loss_ax.set_xlabel('epoch')\n",
    "    loss_ax.set_ylabel('loss')\n",
    "    loss_ax.legend(loc='upper left')\n",
    "\n",
    "    plt.show()\n",
    "    print('acc: {:.4f}\\tloss: {:.4f}'.format(hist.history['accuracy'][-11], hist.history['loss'][-11]))\n",
    "\n",
    "    print(('{} finished'.format(trainDB)).center(100,'='))\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> model loaded: Result-14-0.0044.hdf5\n",
      ">>>> evaluating on 'CASIA-FASD'\n",
      "Found 40389 images belonging to 2 classes.\n",
      "                  pred_fake(0)   pred_real(1)\n",
      "actural_fake(0)           29713            714\n",
      "actual_real(1)             234           9728\n",
      "\n",
      "EER: 0.0235\tHTER: 0.0235\n",
      ">> finished\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학습된 모델 테스트셋에서 성능 평가\n",
    "# 얼굴 스푸핑 분야에서 평가 metric으로 HTER이랑 EER 두 개 주로 사용\n",
    "# =========================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def main():\n",
    "    inputSize = 224\n",
    "    batchSize = 8\n",
    "    \n",
    "    testDB = 'CASIA-FASD'\n",
    "    \n",
    "    dataDir = 'E:\\\\Face_Database\\\\public-Database\\\\1_crop_result\\\\CASIA-FASD'\n",
    "    modelPath = 'C:\\\\Users\\\\ysk00\\\\OneDrive\\\\바탕 화면\\\\prlab\\\\ysg\\\\densenet-spoofing\\\\result_CASIA\\\\Result-14-0.0044.hdf5'\n",
    "    \n",
    "    print('>> model loaded: {}'.format(os.path.basename(modelPath)))\n",
    "    K.clear_session()\n",
    "    model = load_model(modelPath)\n",
    "        \n",
    "    print(\">>>> evaluating on '{}'\".format(testDB))\n",
    "    test_datagen = ImageDataGenerator()\n",
    "    test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    test_generator = test_datagen.flow_from_directory(os.path.join(*[dataDir, testDB, 'test']),\n",
    "                                                      target_size=(inputSize, inputSize),\n",
    "                                                      batch_size=batchSize,\n",
    "                                                      class_mode='binary',\n",
    "                                                      interpolation='bilinear',\n",
    "                                                      shuffle=False,)\n",
    "\n",
    "#     ''' evaluating EER '''\n",
    "#     y_true = val_generator.classes\n",
    "#     y_score = model.predict_generator(val_generator, steps=len(val_generator)).ravel()\n",
    "\n",
    "#     fpr, tpr, threshold = roc_curve(y_true, y_score)\n",
    "#     fnr = 1 - tpr\n",
    "#     eer_threshold = threshold[np.nanargmin(np.absolute((fnr-fpr)))]\n",
    "#     val_eer = (fpr[np.nanargmin(np.absolute((fnr - fpr)))] + fnr[np.nanargmin(np.absolute((fnr - fpr)))]) / 2\n",
    "\n",
    "    ''' evaluating HTER '''\n",
    "    y_true = test_generator.classes\n",
    "    y_score = model.predict_generator(test_generator, steps = len(test_generator)).ravel()\n",
    "\n",
    "    # Calculate EER threshold\n",
    "    fpr, tpr, threshold = roc_curve(y_true, y_score)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr-fpr)))]\n",
    "    \n",
    "    test_eer = (fpr[np.nanargmin(np.absolute((fnr - fpr)))] + fnr[np.nanargmin(np.absolute((fnr - fpr)))]) / 2\n",
    "\n",
    "    # HTER\n",
    "    y_pred = y_score > eer_threshold\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    labels = test_generator.class_indices\n",
    "    print('                  pred_fake({})   pred_real({})\\nactural_fake({})    {:12d}   {:12d}\\nactual_real({})    {:12d}   {:12d}\\n'.format(labels['Fake'], labels['Real'], labels['Fake'], tn, fp, labels['Real'], fn, tp))\n",
    "    hter = (fp/(tn+fp) + fn/(fn+tp)) * 0.5\n",
    "\n",
    "    # ROC curve\n",
    "    Accuracy = ((tn+tp) / (tn+fp+fn+tp)) * 100.0\n",
    "\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "\n",
    "    plt.plot(fpr, tpr, 'r--', label='ROC_curve (acc = %0.2f%%)' % Accuracy)\n",
    "    plt.plot([0, 1], [0, 1], color = 'orange', lw=lw, linestyle='--')\n",
    "\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc = \"lower right\")\n",
    "\n",
    "    plt.savefig('protocol_4_publicDB.png')\n",
    "    plt.figure()\n",
    "    \n",
    "    print('EER: {:.4f}\\tHTER: {:.4f}'.format(test_eer, hter))\n",
    "\n",
    "    print('>> finished')\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Densenet 오류찾기 (정면이용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 230, 230, 3)  0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1/conv (Conv2D)             (None, 112, 112, 64) 9408        zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv1/bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1/conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1/relu (Activation)         (None, 112, 112, 64) 0           conv1/bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, 114, 114, 64) 0           conv1/relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1 (MaxPooling2D)            (None, 56, 56, 64)   0           zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 56, 56, 64)   256         pool1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_relu (Activation (None, 56, 56, 64)   0           conv2_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 128)  8192        conv2_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 56, 56, 128)  0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_concat (Concatenat (None, 56, 56, 96)   0           pool1[0][0]                      \n",
      "                                                                 conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_bn (BatchNormali (None, 56, 56, 96)   384         conv2_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_0_relu (Activation (None, 56, 56, 96)   0           conv2_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 128)  12288       conv2_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 56, 56, 128)  0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_concat (Concatenat (None, 56, 56, 128)  0           conv2_block1_concat[0][0]        \n",
      "                                                                 conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_0_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_0_relu (Activation (None, 56, 56, 128)  0           conv2_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 128)  16384       conv2_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 56, 56, 128)  0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_concat (Concatenat (None, 56, 56, 160)  0           conv2_block2_concat[0][0]        \n",
      "                                                                 conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_0_bn (BatchNormali (None, 56, 56, 160)  640         conv2_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_0_relu (Activation (None, 56, 56, 160)  0           conv2_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_conv (Conv2D)    (None, 56, 56, 128)  20480       conv2_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_1_relu (Activation (None, 56, 56, 128)  0           conv2_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4_concat (Concatenat (None, 56, 56, 192)  0           conv2_block3_concat[0][0]        \n",
      "                                                                 conv2_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_0_bn (BatchNormali (None, 56, 56, 192)  768         conv2_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_0_relu (Activation (None, 56, 56, 192)  0           conv2_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_conv (Conv2D)    (None, 56, 56, 128)  24576       conv2_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_1_relu (Activation (None, 56, 56, 128)  0           conv2_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block5_concat (Concatenat (None, 56, 56, 224)  0           conv2_block4_concat[0][0]        \n",
      "                                                                 conv2_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_0_bn (BatchNormali (None, 56, 56, 224)  896         conv2_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_0_relu (Activation (None, 56, 56, 224)  0           conv2_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_conv (Conv2D)    (None, 56, 56, 128)  28672       conv2_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_bn (BatchNormali (None, 56, 56, 128)  512         conv2_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_1_relu (Activation (None, 56, 56, 128)  0           conv2_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_2_conv (Conv2D)    (None, 56, 56, 32)   36864       conv2_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block6_concat (Concatenat (None, 56, 56, 256)  0           conv2_block5_concat[0][0]        \n",
      "                                                                 conv2_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool2_bn (BatchNormalization)   (None, 56, 56, 256)  1024        conv2_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "pool2_relu (Activation)         (None, 56, 56, 256)  0           pool2_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool2_conv (Conv2D)             (None, 56, 56, 128)  32768       pool2_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool2_pool (AveragePooling2D)   (None, 28, 28, 128)  0           pool2_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 28, 28, 128)  512         pool2_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_relu (Activation (None, 28, 28, 128)  0           conv3_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  16384       conv3_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_concat (Concatenat (None, 28, 28, 160)  0           pool2_pool[0][0]                 \n",
      "                                                                 conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_bn (BatchNormali (None, 28, 28, 160)  640         conv3_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_0_relu (Activation (None, 28, 28, 160)  0           conv3_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  20480       conv3_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_concat (Concatenat (None, 28, 28, 192)  0           conv3_block1_concat[0][0]        \n",
      "                                                                 conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_0_bn (BatchNormali (None, 28, 28, 192)  768         conv3_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_0_relu (Activation (None, 28, 28, 192)  0           conv3_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  24576       conv3_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_concat (Concatenat (None, 28, 28, 224)  0           conv3_block2_concat[0][0]        \n",
      "                                                                 conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_0_bn (BatchNormali (None, 28, 28, 224)  896         conv3_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_0_relu (Activation (None, 28, 28, 224)  0           conv3_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  28672       conv3_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_concat (Concatenat (None, 28, 28, 256)  0           conv3_block3_concat[0][0]        \n",
      "                                                                 conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_0_bn (BatchNormali (None, 28, 28, 256)  1024        conv3_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_0_relu (Activation (None, 28, 28, 256)  0           conv3_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_conv (Conv2D)    (None, 28, 28, 128)  32768       conv3_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_1_relu (Activation (None, 28, 28, 128)  0           conv3_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block5_concat (Concatenat (None, 28, 28, 288)  0           conv3_block4_concat[0][0]        \n",
      "                                                                 conv3_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_0_bn (BatchNormali (None, 28, 28, 288)  1152        conv3_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_0_relu (Activation (None, 28, 28, 288)  0           conv3_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_conv (Conv2D)    (None, 28, 28, 128)  36864       conv3_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_1_relu (Activation (None, 28, 28, 128)  0           conv3_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block6_concat (Concatenat (None, 28, 28, 320)  0           conv3_block5_concat[0][0]        \n",
      "                                                                 conv3_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_0_bn (BatchNormali (None, 28, 28, 320)  1280        conv3_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_0_relu (Activation (None, 28, 28, 320)  0           conv3_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_conv (Conv2D)    (None, 28, 28, 128)  40960       conv3_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_1_relu (Activation (None, 28, 28, 128)  0           conv3_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block7_concat (Concatenat (None, 28, 28, 352)  0           conv3_block6_concat[0][0]        \n",
      "                                                                 conv3_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_0_bn (BatchNormali (None, 28, 28, 352)  1408        conv3_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_0_relu (Activation (None, 28, 28, 352)  0           conv3_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_conv (Conv2D)    (None, 28, 28, 128)  45056       conv3_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_1_relu (Activation (None, 28, 28, 128)  0           conv3_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block8_concat (Concatenat (None, 28, 28, 384)  0           conv3_block7_concat[0][0]        \n",
      "                                                                 conv3_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_0_bn (BatchNormali (None, 28, 28, 384)  1536        conv3_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_0_relu (Activation (None, 28, 28, 384)  0           conv3_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_conv (Conv2D)    (None, 28, 28, 128)  49152       conv3_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_1_relu (Activation (None, 28, 28, 128)  0           conv3_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_2_conv (Conv2D)    (None, 28, 28, 32)   36864       conv3_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block9_concat (Concatenat (None, 28, 28, 416)  0           conv3_block8_concat[0][0]        \n",
      "                                                                 conv3_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_0_bn (BatchNormal (None, 28, 28, 416)  1664        conv3_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_0_relu (Activatio (None, 28, 28, 416)  0           conv3_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_conv (Conv2D)   (None, 28, 28, 128)  53248       conv3_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block10_concat (Concatena (None, 28, 28, 448)  0           conv3_block9_concat[0][0]        \n",
      "                                                                 conv3_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_0_bn (BatchNormal (None, 28, 28, 448)  1792        conv3_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_0_relu (Activatio (None, 28, 28, 448)  0           conv3_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_conv (Conv2D)   (None, 28, 28, 128)  57344       conv3_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block11_concat (Concatena (None, 28, 28, 480)  0           conv3_block10_concat[0][0]       \n",
      "                                                                 conv3_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_0_bn (BatchNormal (None, 28, 28, 480)  1920        conv3_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_0_relu (Activatio (None, 28, 28, 480)  0           conv3_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_conv (Conv2D)   (None, 28, 28, 128)  61440       conv3_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_bn (BatchNormal (None, 28, 28, 128)  512         conv3_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_1_relu (Activatio (None, 28, 28, 128)  0           conv3_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_2_conv (Conv2D)   (None, 28, 28, 32)   36864       conv3_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block12_concat (Concatena (None, 28, 28, 512)  0           conv3_block11_concat[0][0]       \n",
      "                                                                 conv3_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3_bn (BatchNormalization)   (None, 28, 28, 512)  2048        conv3_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool3_relu (Activation)         (None, 28, 28, 512)  0           pool3_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool3_conv (Conv2D)             (None, 28, 28, 256)  131072      pool3_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool3_pool (AveragePooling2D)   (None, 14, 14, 256)  0           pool3_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 14, 14, 256)  1024        pool3_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_relu (Activation (None, 14, 14, 256)  0           conv4_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 128)  32768       conv4_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 14, 14, 128)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_concat (Concatenat (None, 14, 14, 288)  0           pool3_pool[0][0]                 \n",
      "                                                                 conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_bn (BatchNormali (None, 14, 14, 288)  1152        conv4_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_0_relu (Activation (None, 14, 14, 288)  0           conv4_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 128)  36864       conv4_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 14, 14, 128)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_concat (Concatenat (None, 14, 14, 320)  0           conv4_block1_concat[0][0]        \n",
      "                                                                 conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_0_bn (BatchNormali (None, 14, 14, 320)  1280        conv4_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_0_relu (Activation (None, 14, 14, 320)  0           conv4_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 128)  40960       conv4_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 14, 14, 128)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_concat (Concatenat (None, 14, 14, 352)  0           conv4_block2_concat[0][0]        \n",
      "                                                                 conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_0_bn (BatchNormali (None, 14, 14, 352)  1408        conv4_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_0_relu (Activation (None, 14, 14, 352)  0           conv4_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 128)  45056       conv4_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 14, 14, 128)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_concat (Concatenat (None, 14, 14, 384)  0           conv4_block3_concat[0][0]        \n",
      "                                                                 conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_0_bn (BatchNormali (None, 14, 14, 384)  1536        conv4_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_0_relu (Activation (None, 14, 14, 384)  0           conv4_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 128)  49152       conv4_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 14, 14, 128)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_concat (Concatenat (None, 14, 14, 416)  0           conv4_block4_concat[0][0]        \n",
      "                                                                 conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_0_bn (BatchNormali (None, 14, 14, 416)  1664        conv4_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_0_relu (Activation (None, 14, 14, 416)  0           conv4_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 128)  53248       conv4_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 14, 14, 128)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_concat (Concatenat (None, 14, 14, 448)  0           conv4_block5_concat[0][0]        \n",
      "                                                                 conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_0_bn (BatchNormali (None, 14, 14, 448)  1792        conv4_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_0_relu (Activation (None, 14, 14, 448)  0           conv4_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_conv (Conv2D)    (None, 14, 14, 128)  57344       conv4_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_1_relu (Activation (None, 14, 14, 128)  0           conv4_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block7_concat (Concatenat (None, 14, 14, 480)  0           conv4_block6_concat[0][0]        \n",
      "                                                                 conv4_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_0_bn (BatchNormali (None, 14, 14, 480)  1920        conv4_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_0_relu (Activation (None, 14, 14, 480)  0           conv4_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_conv (Conv2D)    (None, 14, 14, 128)  61440       conv4_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_1_relu (Activation (None, 14, 14, 128)  0           conv4_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block8_concat (Concatenat (None, 14, 14, 512)  0           conv4_block7_concat[0][0]        \n",
      "                                                                 conv4_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_0_bn (BatchNormali (None, 14, 14, 512)  2048        conv4_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_0_relu (Activation (None, 14, 14, 512)  0           conv4_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_conv (Conv2D)    (None, 14, 14, 128)  65536       conv4_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_bn (BatchNormali (None, 14, 14, 128)  512         conv4_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_1_relu (Activation (None, 14, 14, 128)  0           conv4_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_2_conv (Conv2D)    (None, 14, 14, 32)   36864       conv4_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block9_concat (Concatenat (None, 14, 14, 544)  0           conv4_block8_concat[0][0]        \n",
      "                                                                 conv4_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_0_bn (BatchNormal (None, 14, 14, 544)  2176        conv4_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_0_relu (Activatio (None, 14, 14, 544)  0           conv4_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_conv (Conv2D)   (None, 14, 14, 128)  69632       conv4_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block10_concat (Concatena (None, 14, 14, 576)  0           conv4_block9_concat[0][0]        \n",
      "                                                                 conv4_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_0_bn (BatchNormal (None, 14, 14, 576)  2304        conv4_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_0_relu (Activatio (None, 14, 14, 576)  0           conv4_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_conv (Conv2D)   (None, 14, 14, 128)  73728       conv4_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block11_concat (Concatena (None, 14, 14, 608)  0           conv4_block10_concat[0][0]       \n",
      "                                                                 conv4_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_0_bn (BatchNormal (None, 14, 14, 608)  2432        conv4_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_0_relu (Activatio (None, 14, 14, 608)  0           conv4_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_conv (Conv2D)   (None, 14, 14, 128)  77824       conv4_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block12_concat (Concatena (None, 14, 14, 640)  0           conv4_block11_concat[0][0]       \n",
      "                                                                 conv4_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_0_bn (BatchNormal (None, 14, 14, 640)  2560        conv4_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_0_relu (Activatio (None, 14, 14, 640)  0           conv4_block13_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_conv (Conv2D)   (None, 14, 14, 128)  81920       conv4_block13_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block13_concat (Concatena (None, 14, 14, 672)  0           conv4_block12_concat[0][0]       \n",
      "                                                                 conv4_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_0_bn (BatchNormal (None, 14, 14, 672)  2688        conv4_block13_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_0_relu (Activatio (None, 14, 14, 672)  0           conv4_block14_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_conv (Conv2D)   (None, 14, 14, 128)  86016       conv4_block14_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block14_concat (Concatena (None, 14, 14, 704)  0           conv4_block13_concat[0][0]       \n",
      "                                                                 conv4_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_0_bn (BatchNormal (None, 14, 14, 704)  2816        conv4_block14_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_0_relu (Activatio (None, 14, 14, 704)  0           conv4_block15_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_conv (Conv2D)   (None, 14, 14, 128)  90112       conv4_block15_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block15_concat (Concatena (None, 14, 14, 736)  0           conv4_block14_concat[0][0]       \n",
      "                                                                 conv4_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_0_bn (BatchNormal (None, 14, 14, 736)  2944        conv4_block15_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_0_relu (Activatio (None, 14, 14, 736)  0           conv4_block16_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_conv (Conv2D)   (None, 14, 14, 128)  94208       conv4_block16_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block16_concat (Concatena (None, 14, 14, 768)  0           conv4_block15_concat[0][0]       \n",
      "                                                                 conv4_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_0_bn (BatchNormal (None, 14, 14, 768)  3072        conv4_block16_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_0_relu (Activatio (None, 14, 14, 768)  0           conv4_block17_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_conv (Conv2D)   (None, 14, 14, 128)  98304       conv4_block17_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block17_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block17_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block17_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block17_concat (Concatena (None, 14, 14, 800)  0           conv4_block16_concat[0][0]       \n",
      "                                                                 conv4_block17_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_0_bn (BatchNormal (None, 14, 14, 800)  3200        conv4_block17_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_0_relu (Activatio (None, 14, 14, 800)  0           conv4_block18_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_conv (Conv2D)   (None, 14, 14, 128)  102400      conv4_block18_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block18_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block18_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block18_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block18_concat (Concatena (None, 14, 14, 832)  0           conv4_block17_concat[0][0]       \n",
      "                                                                 conv4_block18_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_0_bn (BatchNormal (None, 14, 14, 832)  3328        conv4_block18_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_0_relu (Activatio (None, 14, 14, 832)  0           conv4_block19_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_conv (Conv2D)   (None, 14, 14, 128)  106496      conv4_block19_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block19_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block19_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block19_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block19_concat (Concatena (None, 14, 14, 864)  0           conv4_block18_concat[0][0]       \n",
      "                                                                 conv4_block19_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_0_bn (BatchNormal (None, 14, 14, 864)  3456        conv4_block19_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_0_relu (Activatio (None, 14, 14, 864)  0           conv4_block20_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_conv (Conv2D)   (None, 14, 14, 128)  110592      conv4_block20_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block20_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block20_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block20_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block20_concat (Concatena (None, 14, 14, 896)  0           conv4_block19_concat[0][0]       \n",
      "                                                                 conv4_block20_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_0_bn (BatchNormal (None, 14, 14, 896)  3584        conv4_block20_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_0_relu (Activatio (None, 14, 14, 896)  0           conv4_block21_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_conv (Conv2D)   (None, 14, 14, 128)  114688      conv4_block21_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block21_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block21_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block21_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block21_concat (Concatena (None, 14, 14, 928)  0           conv4_block20_concat[0][0]       \n",
      "                                                                 conv4_block21_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_0_bn (BatchNormal (None, 14, 14, 928)  3712        conv4_block21_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_0_relu (Activatio (None, 14, 14, 928)  0           conv4_block22_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_conv (Conv2D)   (None, 14, 14, 128)  118784      conv4_block22_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block22_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block22_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block22_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block22_concat (Concatena (None, 14, 14, 960)  0           conv4_block21_concat[0][0]       \n",
      "                                                                 conv4_block22_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_0_bn (BatchNormal (None, 14, 14, 960)  3840        conv4_block22_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_0_relu (Activatio (None, 14, 14, 960)  0           conv4_block23_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_conv (Conv2D)   (None, 14, 14, 128)  122880      conv4_block23_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block23_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block23_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block23_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block23_concat (Concatena (None, 14, 14, 992)  0           conv4_block22_concat[0][0]       \n",
      "                                                                 conv4_block23_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_0_bn (BatchNormal (None, 14, 14, 992)  3968        conv4_block23_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_0_relu (Activatio (None, 14, 14, 992)  0           conv4_block24_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_conv (Conv2D)   (None, 14, 14, 128)  126976      conv4_block24_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_bn (BatchNormal (None, 14, 14, 128)  512         conv4_block24_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_1_relu (Activatio (None, 14, 14, 128)  0           conv4_block24_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_2_conv (Conv2D)   (None, 14, 14, 32)   36864       conv4_block24_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block24_concat (Concatena (None, 14, 14, 1024) 0           conv4_block23_concat[0][0]       \n",
      "                                                                 conv4_block24_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool4_bn (BatchNormalization)   (None, 14, 14, 1024) 4096        conv4_block24_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "pool4_relu (Activation)         (None, 14, 14, 1024) 0           pool4_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool4_conv (Conv2D)             (None, 14, 14, 512)  524288      pool4_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool4_pool (AveragePooling2D)   (None, 7, 7, 512)    0           pool4_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 7, 7, 512)    2048        pool4_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_relu (Activation (None, 7, 7, 512)    0           conv5_block1_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 128)    65536       conv5_block1_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 7, 7, 128)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_concat (Concatenat (None, 7, 7, 544)    0           pool4_pool[0][0]                 \n",
      "                                                                 conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_0_bn (BatchNormali (None, 7, 7, 544)    2176        conv5_block1_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_0_relu (Activation (None, 7, 7, 544)    0           conv5_block2_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 128)    69632       conv5_block2_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 7, 7, 128)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_concat (Concatenat (None, 7, 7, 576)    0           conv5_block1_concat[0][0]        \n",
      "                                                                 conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_0_bn (BatchNormali (None, 7, 7, 576)    2304        conv5_block2_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_0_relu (Activation (None, 7, 7, 576)    0           conv5_block3_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 128)    73728       conv5_block3_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 7, 7, 128)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_concat (Concatenat (None, 7, 7, 608)    0           conv5_block2_concat[0][0]        \n",
      "                                                                 conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_0_bn (BatchNormali (None, 7, 7, 608)    2432        conv5_block3_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_0_relu (Activation (None, 7, 7, 608)    0           conv5_block4_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_conv (Conv2D)    (None, 7, 7, 128)    77824       conv5_block4_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_1_relu (Activation (None, 7, 7, 128)    0           conv5_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block4_concat (Concatenat (None, 7, 7, 640)    0           conv5_block3_concat[0][0]        \n",
      "                                                                 conv5_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_0_bn (BatchNormali (None, 7, 7, 640)    2560        conv5_block4_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_0_relu (Activation (None, 7, 7, 640)    0           conv5_block5_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_conv (Conv2D)    (None, 7, 7, 128)    81920       conv5_block5_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_1_relu (Activation (None, 7, 7, 128)    0           conv5_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block5_concat (Concatenat (None, 7, 7, 672)    0           conv5_block4_concat[0][0]        \n",
      "                                                                 conv5_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_0_bn (BatchNormali (None, 7, 7, 672)    2688        conv5_block5_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_0_relu (Activation (None, 7, 7, 672)    0           conv5_block6_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_conv (Conv2D)    (None, 7, 7, 128)    86016       conv5_block6_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_1_relu (Activation (None, 7, 7, 128)    0           conv5_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block6_concat (Concatenat (None, 7, 7, 704)    0           conv5_block5_concat[0][0]        \n",
      "                                                                 conv5_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_0_bn (BatchNormali (None, 7, 7, 704)    2816        conv5_block6_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_0_relu (Activation (None, 7, 7, 704)    0           conv5_block7_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_conv (Conv2D)    (None, 7, 7, 128)    90112       conv5_block7_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block7_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_1_relu (Activation (None, 7, 7, 128)    0           conv5_block7_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block7_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block7_concat (Concatenat (None, 7, 7, 736)    0           conv5_block6_concat[0][0]        \n",
      "                                                                 conv5_block7_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_0_bn (BatchNormali (None, 7, 7, 736)    2944        conv5_block7_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_0_relu (Activation (None, 7, 7, 736)    0           conv5_block8_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_conv (Conv2D)    (None, 7, 7, 128)    94208       conv5_block8_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block8_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_1_relu (Activation (None, 7, 7, 128)    0           conv5_block8_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block8_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block8_concat (Concatenat (None, 7, 7, 768)    0           conv5_block7_concat[0][0]        \n",
      "                                                                 conv5_block8_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_0_bn (BatchNormali (None, 7, 7, 768)    3072        conv5_block8_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_0_relu (Activation (None, 7, 7, 768)    0           conv5_block9_0_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_conv (Conv2D)    (None, 7, 7, 128)    98304       conv5_block9_0_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_bn (BatchNormali (None, 7, 7, 128)    512         conv5_block9_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_1_relu (Activation (None, 7, 7, 128)    0           conv5_block9_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_2_conv (Conv2D)    (None, 7, 7, 32)     36864       conv5_block9_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block9_concat (Concatenat (None, 7, 7, 800)    0           conv5_block8_concat[0][0]        \n",
      "                                                                 conv5_block9_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_0_bn (BatchNormal (None, 7, 7, 800)    3200        conv5_block9_concat[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_0_relu (Activatio (None, 7, 7, 800)    0           conv5_block10_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_conv (Conv2D)   (None, 7, 7, 128)    102400      conv5_block10_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block10_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block10_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block10_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block10_concat (Concatena (None, 7, 7, 832)    0           conv5_block9_concat[0][0]        \n",
      "                                                                 conv5_block10_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_0_bn (BatchNormal (None, 7, 7, 832)    3328        conv5_block10_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_0_relu (Activatio (None, 7, 7, 832)    0           conv5_block11_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_conv (Conv2D)   (None, 7, 7, 128)    106496      conv5_block11_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block11_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block11_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block11_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block11_concat (Concatena (None, 7, 7, 864)    0           conv5_block10_concat[0][0]       \n",
      "                                                                 conv5_block11_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_0_bn (BatchNormal (None, 7, 7, 864)    3456        conv5_block11_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_0_relu (Activatio (None, 7, 7, 864)    0           conv5_block12_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_conv (Conv2D)   (None, 7, 7, 128)    110592      conv5_block12_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block12_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block12_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block12_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block12_concat (Concatena (None, 7, 7, 896)    0           conv5_block11_concat[0][0]       \n",
      "                                                                 conv5_block12_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_0_bn (BatchNormal (None, 7, 7, 896)    3584        conv5_block12_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_0_relu (Activatio (None, 7, 7, 896)    0           conv5_block13_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_conv (Conv2D)   (None, 7, 7, 128)    114688      conv5_block13_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block13_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block13_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block13_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block13_concat (Concatena (None, 7, 7, 928)    0           conv5_block12_concat[0][0]       \n",
      "                                                                 conv5_block13_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_0_bn (BatchNormal (None, 7, 7, 928)    3712        conv5_block13_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_0_relu (Activatio (None, 7, 7, 928)    0           conv5_block14_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_conv (Conv2D)   (None, 7, 7, 128)    118784      conv5_block14_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block14_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block14_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block14_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block14_concat (Concatena (None, 7, 7, 960)    0           conv5_block13_concat[0][0]       \n",
      "                                                                 conv5_block14_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_0_bn (BatchNormal (None, 7, 7, 960)    3840        conv5_block14_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_0_relu (Activatio (None, 7, 7, 960)    0           conv5_block15_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_conv (Conv2D)   (None, 7, 7, 128)    122880      conv5_block15_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block15_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block15_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block15_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block15_concat (Concatena (None, 7, 7, 992)    0           conv5_block14_concat[0][0]       \n",
      "                                                                 conv5_block15_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_0_bn (BatchNormal (None, 7, 7, 992)    3968        conv5_block15_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_0_relu (Activatio (None, 7, 7, 992)    0           conv5_block16_0_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_conv (Conv2D)   (None, 7, 7, 128)    126976      conv5_block16_0_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_bn (BatchNormal (None, 7, 7, 128)    512         conv5_block16_1_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_1_relu (Activatio (None, 7, 7, 128)    0           conv5_block16_1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_2_conv (Conv2D)   (None, 7, 7, 32)     36864       conv5_block16_1_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block16_concat (Concatena (None, 7, 7, 1024)   0           conv5_block15_concat[0][0]       \n",
      "                                                                 conv5_block16_2_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bn (BatchNormalization)         (None, 7, 7, 1024)   4096        conv5_block16_concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "relu (Activation)               (None, 7, 7, 1024)   0           bn[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 1024)         0           relu[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            1025        global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 7,038,529\n",
      "Trainable params: 6,954,881\n",
      "Non-trainable params: 83,648\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "#keras.applications.densenet.DenseNet121(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n",
    "from tensorflow.keras.applications.densenet import DenseNet121\n",
    "#from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.densenet import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Activation\n",
    "\n",
    "def Densenet121(show_layers,weights,input_shape):\n",
    "        base_model = DenseNet121(include_top=False, weights=weights, input_shape=input_shape)\n",
    "        x = GlobalAveragePooling2D()(base_model.output)\n",
    "        pred = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "        model = Model(inputs=base_model.input, outputs=pred)\n",
    "        model.compile(optimizer=SGD(lr=1e-3, decay=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "        if show_layers:\n",
    "            for i, layer in enumerate(model.layers):\n",
    "                print(i, layer.name, layer.trainable)\n",
    "        return model\n",
    "    \n",
    "model = Densenet121(False,weights=None,input_shape=(224, 224, 3))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>  M_angle\n",
      "============================================Densenet121=============================================\n",
      "Found 13680 images belonging to 2 classes.\n",
      "Found 10080 images belonging to 2 classes.\n",
      "train shape : (13680,)\n",
      "Epoch 1/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.3486 - accuracy: 0.8550\n",
      "Epoch 00001: val_loss improved from inf to 0.44960, saving model to .\\result_cuda\\M_angle-01-0.4496.hdf5\n",
      "1710/1710 [==============================] - 1232s 720ms/step - loss: 0.3488 - accuracy: 0.8548 - val_loss: 0.4496 - val_accuracy: 0.7831\n",
      "Epoch 2/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.2094 - accuracy: 0.9195\n",
      "Epoch 00002: val_loss did not improve from 0.44960\n",
      "1710/1710 [==============================] - 1192s 697ms/step - loss: 0.2095 - accuracy: 0.9195 - val_loss: 1.1868 - val_accuracy: 0.6801\n",
      "Epoch 3/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.1516 - accuracy: 0.9447\n",
      "Epoch 00003: val_loss did not improve from 0.44960\n",
      "1710/1710 [==============================] - 1165s 681ms/step - loss: 0.1515 - accuracy: 0.9447 - val_loss: 0.9999 - val_accuracy: 0.7188\n",
      "Epoch 4/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.1282 - accuracy: 0.9538\n",
      "Epoch 00004: val_loss improved from 0.44960 to 0.43540, saving model to .\\result_cuda\\M_angle-04-0.4354.hdf5\n",
      "1710/1710 [==============================] - 1175s 687ms/step - loss: 0.1282 - accuracy: 0.9538 - val_loss: 0.4354 - val_accuracy: 0.7970\n",
      "Epoch 5/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.1000 - accuracy: 0.9649\n",
      "Epoch 00005: val_loss improved from 0.43540 to 0.34059, saving model to .\\result_cuda\\M_angle-05-0.3406.hdf5\n",
      "1710/1710 [==============================] - 1167s 682ms/step - loss: 0.1000 - accuracy: 0.9649 - val_loss: 0.3406 - val_accuracy: 0.9180\n",
      "Epoch 6/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0772 - accuracy: 0.9750\n",
      "Epoch 00006: val_loss improved from 0.34059 to 0.21726, saving model to .\\result_cuda\\M_angle-06-0.2173.hdf5\n",
      "1710/1710 [==============================] - 1169s 684ms/step - loss: 0.0771 - accuracy: 0.9750 - val_loss: 0.2173 - val_accuracy: 0.9249\n",
      "Epoch 7/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0653 - accuracy: 0.9776\n",
      "Epoch 00007: val_loss did not improve from 0.21726\n",
      "1710/1710 [==============================] - 1166s 682ms/step - loss: 0.0653 - accuracy: 0.9776 - val_loss: 0.3194 - val_accuracy: 0.9185\n",
      "Epoch 8/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0623 - accuracy: 0.9803\n",
      "Epoch 00008: val_loss did not improve from 0.21726\n",
      "1710/1710 [==============================] - 1167s 682ms/step - loss: 0.0623 - accuracy: 0.9803 - val_loss: 1.9100 - val_accuracy: 0.7289\n",
      "Epoch 9/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0444 - accuracy: 0.9873\n",
      "Epoch 00009: val_loss improved from 0.21726 to 0.13641, saving model to .\\result_cuda\\M_angle-09-0.1364.hdf5\n",
      "1710/1710 [==============================] - 1166s 682ms/step - loss: 0.0444 - accuracy: 0.9873 - val_loss: 0.1364 - val_accuracy: 0.9585\n",
      "Epoch 10/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0445 - accuracy: 0.9869\n",
      "Epoch 00010: val_loss did not improve from 0.13641\n",
      "1710/1710 [==============================] - 1167s 683ms/step - loss: 0.0445 - accuracy: 0.9869 - val_loss: 0.2264 - val_accuracy: 0.9395\n",
      "Epoch 11/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0383 - accuracy: 0.9895\n",
      "Epoch 00011: val_loss did not improve from 0.13641\n",
      "1710/1710 [==============================] - 1182s 691ms/step - loss: 0.0383 - accuracy: 0.9895 - val_loss: 0.1602 - val_accuracy: 0.9592\n",
      "Epoch 12/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0313 - accuracy: 0.9930\n",
      "Epoch 00012: val_loss did not improve from 0.13641\n",
      "1710/1710 [==============================] - 1171s 685ms/step - loss: 0.0312 - accuracy: 0.9930 - val_loss: 0.2348 - val_accuracy: 0.9484\n",
      "Epoch 13/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0286 - accuracy: 0.9921\n",
      "Epoch 00013: val_loss did not improve from 0.13641\n",
      "1710/1710 [==============================] - 1165s 682ms/step - loss: 0.0286 - accuracy: 0.9921 - val_loss: 0.4615 - val_accuracy: 0.7899\n",
      "Epoch 14/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0338 - accuracy: 0.9907\n",
      "Epoch 00014: val_loss improved from 0.13641 to 0.13290, saving model to .\\result_cuda\\M_angle-14-0.1329.hdf5\n",
      "1710/1710 [==============================] - 1165s 681ms/step - loss: 0.0338 - accuracy: 0.9907 - val_loss: 0.1329 - val_accuracy: 0.9529\n",
      "Epoch 15/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0275 - accuracy: 0.9925\n",
      "Epoch 00015: val_loss did not improve from 0.13290\n",
      "1710/1710 [==============================] - 1170s 684ms/step - loss: 0.0275 - accuracy: 0.9925 - val_loss: 0.3928 - val_accuracy: 0.9239\n",
      "Epoch 16/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0271 - accuracy: 0.9925\n",
      "Epoch 00016: val_loss did not improve from 0.13290\n",
      "1710/1710 [==============================] - 1168s 683ms/step - loss: 0.0271 - accuracy: 0.9925 - val_loss: 0.1872 - val_accuracy: 0.9576\n",
      "Epoch 17/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0221 - accuracy: 0.9942\n",
      "Epoch 00017: val_loss did not improve from 0.13290\n",
      "1710/1710 [==============================] - 1166s 682ms/step - loss: 0.0221 - accuracy: 0.9942 - val_loss: 0.2885 - val_accuracy: 0.9171\n",
      "Epoch 18/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0213 - accuracy: 0.9944\n",
      "Epoch 00018: val_loss did not improve from 0.13290\n",
      "1710/1710 [==============================] - 1169s 684ms/step - loss: 0.0213 - accuracy: 0.9944 - val_loss: 0.1888 - val_accuracy: 0.9585\n",
      "Epoch 19/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0175 - accuracy: 0.9956\n",
      "Epoch 00019: val_loss did not improve from 0.13290\n",
      "1710/1710 [==============================] - 1168s 683ms/step - loss: 0.0175 - accuracy: 0.9956 - val_loss: 0.1658 - val_accuracy: 0.9520\n",
      "Epoch 20/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0177 - accuracy: 0.9963\n",
      "Epoch 00020: val_loss improved from 0.13290 to 0.13117, saving model to .\\result_cuda\\M_angle-20-0.1312.hdf5\n",
      "1710/1710 [==============================] - 1166s 682ms/step - loss: 0.0177 - accuracy: 0.9963 - val_loss: 0.1312 - val_accuracy: 0.9714\n",
      "Epoch 21/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0144 - accuracy: 0.9970\n",
      "Epoch 00021: val_loss did not improve from 0.13117\n",
      "1710/1710 [==============================] - 1167s 683ms/step - loss: 0.0144 - accuracy: 0.9970 - val_loss: 0.1608 - val_accuracy: 0.9648\n",
      "Epoch 22/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0148 - accuracy: 0.9959\n",
      "Epoch 00022: val_loss did not improve from 0.13117\n",
      "1710/1710 [==============================] - 1166s 682ms/step - loss: 0.0148 - accuracy: 0.9959 - val_loss: 0.1710 - val_accuracy: 0.9668\n",
      "Epoch 23/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0163 - accuracy: 0.9966\n",
      "Epoch 00023: val_loss did not improve from 0.13117\n",
      "1710/1710 [==============================] - 1166s 682ms/step - loss: 0.0163 - accuracy: 0.9966 - val_loss: 0.1521 - val_accuracy: 0.9696\n",
      "Epoch 24/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0169 - accuracy: 0.9957\n",
      "Epoch 00024: val_loss did not improve from 0.13117\n",
      "1710/1710 [==============================] - 1166s 682ms/step - loss: 0.0169 - accuracy: 0.9957 - val_loss: 0.1550 - val_accuracy: 0.9688\n",
      "Epoch 25/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0144 - accuracy: 0.9967\n",
      "Epoch 00025: val_loss did not improve from 0.13117\n",
      "1710/1710 [==============================] - 1167s 682ms/step - loss: 0.0144 - accuracy: 0.9967 - val_loss: 0.5320 - val_accuracy: 0.8935\n",
      "Epoch 26/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0143 - accuracy: 0.9958\n",
      "Epoch 00026: val_loss did not improve from 0.13117\n",
      "1710/1710 [==============================] - 1163s 680ms/step - loss: 0.0143 - accuracy: 0.9958 - val_loss: 0.1594 - val_accuracy: 0.9622\n",
      "Epoch 27/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0108 - accuracy: 0.9976\n",
      "Epoch 00027: val_loss improved from 0.13117 to 0.12523, saving model to .\\result_cuda\\M_angle-27-0.1252.hdf5\n",
      "1710/1710 [==============================] - 1164s 680ms/step - loss: 0.0108 - accuracy: 0.9976 - val_loss: 0.1252 - val_accuracy: 0.9699\n",
      "Epoch 28/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0108 - accuracy: 0.9980\n",
      "Epoch 00028: val_loss did not improve from 0.12523\n",
      "1710/1710 [==============================] - 1165s 681ms/step - loss: 0.0108 - accuracy: 0.9980 - val_loss: 0.2492 - val_accuracy: 0.9440\n",
      "Epoch 29/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0109 - accuracy: 0.9977\n",
      "Epoch 00029: val_loss did not improve from 0.12523\n",
      "1710/1710 [==============================] - 1167s 683ms/step - loss: 0.0109 - accuracy: 0.9977 - val_loss: 0.2285 - val_accuracy: 0.9491\n",
      "Epoch 30/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0096 - accuracy: 0.9983\n",
      "Epoch 00030: val_loss did not improve from 0.12523\n",
      "1710/1710 [==============================] - 1169s 684ms/step - loss: 0.0096 - accuracy: 0.9983 - val_loss: 0.2642 - val_accuracy: 0.9493\n",
      "Epoch 31/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0106 - accuracy: 0.9978\n",
      "Epoch 00031: val_loss did not improve from 0.12523\n",
      "1710/1710 [==============================] - 1167s 682ms/step - loss: 0.0106 - accuracy: 0.9978 - val_loss: 0.1661 - val_accuracy: 0.9688\n",
      "Epoch 32/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0106 - accuracy: 0.9977\n",
      "Epoch 00032: val_loss did not improve from 0.12523\n",
      "1710/1710 [==============================] - 1165s 682ms/step - loss: 0.0106 - accuracy: 0.9977 - val_loss: 0.1592 - val_accuracy: 0.9696\n",
      "Epoch 33/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0066 - accuracy: 0.9988\n",
      "Epoch 00033: val_loss did not improve from 0.12523\n",
      "1710/1710 [==============================] - 1167s 682ms/step - loss: 0.0066 - accuracy: 0.9988 - val_loss: 0.1591 - val_accuracy: 0.9704\n",
      "Epoch 34/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0133 - accuracy: 0.9971\n",
      "Epoch 00034: val_loss did not improve from 0.12523\n",
      "1710/1710 [==============================] - 1164s 681ms/step - loss: 0.0133 - accuracy: 0.9971 - val_loss: 0.1847 - val_accuracy: 0.9606\n",
      "Epoch 35/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0085 - accuracy: 0.9981\n",
      "Epoch 00035: val_loss did not improve from 0.12523\n",
      "1710/1710 [==============================] - 1167s 683ms/step - loss: 0.0085 - accuracy: 0.9981 - val_loss: 0.1490 - val_accuracy: 0.9726\n",
      "Epoch 36/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0088 - accuracy: 0.9980\n",
      "Epoch 00036: val_loss did not improve from 0.12523\n",
      "1710/1710 [==============================] - 1165s 681ms/step - loss: 0.0088 - accuracy: 0.9980 - val_loss: 0.1547 - val_accuracy: 0.9702\n",
      "Epoch 37/100\n",
      "1709/1710 [============================>.] - ETA: 0s - loss: 0.0192 - accuracy: 0.9950\n",
      "Epoch 00037: val_loss did not improve from 0.12523\n",
      "1710/1710 [==============================] - 1170s 684ms/step - loss: 0.0192 - accuracy: 0.9950 - val_loss: 0.2084 - val_accuracy: 0.9473\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9699\tloss: 0.1252\n",
      "==========================================M_angle finished==========================================\n"
     ]
    }
   ],
   "source": [
    "#### import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "%matplotlib inline\n",
    "SEED = 777\n",
    "\n",
    "def main():\n",
    "    inputSize = 224\n",
    "    batchSize = 8\n",
    "    nEpoch = 100\n",
    "    \n",
    "    # 디렉토리 정보 설정\n",
    "    # dataDir = 'D:\\\\Face_Database\\\\B-Database'\n",
    "    dataDir = 'E:\\\\Face_Database\\\\B-Database\\\\protocol_1'   \n",
    "    # 학습 및 테스트 할 데이터베이스 디렉토리명\n",
    "    trainDB = 'M_angle'  # D:\\Face_Database\\B-Database\\protocol_4\\train\n",
    "    validDB = 'M_angle'\n",
    "    \n",
    "    saveDir =  '.\\\\result_cuda'\n",
    "    if not os.path.exists(saveDir):    \n",
    "        os.makedirs(saveDir)\n",
    "    model_path = os.path.join(saveDir, trainDB + '-{epoch:02d}-{val_loss:.4f}.hdf5')\n",
    "    \n",
    "    # 데이터베이스별 학습 수행\n",
    "    print('>> ', trainDB)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    print('Densenet121'.center(100, '='))\n",
    "   \n",
    "    '''\n",
    "    training\n",
    "    '''\n",
    "    K.clear_session()\n",
    "\n",
    "    # 네트워크 정의\n",
    "    model = Densenet121(False,weights=None,input_shape=(224, 224, 3))\n",
    "\n",
    "    # 데이터 generator 생성\n",
    "    train_datagen = ImageDataGenerator(rescale=1.0/255.0,\n",
    "                                       horizontal_flip=True,)#이미지augmentation > 이미지를 회전,확대,축소 등 여러변형해보는것.\n",
    "    train_generator = train_datagen.flow_from_directory(os.path.join(*[dataDir,trainDB,'train']),\n",
    "                                                        target_size=(inputSize, inputSize),\n",
    "                                                        batch_size=batchSize,\n",
    "                                                        class_mode='binary',\n",
    "                                                        interpolation='bilinear',\n",
    "                                                        shuffle=True,)\n",
    "\n",
    "    valid_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    valid_generator = valid_datagen.flow_from_directory(os.path.join(*[dataDir, validDB,'val']),\n",
    "                                                        target_size=(inputSize, inputSize),\n",
    "                                                        batch_size=batchSize,\n",
    "                                                        class_mode='binary',\n",
    "                                                        interpolation='bilinear',\n",
    "                                                        shuffle=False,)\n",
    "\n",
    "    print('train shape :',train_generator.classes.shape)\n",
    "    # unbalanced class를 해결하기 위한 class_weight 설정 #np.unique는 중복원소제거하는거\n",
    "    class_weights = class_weight.compute_class_weight('balanced', np.unique(train_generator.classes), train_generator.classes) \n",
    "\n",
    "    # callback 함수 정의 dropout함수\n",
    "    early_stop = EarlyStopping(patience=10, monitor='val_loss')\n",
    "    cb_checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "    # 모델 학습\n",
    "    hist = model.fit_generator(train_generator, \n",
    "                               epochs=nEpoch,\n",
    "                               steps_per_epoch=len(train_generator),\n",
    "                               class_weight=class_weights,\n",
    "                               validation_data=valid_generator,\n",
    "                               validation_steps=len(valid_generator),\n",
    "                               verbose=1,\n",
    "                               callbacks=[early_stop, cb_checkpoint])\n",
    "\n",
    "    \n",
    "    # 학습 결과 그래프 그리기\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, loss_ax = plt.subplots()\n",
    "    acc_ax = loss_ax.twinx()\n",
    "\n",
    "    loss_ax.plot(hist.history['loss'], 'b', label='train loss')\n",
    "    loss_ax.plot(hist.history['val_loss'], '--r', label='val loss')\n",
    "    loss_ax.set_xlabel('epoch')\n",
    "    loss_ax.set_ylabel('loss')\n",
    "    loss_ax.legend(loc='upper left')\n",
    "\n",
    "    plt.show()\n",
    "    print('acc: {:.4f}\\tloss: {:.4f}'.format(hist.history['val_accuracy'][-11], hist.history['val_loss'][-11]))\n",
    "\n",
    "    print(('{} finished'.format(trainDB)).center(100,'='))\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> model loaded: protocol_4-21-0.0745_0.001.hdf5\n",
      ">>>> evaluating on 'protocol_4'\n",
      "Found 30240 images belonging to 2 classes.\n",
      "Found 56160 images belonging to 2 classes.\n",
      "                  pred_fake(0)   pred_real(1)\n",
      "actural_fake(0)           36565            875\n",
      "actual_real(1)             438          18282\n",
      "\n",
      "=======================\n",
      "<class 'numpy.ndarray'>\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False  True False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False  True False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False  True\n",
      " False  True  True False  True False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True False False False  True False False False\n",
      " False False False False False  True False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True  True False False False False False False\n",
      "  True  True  True  True  True False  True False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False  True  True  True  True  True  True False\n",
      "  True  True False False False  True False False False False  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True  True  True  True\n",
      "  True  True  True  True  True  True False False False  True False False\n",
      "  True False False  True False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False  True\n",
      " False  True False False False  True False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False  True False False  True False False\n",
      " False False False False  True False False False False  True  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True False  True False False  True  True False  True  True False False\n",
      " False False  True  True False  True False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True  True False False False False False False False False False\n",
      " False False False  True  True  True  True  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False  True  True  True False  True False False\n",
      "  True False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False  True False False False False False False False False  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True False  True False False False False  True\n",
      "  True  True False False False  True False  True False  True  True  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False  True  True  True False False False  True False  True False  True\n",
      "  True  True False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False  True False False False False False False\n",
      " False  True False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False  True False False\n",
      "  True False False False False False False False  True  True False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True False  True  True False  True False False\n",
      " False  True False  True False False  True False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False  True False False False False False False False False False\n",
      " False False False False False False False  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False  True  True  True  True  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False  True False False  True  True  True  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True  True  True False\n",
      "  True False  True False False False False  True False False  True  True\n",
      "  True  True False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False  True False  True\n",
      "  True  True  True  True  True  True  True  True  True  True False  True\n",
      "  True  True  True  True False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True False  True False False False  True False False False False False\n",
      "  True  True  True  True  True False  True False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False  True  True False\n",
      "  True  True  True  True  True  True  True False  True  True  True  True\n",
      "  True  True False  True False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True  True  True False False  True False  True\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False  True  True  True False  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True False False  True False  True False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False  True  True  True\n",
      "  True False False False False False  True  True  True  True  True  True\n",
      " False False False  True False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False  True  True False False False  True  True\n",
      " False  True  True  True  True  True  True False  True  True False  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True  True False False  True  True  True False\n",
      "  True  True  True False  True  True False  True  True  True False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False  True\n",
      " False False False False False False False False False  True False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False  True False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True  True False False False False  True  True\n",
      " False  True False False False  True  True  True False  True  True  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True False False False False False False False False  True False\n",
      " False  True False False False  True  True False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True  True  True  True  True False False False\n",
      "  True False  True  True  True  True  True False  True False False  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True False  True  True  True False False  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True  True  True  True  True  True  True False  True  True  True\n",
      "  True  True  True  True  True  True  True False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False  True False\n",
      "  True False False False  True  True  True False  True  True  True  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True  True  True  True  True False  True  True False  True  True\n",
      " False False  True False False False  True  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True False False False False False False False False False  True  True\n",
      " False False False False False False  True False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False  True\n",
      " False  True False False  True  True  True  True False  True  True  True\n",
      "  True False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False  True False\n",
      " False  True False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True  True False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True False  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True False  True\n",
      "  True  True False  True False False False False False False False False\n",
      " False False False False False False  True False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False  True False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True False  True  True  True  True  True  True  True  True False\n",
      "  True  True  True  True  True False  True False False False False  True\n",
      " False False False False False  True False  True  True  True  True  True\n",
      "  True  True  True False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True False  True  True False False  True  True  True  True  True False\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False  True  True False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True False\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False False False False False False False False\n",
      " False  True False  True  True  True False False  True False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True False  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True False  True False  True False  True  True False  True\n",
      "  True  True  True  True  True  True  True False  True  True False False\n",
      " False False False False False False False False False False False  True\n",
      " False  True False  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True False  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True False False  True  True False False  True  True  True\n",
      " False False  True False  True  True  True False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True False  True False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False  True False False  True  True  True False  True False  True\n",
      "  True False  True False  True  True False False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True False False False\n",
      "  True False  True  True  True False  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True False  True  True\n",
      "  True  True  True  True  True False False  True  True  True  True  True\n",
      "  True  True  True  True  True False  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False  True  True  True  True  True\n",
      "  True  True  True  True  True  True False  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True False  True  True False False False  True  True  True  True\n",
      "  True  True  True False  True False  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False False False  True False  True  True False\n",
      " False False  True False  True  True  True False  True  True False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True False  True False False False False False False  True False\n",
      "  True False False False False  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False False False False False False False False\n",
      "  True False False  True  True False False False False  True False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False False False  True False False False False False  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False False False False\n",
      " False False False False False  True False False False False False False\n",
      " False False False False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False False  True  True  True False False False  True False False\n",
      " False False False  True  True False  True False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False  True  True  True False  True\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True False  True  True False  True  True  True  True False  True\n",
      "  True False  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False  True  True  True\n",
      "  True  True  True False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True False  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True False  True  True False  True False  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True False  True False\n",
      "  True False  True False  True  True  True  True  True  True False  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True False  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False False False False False False  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True False  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True False False False\n",
      "  True False  True  True False False False False False False False  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True False False False False False False False\n",
      "  True False False  True  True False False  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False  True False False  True  True  True False  True False False False\n",
      "  True  True  True False False  True  True  True False False  True False\n",
      "  True False False  True False False False False False False False False\n",
      " False False False  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False  True  True  True  True  True\n",
      "  True False  True  True  True False  True False False False  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True False  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True False  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True]\n",
      "=======================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EER: 0.0285\tHTER: 0.0234\n",
      ">> finished\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학습된 모델 테스트셋에서 성능 평가\n",
    "# 얼굴 스푸핑 분야에서 평가 metric으로 HTER이랑 EER 두 개 주로 사용\n",
    "# =========================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def main():\n",
    "    inputSize = 224\n",
    "    batchSize = 8\n",
    "    \n",
    "    trainDB = 'protocol_4'\n",
    "    testDB = 'protocol_4'\n",
    "    \n",
    "    dataDir = 'E:\\\\Face_Database\\\\B-Database'\n",
    "    modelPath = 'C:\\\\Users\\\\ysk00\\\\OneDrive\\\\바탕 화면\\\\prlab\\\\ysg\\\\densenet-spoofing\\\\result_cuda\\\\protocol_4-21-0.0745_0.001.hdf5'\n",
    "    \n",
    "    print('>> model loaded: {}'.format(os.path.basename(modelPath)))\n",
    "    K.clear_session()\n",
    "    model = load_model(modelPath)\n",
    "        \n",
    "    print(\">>>> evaluating on '{}'\".format(testDB))\n",
    "    test_datagen = ImageDataGenerator()\n",
    "\n",
    "    val_datagen = ImageDataGenerator()\n",
    "    \n",
    "    val_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    val_generator = val_datagen.flow_from_directory(os.path.join(*[dataDir, testDB, 'val']),\n",
    "                                                      target_size=(inputSize, inputSize),\n",
    "                                                      batch_size=batchSize,\n",
    "                                                      class_mode='binary',\n",
    "                                                      interpolation='bilinear',\n",
    "                                                      shuffle=False,)\n",
    "    \n",
    "    test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    test_generator = test_datagen.flow_from_directory(os.path.join(*[dataDir, testDB, 'test']),\n",
    "                                                      target_size=(inputSize, inputSize),\n",
    "                                                      batch_size=batchSize,\n",
    "                                                      class_mode='binary',\n",
    "                                                      interpolation='bilinear',\n",
    "                                                      shuffle=False,)\n",
    "\n",
    "    ''' evaluating EER '''\n",
    "    y_true = val_generator.classes\n",
    "    y_score = model.predict_generator(val_generator, steps=len(val_generator)).ravel()\n",
    "\n",
    "    fpr, tpr, threshold = roc_curve(y_true, y_score)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr-fpr)))]\n",
    "    val_eer = (fpr[np.nanargmin(np.absolute((fnr - fpr)))] + fnr[np.nanargmin(np.absolute((fnr - fpr)))]) / 2\n",
    "\n",
    "    ''' evaluating HTER '''\n",
    "    y_true = test_generator.classes\n",
    "    y_score = model.predict_generator(test_generator, steps = len(test_generator)).ravel()\n",
    "\n",
    "    # Calculate EER threshold\n",
    "    fpr, tpr, threshold = roc_curve(y_true, y_score)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr-fpr)))]\n",
    "\n",
    "    # HTER\n",
    "    y_pred = y_score > eer_threshold\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    labels = test_generator.class_indices\n",
    "    print('                  pred_fake({})   pred_real({})\\nactural_fake({})    {:12d}   {:12d}\\nactual_real({})    {:12d}   {:12d}\\n'.format(labels['Fake'], labels['Real'], labels['Fake'], tn, fp, labels['Real'], fn, tp))\n",
    "    hter = (fp/(tn+fp) + fn/(fn+tp)) * 0.5\n",
    "    print('=======================')\n",
    "    np.set_printoptions(threshold=sys.maxsize)\n",
    "    print(type(y_pred))\n",
    "    print(y_pred)\n",
    "    print('=======================')\n",
    "\n",
    "    # ROC curve\n",
    "    Accuracy = ((tn+tp) / (tn+fp+fn+tp)) * 100.0\n",
    "\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "\n",
    "    plt.plot(fpr, tpr, 'r--', label='ROC_curve (acc_1st = %0.2f%%)' % Accuracy)\n",
    "    plt.plot([0, 1], [0, 1], color = 'orange', lw=lw, linestyle='--')\n",
    "\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc = \"lower right\")\n",
    "\n",
    "    plt.savefig('protocol_4_001.png')\n",
    "    plt.figure()\n",
    "    \n",
    "    print('EER: {:.4f}\\tHTER: {:.4f}'.format(val_eer, hter))\n",
    "\n",
    "    print('>> finished')\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train(CASIA0 -> Test(PR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> model loaded: protocol_4-21-0.0745_0.001.hdf5\n"
     ]
    }
   ],
   "source": [
    "# 학습된 모델 테스트셋에서 성능 평가\n",
    "# 얼굴 스푸핑 분야에서 평가 metric으로 HTER이랑 EER 두 개 주로 사용\n",
    "# =========================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import roc_curve, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def main():\n",
    "    inputSize = 224\n",
    "    batchSize = 8\n",
    "    \n",
    "    trainDB = 'protocol_4'\n",
    "    testDB = 'CASIA-FASD'\n",
    "    \n",
    "    dataDir = 'E:\\\\Face_Database\\\\public-Database\\\\1_crop_result'\n",
    "    modelPath = 'F:\\\\prlab\\\\ysg\\\\densenet-spoofing\\\\Densenet_result\\\\0.001\\\\result_PR-FASD\\\\protocol_4-21-0.0745_0.001.hdf5'\n",
    "    \n",
    "    print('>> model loaded: {}'.format(os.path.basename(modelPath)))\n",
    "    K.clear_session()\n",
    "    model = load_model(modelPath)\n",
    "        \n",
    "    print(\">>>> evaluating on '{}'\".format(testDB))\n",
    "    test_datagen = ImageDataGenerator()\n",
    "\n",
    "    val_datagen = ImageDataGenerator()\n",
    "    \n",
    "    val_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    val_generator = val_datagen.flow_from_directory(os.path.join(*[dataDir, testDB, 'val']),\n",
    "                                                      target_size=(inputSize, inputSize),\n",
    "                                                      batch_size=batchSize,\n",
    "                                                      class_mode='binary',\n",
    "                                                      interpolation='bilinear',\n",
    "                                                      shuffle=False,)\n",
    "    \n",
    "    test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    test_generator = test_datagen.flow_from_directory(os.path.join(*[dataDir, testDB, 'test']),\n",
    "                                                      target_size=(inputSize, inputSize),\n",
    "                                                      batch_size=batchSize,\n",
    "                                                      class_mode='binary',\n",
    "                                                      interpolation='bilinear',\n",
    "                                                      shuffle=False,)\n",
    "\n",
    "    ''' evaluating EER '''\n",
    "    y_true = val_generator.classes\n",
    "    y_score = model.predict_generator(val_generator, steps=len(val_generator)).ravel()\n",
    "\n",
    "    fpr, tpr, threshold = roc_curve(y_true, y_score)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr-fpr)))]\n",
    "    val_eer = (fpr[np.nanargmin(np.absolute((fnr - fpr)))] + fnr[np.nanargmin(np.absolute((fnr - fpr)))]) / 2\n",
    "\n",
    "    ''' evaluating HTER '''\n",
    "    y_true = test_generator.classes\n",
    "    y_score = model.predict_generator(test_generator, steps = len(test_generator)).ravel()\n",
    "\n",
    "    # Calculate EER threshold\n",
    "    fpr, tpr, threshold = roc_curve(y_true, y_score)\n",
    "    fnr = 1 - tpr\n",
    "    eer_threshold = threshold[np.nanargmin(np.absolute((fnr-fpr)))]\n",
    "\n",
    "    # HTER\n",
    "    y_pred = y_score > eer_threshold\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    labels = test_generator.class_indices\n",
    "    print('                  pred_fake({})   pred_real({})\\nactural_fake({})    {:12d}   {:12d}\\nactual_real({})    {:12d}   {:12d}\\n'.format(labels['Fake'], labels['Real'], labels['Fake'], tn, fp, labels['Real'], fn, tp))\n",
    "    hter = (fp/(tn+fp) + fn/(fn+tp)) * 0.5\n",
    "    print('=======================')\n",
    "    np.set_printoptions(threshold=sys.maxsize)\n",
    "    print(type(y_pred))\n",
    "    print(y_pred)\n",
    "    print('=======================')\n",
    "\n",
    "    # ROC curve\n",
    "    Accuracy = ((tn+tp) / (tn+fp+fn+tp)) * 100.0\n",
    "\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "\n",
    "    plt.plot(fpr, tpr, 'r--', label='ROC_curve (acc_1st = %0.2f%%)' % Accuracy)\n",
    "    plt.plot([0, 1], [0, 1], color = 'orange', lw=lw, linestyle='--')\n",
    "\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc = \"lower right\")\n",
    "\n",
    "    plt.savefig('protocol_4_001.png')\n",
    "    plt.figure()\n",
    "    \n",
    "    print('EER: {:.4f}\\tHTER: {:.4f}'.format(val_eer, hter))\n",
    "\n",
    "    print('>> finished')\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ysg",
   "language": "python",
   "name": "ysg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
